<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/logo-32x32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/logo-16x16.ico">
  <link rel="mask-icon" href="/images/logo.jpg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"guoyongyu.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.11.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="注意力机制在 Transformer 中很常见，目前也经常被用于计算机视觉当中。本文整理总结了网络上关于注意力机制的介绍。">
<meta property="og:type" content="article">
<meta property="og:title" content="Attention Mechanism">
<meta property="og:url" content="https://guoyongyu.github.io/summary/Attention%20Mechanism/index.html">
<meta property="og:site_name" content="The Blog of Star Rain">
<meta property="og:description" content="注意力机制在 Transformer 中很常见，目前也经常被用于计算机视觉当中。本文整理总结了网络上关于注意力机制的介绍。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://guoyongyu.github.io/summary/Attention%20Mechanism/1%20Encoder-Decoder框架.png">
<meta property="og:image" content="https://guoyongyu.github.io/summary/Attention%20Mechanism/2%20注意力Encoder-Decoder框架.png">
<meta property="og:image" content="https://guoyongyu.github.io/summary/Attention%20Mechanism/3%20Attention在翻译时的形成过程示例.png">
<meta property="og:image" content="https://guoyongyu.github.io/summary/Attention%20Mechanism/4%20使用RNN细化后的Encoder-Decoder框架.png">
<meta property="og:image" content="https://guoyongyu.github.io/summary/Attention%20Mechanism/5%20注意力概率分布的计算过程.png">
<meta property="og:image" content="https://guoyongyu.github.io/summary/Attention%20Mechanism/6%20英语-德语翻译中Attention的概率分布.png">
<meta property="og:image" content="https://guoyongyu.github.io/summary/Attention%20Mechanism/7%20Attention机制的本质思想.png">
<meta property="og:image" content="https://guoyongyu.github.io/summary/Attention%20Mechanism/8%20计算Attention的三个阶段.png">
<meta property="og:image" content="https://guoyongyu.github.io/summary/Attention%20Mechanism/9%20自注意力机制的计算过程.jpg">
<meta property="og:image" content="https://guoyongyu.github.io/summary/Attention%20Mechanism/10%20自注意力机制的详细计算过程.jpg">
<meta property="article:published_time" content="2022-06-16T16:00:00.000Z">
<meta property="article:modified_time" content="2022-11-17T02:23:41.759Z">
<meta property="article:author" content="Star Rain">
<meta property="article:tag" content="Summary">
<meta property="article:tag" content="Knowledge">
<meta property="article:tag" content="Deep Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://guoyongyu.github.io/summary/Attention%20Mechanism/1%20Encoder-Decoder框架.png">


<link rel="canonical" href="https://guoyongyu.github.io/summary/Attention%20Mechanism/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://guoyongyu.github.io/summary/Attention%20Mechanism/","path":"summary/Attention Mechanism/","title":"Attention Mechanism"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Attention Mechanism | The Blog of Star Rain</title>
  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">The Blog of Star Rain</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>







</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-Encoder-Decoder-%E6%A1%86%E6%9E%B6"><span class="nav-number">1.</span> <span class="nav-text">1  Encoder-Decoder 框架</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-%E7%BB%8F%E5%85%B8-Attention-%E6%A8%A1%E5%9E%8B%EF%BC%88Soft-Attention%EF%BC%89"><span class="nav-number">2.</span> <span class="nav-text">2  经典 Attention 模型（Soft-Attention）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-Attention-%E7%9A%84%E5%BC%95%E5%85%A5"><span class="nav-number">2.1.</span> <span class="nav-text">2.1  Attention 的引入</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-Attention-%E4%B8%AD%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%88%86%E5%B8%83%E7%9A%84%E7%94%9F%E6%88%90"><span class="nav-number">2.2.</span> <span class="nav-text">2.2  Attention 中注意力分布的生成</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-Attention-%E6%9C%BA%E5%88%B6%E7%9A%84%E6%9C%AC%E8%B4%A8"><span class="nav-number">2.3.</span> <span class="nav-text">2.3  Attention 机制的本质</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-Attention-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E5%8F%98%E4%BD%93"><span class="nav-number">3.</span> <span class="nav-text">3  Attention 注意力机制的变体</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-%E7%A1%AC%E6%80%A7%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%88Hard-Attention%EF%BC%89"><span class="nav-number">3.1.</span> <span class="nav-text">3.1  硬性注意力（Hard Attention）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%88Multi-Head-Attention%EF%BC%89"><span class="nav-number">3.2.</span> <span class="nav-text">3.2  多头注意力（Multi-Head Attention）</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%88Self-Attention%EF%BC%89"><span class="nav-number">4.</span> <span class="nav-text">4  自注意力机制（Self-Attention）</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">5.</span> <span class="nav-text">参考</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Star Rain</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">6</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">categories</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://guoyongyu.github.io/summary/Attention%20Mechanism/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Star Rain">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="The Blog of Star Rain">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Attention Mechanism | The Blog of Star Rain">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Attention Mechanism
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-06-17 00:00:00" itemprop="dateCreated datePublished" datetime="2022-06-17T00:00:00+08:00">2022-06-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-11-17 10:23:41" itemprop="dateModified" datetime="2022-11-17T10:23:41+08:00">2022-11-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Summary/" itemprop="url" rel="index"><span itemprop="name">Summary</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>注意力机制在 Transformer 中很常见，目前也经常被用于计算机视觉当中。本文整理总结了网络上关于注意力机制的介绍。</p>
<span id="more"></span>
<center><font size="9"><b>注意力机制</b></font></center>

<hr>
<h1 id="1-Encoder-Decoder-框架"><a href="#1-Encoder-Decoder-框架" class="headerlink" title="1  Encoder-Decoder 框架"></a>1  Encoder-Decoder 框架</h1><p>目前大多数注意力模型附着在 Encoder-Decoder 框架下。下图是 NLP 处理中常用的 Encoder-Decoder 框架：</p>
<center>
    <img src="/summary/Attention%20Mechanism/1 Encoder-Decoder框架.png" weight="600" height="300">
    <div>
        图-1 Encoder-Decoder 框架
    </div>
</center>

<p>可以简单理解为处理一个语句（篇章）生成另一个语句（篇章）的模型：对于句子 $<Source, target>$，目标是输入句子 $Source$，期待通过 Encoder-Decoder 框架生成目标句子 $Target$，$Source$ 和 $Target$ 的单词序列为：</Source,></p>
<script type="math/tex; mode=display">
Source=<x_1,x_2,...,x_m>\\
Target=<y_1,y_2,...,y_n></script><p>编码器 Encoder 通过对句子 $Source$ 进行编码，将输入的句子通过非线性变换转化为中间语义编码</p>
<script type="math/tex; mode=display">
C=\mathcal{F}(x_1,x_2,...,x_m)</script><p>解码器 Decoder 将中间语义编码 $C$ 和之前已经产生的历史输出单词序列 $y_1,y_2,…,y_{i-1}$ 来生成第 $i$ 步的单词</p>
<script type="math/tex; mode=display">
y_i=\mathcal{G}(C,y_1,y_2,...,y_{i-1})</script><p>若 $Source$ 和 $Target$ 是不同语言的句子，那么是机器翻译 Encoder-Decoder 框架；若 $Source$ 是一篇文章，而 $Target$ 是文章的概括性描述，那么是文本摘要 Encoder-Decoder 框架；若 $Source$ 是一句问句，而 $Target$ 是问句的回答，那么是问答系统或对话机器人 Encoder-Decoder 框架。</p>
<p>除 NLP 领域外，Encoder-Decoder 框架在其他领域也被广泛应用，如语音识别领域，Encoder 的输入是语音流，而 Decoder 的输出为语音所对应的文本；在 CV 领域，Encoder 的输入是一帧图片，Decoder 的输出是描述图片语义特征的描述语。通常 NLP 和语音识别时 Encoder 采用 RNN，而 CV 则采用 CNN。</p>
<h1 id="2-经典-Attention-模型（Soft-Attention）"><a href="#2-经典-Attention-模型（Soft-Attention）" class="headerlink" title="2  经典 Attention 模型（Soft-Attention）"></a>2  经典 Attention 模型（Soft-Attention）</h1><h2 id="2-1-Attention-的引入"><a href="#2-1-Attention-的引入" class="headerlink" title="2.1  Attention 的引入"></a>2.1  Attention 的引入</h2><p>图-1 中的 Encoder-Decoder 框架没有体现出 “注意力机制”，因此可以将其看作注意力不集中的分心模型。为什么称其注意力不集中？先观察下面 $Target$ 中每个单词的生成过程：</p>
<script type="math/tex; mode=display">
\begin{aligned}
y_1 &= \mathcal{G}(C) \\
y_2 &= \mathcal{G}(C, y_1) \\
y_3 &= \mathcal{G}(C, y_1, y_2) \\
...
\end{aligned}</script><p>式中 $\mathcal{G}$ 为 Decoder 的非线性变化函数。从上面的生成过程可以看出，不论生成哪一个单词，所使用的输入句子 $Source$ 的语义编码 $C$ 是相同的。需要注意的是 $C$ 由 $Sourc$ 的每个单词经 Encoder 编码生成，这也意味着，对于每一个 $Target$ 中的单词 $y_i$ 来说，输入句子 $Source$ 中的每一个单词对其生成都有着相同的影响力，因此称这个模型注意力不集中。</p>
<p><b><font face="楷体" color="#800000" size="4"><br>例：对于机器翻译，输入句子 “Tom chase Jerry”，Encoder-Decoder 逐步生成中文单词：“汤姆”、“追逐”、“杰瑞”。在翻译 “杰瑞” 时，分心模型里所有输入英文单词对目标单词 “杰瑞” 的影响是相同的，这显然不合理，因为 “Jerry” 翻译为 “杰瑞” 更加合理，而分心模型无法体现这一点。
</font></b></p>
<p>不引入注意力机制，在输入短句子时可能没有明显的问题，但是对于长难句来说，输入句子的语义信息仅通过一个中间语义编码向量来表示，单词本身的信息大量丢失，极大影响输出结果的正确性，因此，这就是引入注意力机制的重要原因。</p>
<p>再用上面的翻译例子，引入 Attention 后，在翻译 “杰瑞” 时，将体现出不同英文单词对于翻译当前中文单词不同的影响程度，例如如下的概率分布：</p>
<script type="math/tex; mode=display">
(\mathrm{Tom}, 0.3),\ (\mathrm{Chase}, 0.2),\ (\mathrm{Jerry}, 0.5)</script><p>每个英文单词的概率表示翻译 “杰瑞” 时，注意力分配给不同英文单词的注意力大小，这引入了新的信息，将有助于正确翻译目标单词。</p>
<p>同理，$Target$ 中的每个单词都应学到其对应的 $Source$ 中单词的注意力概率分布。这也就意味着，在生成单词 $y_i$ 时，前面所使用的相同的中间语义编码 $C$ 被替换为根据当前生成单词而不断变化的 $C_i$ ，如下图所示：</p>
<center>
    <img src="/summary/Attention%20Mechanism/2 注意力Encoder-Decoder框架.png">
    <br>
    <div>
        图-2 引入注意力机制的 Encoder-Decoder 框架
    </div>
</center>

<p>即生成 $Target$ 的过程变为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
y_1 &= \mathcal{G}_1 (C_1) \\
y_2 &= \mathcal{G}_2 (C_2, y_1) \\
y_3 &= \mathcal{G}_3 (C_3, y_1, y_2)
\end{aligned}</script><p>每个 $C_i$ 可能对应不同的 $Source$ 中单词的注意力概率分布，以前面的翻译例子，其对应的信息可能如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
C_{\mathrm{汤姆}} &= \mathcal{g}(0.6*\mathcal{f}(\mathrm{Tom}),\ 0.2*\mathcal{f}(\mathrm{Chase}),\ 0.2*\mathcal{f}(\mathrm{Jerry})) \\
C_{\mathrm{追逐}} &= \mathcal{g}(0.2*\mathcal{f}(\mathrm{Tom}),\ 0.7*\mathcal{f}(\mathrm{Chase}),\ 0.1*\mathcal{f}(\mathrm{Jerry})) \\
C_{\mathrm{杰瑞}} &= \mathcal{g}(0.3*\mathcal{f}(\mathrm{Tom}),\ 0.2*\mathcal{f}(\mathrm{Chase}),\ 0.5*\mathcal{f}(\mathrm{Jerry}))
\end{aligned}</script><p>其中，$\mathcal{f}$ 表示 Encoder 对输入英文单词的某种变换函数，例如：如果 Encoder 使用 RNN 模型，那么 $\mathcal{f}$ 的函数结果往往是某时刻输入 $x_i$ 后隐藏节点的状态值；$\mathcal{g}$ 表示 Encoder 根据单词的中间编码合成整个句子中间语义编码的变换函数，一般，$\mathcal{g}$ 函数是对构成元素的加权求和，即下式：</p>
<script type="math/tex; mode=display">
C_i = \sum_{j=1}^{L_x} a_{ij}h_j</script><p>其中：$L_x$ 表示句子 $Source$ 的长度；$a_{ij}$ 表示在 $Target$ 输出第 $i$ 个单词时 $Source$ 句子中第 $j$ 个单词的注意力系数分配；$h_j$ 表示 $Source$ 句子中第 $j$ 个单词的语义编码。以前面的例子为例，则有 $L_x = 3$，$h_1 = \mathcal{f}(\mathrm{Tom})$，$h_2 = \mathcal{f}(\mathrm{Chase})$，$h_3 = \mathcal{f}(\mathrm{Jerry})$ 分别是 $Source$ 中每个单词的语义编码，对于编码 $C_1$ 而言，权重为 $a_{11} = 0.6$，$a_{12}=0.2$，$a_{13}=0.2$，其形成过程如下图：</p>
<center>
    <img src="/summary/Attention%20Mechanism/3 Attention在翻译时的形成过程示例.png" weight="400" height="400">
    <br>
    <div>
        图-3 Attention 在翻译时的形成过程示意图
    </div>
</center>

<p>这个过程还存在一个问题，即在形成 Attention 时，如何得到 $Source$ 单词的注意力概率分布，例如翻译 “杰瑞” 时如何得到概率分布 $(\mathrm{Tom}, 0.3)$，$(\mathrm{Chase}, 0.2)$，$(\mathrm{Jerry}, 0.5)$？</p>
<h2 id="2-2-Attention-中注意力分布的生成"><a href="#2-2-Attention-中注意力分布的生成" class="headerlink" title="2.2  Attention 中注意力分布的生成"></a>2.2  Attention 中注意力分布的生成</h2><p>先对图-1 中非 Attention 的 Encoder-Decoder 框架细化，Encoder 和 Decoder 采用 RNN 模型，得到下面的框架图：</p>
<center>
    <img src="/summary/Attention%20Mechanism/4 使用RNN细化后的Encoder-Decoder框架.png">
    <br>
    <div>
        图-4 使用 RNN 细化后的 Encoder-Decoder 框架
    </div>
</center>

<p>用下图说明注意力概率分布的通用计算过程：</p>
<center>
    <img src="/summary/Attention%20Mechanism/5 注意力概率分布的计算过程.png" height="400" width="600">
    <br>
    <div>
        图-5 注意力概率分布的计算过程
    </div>
</center>

<p>对于使用 RNN 的 Decoder 而言，在时刻 $i$ 若要生成单词 $y_i$，由于可以得知时刻 $i-1$ 的隐藏层节点 $H_{i-1}$ 的值，那么可以把 $H_{i-1}$ 和 $Source$ 中每个单词对应的隐藏层节点 $h_j$ 分别计算，即通过函数 $F(h_j,H_{i-1})$ 获得目标单词 $y_i$ 和每个输入单词的对齐可能性（$F$ 在不同模型中使用的方法不同），然后函数 $F$ 的输出经过 Softmax 进行归一化后，即得到符合概率分布的注意力概率分布。图-6 中可视化的是英语-德语翻译系统加入 Attention 机制后，$Source$ 和 $Target$ 中单词对应的概率分布：</p>
<center>
    <img src="/summary/Attention%20Mechanism/6 英语-德语翻译中Attention的概率分布.png" height="400" width="400">
    <br>
    <div>
        图-6 英语-德语翻译中 Attention 的概率分布
    </div>
</center>



<h2 id="2-3-Attention-机制的本质"><a href="#2-3-Attention-机制的本质" class="headerlink" title="2.3  Attention 机制的本质"></a>2.3  Attention 机制的本质</h2><p>将 Attention 机制从 Encoder-Decoder 框架中剥离，并进一步抽象，可以更简单地看出 Attention 机制的本质。</p>
<center>
    <img src="/summary/Attention%20Mechanism/7 Attention机制的本质思想.png">
    <br>
    <div>
        图-7 Attention 机制的本质思想
    </div>
</center>

<p>以图-7 为例：将 $Source$ 中的构成元素抽象为一些列的键值对数据 $<key, value>$，此时，对于给定 $Target$ 中的某个元素 $query$，通过计算 $query$ 和各个 $key$ 的相似性或相关性，得到每个 $key$ 对应 $value$ 的权重系数，然后对 $value$ 进行加权求和，即得到最终的 Attention 数值。因此本质上，Attention 机制是对 $Source$ 中元素的 $value$ 值进行加权求和，而 $query$ 和 $key$ 用来计算对应 $value$ 的权重系数。即可以将其本质思想改写为如下公式：</key,></p>
<script type="math/tex; mode=display">
\mathrm{Attention}(query, Source) = \sum_{i=1}^{L_x} \mathrm{Similarity}(query, key_i) * value_i</script><p>其中，$L_x=||Source||$。在前面机器翻译的例子里计算 Attention 的过程中，$Source$ 中的 $key$ 和 $value$ 指向的则是同一个数据，即输入句子中单词的语义编码。</p>
<p>从图-7 可以引出另外一种理解，将 Attention 机制看作一种软寻址（Soft Addressing）：$Source$ 可以看作存储器内存储的内容，元素由地址 $key$ 和值 $value$ 组成，当前有个 $key=query$ 的查询，目的是取出存储器中对应的 $value$ 值，即 Attention 数值。通过 $query$ 和存储器内元素的地址 $key$ 进行相似性比较寻址。之所以成为<strong>软寻址</strong>，指的不同一般寻址从存储中找出取出一条数据，而是可能从每个 $key$ 地址都会取出内容，取出内容的重要性根据 $query$ 和 $key$ 的相似性决定，之后对 $value$ 进行加权求和，从而得到最终的 $value$ 值，即 Attention 值。</p>
<p>对于 Attention 机制的具体计算过程，对目前大多数方法进行抽象，可以归纳为两个过程三个阶段：</p>
<ul>
<li><p>过程 1：根据 $query$ 和 $key$ 计算权重系数</p>
<ul>
<li>阶段①：根据 $query$ 和 $key$ 计算两者的相似性或相关性</li>
<li>阶段②：对阶段①的原始分值进行归一化处理</li>
</ul>
</li>
<li><p>过程 2（阶段③）：根据权重系数对 $value$ 进行加权求和</p>
</li>
</ul>
<p>上面的过程可以得到图-8 中的结果：</p>
<center>
    <img src="/summary/Attention%20Mechanism/8 计算Attention的三个阶段.png">
    <br>
    <div>
        图-8 计算 Attention 的三个阶段
    </div>
</center>

<p>在阶段①，可以使用不同的函数和计算方法，根据 $query(=q)$ 和 $key_i(=k_i)$，计算两者的相似性或相关性，常见的方法如下（式中 $W$、$U$ 和 $v$ 为可学习的参数矩阵或向量，$D$ 为输入向量的维度）：</p>
<ul>
<li><p>点积</p>
<script type="math/tex; mode=display">
s_i = \mathrm{Similarity}(query, key_i) = query \cdot key_i = q^T k_i</script></li>
<li><p>缩放点积</p>
<script type="math/tex; mode=display">
s_i = \mathrm{Similarity}(query, key_i) = \frac{q^T k_i}{\sqrt{D}}</script></li>
<li><p>余弦（cosine）相似性</p>
<script type="math/tex; mode=display">
s_i = \mathrm{Similarity}(query, key_i) = \frac{query \cdot key_i}{||query|| \cdot ||key_i||}</script></li>
<li><p>MLP 网络：$s_i = \mathrm{Similarity}(query, key_i) = \mathrm{MLP}(query, key_i)$</p>
<ul>
<li>加性模型：$s_i = \mathrm{Similarity}(query, key_i) = v^T \tanh (Wk_i+Uq)$</li>
<li>双线性模型：$s_i = \mathrm{Similarity}(query, key_i) = k_i^T W q$，可以重塑为 $s_i = \mathrm{Similarity}(query, key_i) = k_i^T (U^T V) q = (U k_i)^T (Vq)$，即对 $query$ 和 $key$ 进行线性变换后，再计算点积。</li>
</ul>
</li>
</ul>
<p>阶段②引入类似 Softmax 的计算，对阶段①的结果一方面进行归一化，另一方面也突出重要元素的权重：</p>
<script type="math/tex; mode=display">
a_i = \mathrm{Softmax}(s_i) = \frac{\exp(s_i)}{\sum_{j=1}^{L_x}\exp(s_j)}</script><p>然后加权得到 Attention 值：</p>
<script type="math/tex; mode=display">
\mathrm{Attention}(query, Source) = \sum_{i=1}^{L_x} a_i \cdot value_i</script><p>目前大多数的注意力机制都符合上述的三阶段的计算过程。</p>
<h1 id="3-Attention-注意力机制的变体"><a href="#3-Attention-注意力机制的变体" class="headerlink" title="3  Attention 注意力机制的变体"></a>3  Attention 注意力机制的变体</h1><h2 id="3-1-硬性注意力（Hard-Attention）"><a href="#3-1-硬性注意力（Hard-Attention）" class="headerlink" title="3.1  硬性注意力（Hard Attention）"></a>3.1  硬性注意力（Hard Attention）</h2><p>软性注意力通过注意力分布加权求和来融合输入向量。而<strong>硬性注意力</strong>则不采用这种方式，它根据注意力分布选择输入向量中的一个作为输出：</p>
<script type="math/tex; mode=display">
\mathrm{Attention}(query, Source) = value_{i*} \\
\mathrm{where}\ i* = \mathrm{ArgCondition}(a_i\ \mathrm{or}\ s_i) \\
i*\ 即满足\ \mathrm{Condition}\ 条件的\ a_i\ 或\ s_i\ 的索引</script><p>这种选择有两种选择方式：</p>
<ul>
<li>选择注意力分布中，分数（概率）最大的一项所对应的输入向量作为 Attention 的输出。</li>
<li>根据注意力分布进行随机采样，采样结果作为 Attention 的输出。</li>
</ul>
<p>硬性注意力通过上面的方式选择 Attention 的输出，会使最终的损失函数与注意力分布之间的函数关系不可导，从而无法使用反向传播算法训练模型，硬性注意力通常需要使用强化学习来进行训练。一般深度学习算法会使用软性注意力的方式进行计算。</p>
<h2 id="3-2-多头注意力（Multi-Head-Attention）"><a href="#3-2-多头注意力（Multi-Head-Attention）" class="headerlink" title="3.2  多头注意力（Multi-Head Attention）"></a>3.2  多头注意力（Multi-Head Attention）</h2><p>多头注意力机制是利用多个查询向量 $Q = [q_1, q_2, … , q_m]$，并行地从输入信息 $<key, value>$ 或 $(K,V)=[(k_1,v_1),(k_2,v_2),…,(k_n,v_n)]$ 中选取多组信息。在查询过程中，每个查询向量 $q_i$ 将会关注输入信息的不同部分。</key,></p>
<p>假设 $a_{ij}$ 表示第 $i$ 个查询向量 $q_i$ 与第 $j$ 个输入信息 $k_j$ 的注意力权重，$s(\cdot)=\mathrm{Similarity}(\cdot)$，$context_i$ 表示由查询向量 $q_i$ 计算得出的 Attention 输出向量，其计算方式为：</p>
<script type="math/tex; mode=display">
a_{ij} = \mathrm{Softmax}(s(q_i,k_j)) = \frac{\exp(s(q_i,k_j))}{\sum_{t=1}^n \exp(s(q_i,k_t))} \\
context_i = \sum_{j=1}^n a_{ij} \cdot v_j</script><p>最终将所有查询向量的计算结果进行拼接得到最终结果（$\oplus$ 表示向量拼接操作）：</p>
<script type="math/tex; mode=display">
\mathrm{Attention} = context_1 \oplus context_2 \oplus \cdots \oplus context_m</script><h1 id="4-自注意力机制（Self-Attention）"><a href="#4-自注意力机制（Self-Attention）" class="headerlink" title="4  自注意力机制（Self-Attention）"></a>4  自注意力机制（Self-Attention）</h1><p>Self-Attention 也被称为 Intra-Attention（内部 Attention）。在一般 Encoder-Decoder 框架中，输入 $Source$ 和输出 $Target$ 内容是不相同的，Attention 机制发生在 $Target$ 的元素 $query$ 和 $Source$ 中的所有元素之间。Self-Attention 不是 $Target$ 和 $Source$ 之间的 Attention 机制，而是 $Source$ 内部元素之间或者 $Target$ 内部元素之间发生的 Attention 机制，也可以理解为 $Target=Source$ 这种特殊情况下的注意力计算机制。</p>
<p>在前面所介绍的 Attention 机制中，会使用一个查询向量 $q$ 和对应的输入 $H=[h_1,h_2,…,h_n]$ 进行计算，查询向量 $q$ 则和任务相关，例如 Encoder-Decoder 框架中，$q$ 可以是 Decoder 端前一时刻的<strong>输出状态向量</strong>。而<strong>在 Self-Attention 中，查询向量也可以使用输入信息生成</strong>，而非任务相关的向量。即模型读到输入信息后，根据输入信息本身决定当前的重要信息。</p>
<p>自注意力机制往往采用 <strong>Query-Key-Value</strong> 的模式，以 BERT 重点自注意力机制为例，展开下面的讨论，如图-9 所示：</p>
<center>
    <img src="/summary/Attention%20Mechanism/9 自注意力机制的计算过程.jpg" width="640" height="400">
    <br>
    <div>
        图-9 自注意力机制的计算过程
    </div>
</center>

<p>图-9 中，输入信息 $H=[h_1,h_2]$，蓝色矩阵中每行表示一个输入向量，三个矩阵 $W_q$，$W_k$，$W_v$ 将输入信息 $H$ 以此转换到对应的查询空间 $Q=[q_1,q_2]$，键空间 $K=[k_1,k_2]$ 和值空间 $V=[v_1,v_2]$：</p>
<script type="math/tex; mode=display">
\begin{aligned}
Q &= [q_1=h_1 W_q,\ q_2=h_2 W_q] &\Rightarrow Q = HW_q \\
K &= [k_1=h_1 W_k,\ k_2=h_2 W_k] &\Rightarrow K = HW_k \\
V &= [v_1=h_1 W_v,\ v_2=h_2 W_v] &\Rightarrow V = HW_v
\end{aligned}</script><p>不妨以 $h_1$ 为例计算这个位置的 Attention 输出向量 $context_1$，如图-10 所示，其中 $D_k$ 表示 $key$ 向量的维度（$query$ 向量、$key$ 向量和 $value$ 向量的维度相同）：</p>
<center>
    <img src="/summary/Attention%20Mechanism/10 自注意力机制的详细计算过程.jpg" width="600" height="600">
    <br>
    <div>
        图-10 自注意力机制的详细计算过程
    </div>
</center>

<p>在获得输入信息 $H$ 在不同空间的表达 $Q$、$K$ 和 $V$ 后，计算 $q_1$ 在 $h_1$ 和 $h_2$ 的分数（相似性或相关性） $s_{11}$ 和 $s_{12}$。然后用 Softmax 进行归一化，获得在 $h_1$ 这个位置的注意力分布 $a_{11}$ 和 $a_{12}$，代表了模型当前在 $h_1$ 这个位置需要对输入信息 $h_1$ 和 $h_2$ 的关注程度。最后，根据该位置的注意力分布对 $v_1$ 和 $v_2$ 进行加权平均获得 $h_1$ 位置的 Attention 向量 $context_1$。</p>
<p>同理，对于输入信息 $H=[h_1,h_2,…,h_n]$，可以得到每个位置的 Attention 向量  $\mathrm{Attention}=[context_1,context_2,…,context_n]$。</p>
<p>整个 Self-Attention 的计算过程的矩阵形式为：</p>
<script type="math/tex; mode=display">
\mathrm{Attention} = \mathrm{Softmax} (\frac{QK^T}{\sqrt{D_k}})V</script><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/jins-note/p/13056604.html">https://www.cnblogs.com/jins-note/p/13056604.html</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/393940472">https://zhuanlan.zhihu.com/p/393940472</a></li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Summary/" rel="tag"># Summary</a>
              <a href="/tags/Knowledge/" rel="tag"># Knowledge</a>
              <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/summary/Anchor/" rel="prev" title="Anchor">
                  <i class="fa fa-chevron-left"></i> Anchor
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/collection/Awesome%20Semantic%20SLAM/" rel="next" title="Awesome Semantic SLAM">
                  Awesome Semantic SLAM <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Star Rain</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"cdn":"//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML","tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
