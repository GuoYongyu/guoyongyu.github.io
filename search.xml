<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Anchor</title>
    <url>/summary/Anchor/</url>
    <content><![CDATA[<p>计算机视觉，尤其目标检测相关算法中，anchor 的介绍与用途。</p>
<span id="more"></span>
<center><b><font size="9">Anchor</font></b></center>

<p>anchor 来源于 Faster R-CNN 中提到的在目标检测时的一种思想。在 Faster R-CNN 中，对于输入图像，先被缩放为 $800\times600$ 大小的图像 A，输入 backbone 中经 $1/16$ 下采样得到 feature map，大小为 $\lceil 800/16 \rceil \times \lceil 600/16 \rceil = 50 \times 38$，feature map 中的一个小方格就对应原图像 A 中 $16\times16$ 的像素框。</p>
<p>在 Faster R-CNN 中，定义了三种 scale 和 ratio：</p>
<ul>
<li>scale 为缩放比，取 $[8,16,32]$</li>
<li>ratio 为宽高比，取 $[1:2,1:1,2:1]$</li>
</ul>
<p>在特征图中给每个点取 anchor 时，先定义 $base_anchor$，对应特征图的一个点，即原图像 $16\times16$ 的像素。先考虑 ratio 的影响，根据三个 ratio，保持 $base_anchor$ 的面积不变，可以得到三种变形的 $base_anchor$，大小对应原图像为 $12\times24$ 和 $24\times12$（结果向上取整）。然后将 $base_anchor$ 的宽和高乘以 scale，即可得到在原图像上取的 anchor。</p>
]]></content>
      <categories>
        <category>Summary</category>
      </categories>
      <tags>
        <tag>Summary</tag>
        <tag>Deep Learning</tag>
        <tag>Knowledge</tag>
      </tags>
  </entry>
  <entry>
    <title>Activation Function</title>
    <url>/summary/Activation%20Function/</url>
    <content><![CDATA[<p>关于深度学习中激活函数的简单总结。</p>
<span id="more"></span>
<center><font size="9"><b>激活函数</b></font></center>

<hr>
<h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1  简介"></a>1  简介</h1><p>激活函数是向神经网络中引入非线性因素，通过激活函数神经网络就可以拟合各种曲线。激活函数主要分为饱和激活函数（Saturated Neurons）和非饱和函数（One-sided Saturations）。</p>
<h1 id="2-Sigmoid-函数"><a href="#2-Sigmoid-函数" class="headerlink" title="2  Sigmoid 函数"></a>2  Sigmoid 函数</h1><p>数学形式：</p>
<script type="math/tex; mode=display">
\mathrm{sigmoid}(x) = \frac{1}{1 + \mathrm{e}^{-x}}</script><p>特点：</p>
<ul>
<li>能够把输入的连续实值变换为 $0$ 和 $1$ 之间的输出。特别的，如果是非常大的负数，那么输出就是 $0$；如果是非常大的正数，输出就是 $1$。</li>
<li>缺点 1：在深度神经网络中梯度反向传递时导致梯度爆炸和梯度消失。</li>
<li>缺点 2：Sigmoid 的 output 不是 0 均值（zero-centered）。</li>
</ul>
<h1 id="3-tanh-函数"><a href="#3-tanh-函数" class="headerlink" title="3  tanh 函数"></a>3  tanh 函数</h1><p>数学形式：</p>
<script type="math/tex; mode=display">
\mathrm{tanh} (x) = \frac{\mathrm{e}^x - \mathrm{e}^{-x}}{\mathrm{e}^x + \mathrm{e}^{-x}}</script><p>特点：</p>
<ul>
<li>解决了 Sigmoid 函数的不是 zero-centered 的输出问题。</li>
</ul>
<h1 id="4-ReLU-函数"><a href="#4-ReLU-函数" class="headerlink" title="4  ReLU 函数"></a>4  ReLU 函数</h1><p>数学形式：</p>
<script type="math/tex; mode=display">
\mathrm{ReLU} (x) = \max (0, x)</script><p>特点：</p>
<ul>
<li><p>优点：</p>
<ul>
<li>解决了梯度消亡问题</li>
<li>计算速度非常快</li>
<li>收敛速度远快于 Sigmoid 和 tanh</li>
</ul>
</li>
<li><p>缺点：</p>
<ul>
<li>ReLU 的输出不是 zero-centered</li>
<li>Dead ReLU 问题，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新</li>
</ul>
</li>
</ul>
<h1 id="5-Leaky-ReLU函数"><a href="#5-Leaky-ReLU函数" class="headerlink" title="5  Leaky ReLU函数"></a>5  Leaky ReLU函数</h1><p>数学形式：</p>
<script type="math/tex; mode=display">
\mathrm{LeakyReLU} (x) = \max (\alpha x, x)</script><p>特点：</p>
<ul>
<li>为解决 Dead ReLU 问题，提出了将 ReLU 的前半段设为 $\alpha x$，通常 $\alpha=0.01$。</li>
<li>理论上来讲，Leaky ReLU 有 ReLU 的所有优点，且没有 Dead ReLU 问题，但在实际中，并没有完全证明 Leaky ReLU 总是优于 ReLU。</li>
<li>PReLU 函数与 Leaky ReLU 函数相类似</li>
</ul>
<h1 id="6-ELU（Exponential-Linear-Units）函数"><a href="#6-ELU（Exponential-Linear-Units）函数" class="headerlink" title="6  ELU（Exponential Linear Units）函数"></a>6  ELU（Exponential Linear Units）函数</h1><p>数学形式：</p>
<script type="math/tex; mode=display">
\mathrm{ELU} (x) = 
\begin{cases}
x, & \mathrm{if} \ x > 0\\
\alpha (\mathrm{e}^x - 1), & \mathrm{otherwise}
\end{cases}</script><p>特点：</p>
<ul>
<li>ELU 也是为解决 ReLU 存在的问题而提出，基本有 ReLU 的所有优点，没有 Dead ReLU 问题且输出接近 0</li>
<li>问题是计算开销太大</li>
</ul>
<h1 id="7-Softmax-函数"><a href="#7-Softmax-函数" class="headerlink" title="7  Softmax 函数"></a>7  Softmax 函数</h1><p>数学形式：</p>
<script type="math/tex; mode=display">
\mathrm{softmax}: \bold{x} \in \mathbb{R}^n \rightarrow \bold{x'} \in \mathbb{R}^n\\
\mathrm{s.t.}\ \forall x'_i \in \bold{x'},\ x'_i = \frac{\mathrm{e}^{x_i}}{\sum_{j=1}^n \mathrm{e}^{x_j}}</script><p>特点：</p>
<ul>
<li>在零点不可微</li>
<li>负输入的梯度为零，这意味着对于该区域的激活，权重不会在反向传播期间更新，因此会产生永不激活的死亡神经元</li>
</ul>
<h1 id="8-Swish-函数"><a href="#8-Swish-函数" class="headerlink" title="8  Swish 函数"></a>8  Swish 函数</h1><p>数学形式：</p>
<script type="math/tex; mode=display">
\mathrm{swish} (x) = x \cdot \mathrm{sigmoid} (x)</script><p>特点：</p>
<ul>
<li><strong>无界性</strong>有助于防止慢速训练期间，梯度逐渐接近 0 并导致饱和</li>
<li>导数恒大于 0</li>
</ul>
<h1 id="9-Softplus-函数"><a href="#9-Softplus-函数" class="headerlink" title="9  Softplus 函数"></a>9  Softplus 函数</h1><p>数学形式：</p>
<script type="math/tex; mode=display">
\mathrm{softplus} (x) = \ln (1 + \mathrm{e}^x)</script><p>特点：</p>
<ul>
<li>类似于 ReLU 函数，但是相对较平滑</li>
</ul>
]]></content>
      <categories>
        <category>Summary</category>
      </categories>
      <tags>
        <tag>Summary</tag>
        <tag>Deep Learning</tag>
        <tag>Knowledge</tag>
      </tags>
  </entry>
  <entry>
    <title>Awesome Semantic SLAM</title>
    <url>/collection/Awesome%20Semantic%20SLAM/</url>
    <content><![CDATA[<p>A collection note about papers in the field of semantic SLAM.</p>
<p>Continuously updating…</p>
<span id="more"></span>
<center><b><font size="9">Awesome Semantic SLAM</font></b></center>

<h1 id="0-Preface"><a href="#0-Preface" class="headerlink" title="0  Preface"></a>0  Preface</h1><ul>
<li>All of the data is from Internet or webpages, so I can’t make sure that all of the information is right. If there are some errors, please leave your advice and evidence in <code>issue</code>, I will revise the document as immediately as possible, which is based on your <strong>right</strong> or <strong>convincing</strong> advice and evidence.</li>
<li>It is not easy and troublesome to organize the data. Please give me a <code>star</code> if you like it.</li>
</ul>
<h1 id="1-Papers-and-Codes"><a href="#1-Papers-and-Codes" class="headerlink" title="1  Papers and Codes"></a>1  Papers and Codes</h1><h2 id="1-1-C-SLAM"><a href="#1-1-C-SLAM" class="headerlink" title="1.1  C-SLAM"></a>1.1  C-SLAM</h2><p>Paper: <a href="https://arxiv.org/abs/2206.10263">Object Structural Points Representation for Graph-based Semantic Monocular Localization and Mapping</a></p>
<p>Codes: <a href="https://github.com/airlab-polimi/c-slam">https://github.com/airlab-polimi/c-slam</a></p>
<p>Conference/Journal: arXiv: 2206.10263</p>
<h2 id="1-2-DSP-SLAM"><a href="#1-2-DSP-SLAM" class="headerlink" title="1.2  DSP-SLAM"></a>1.2  DSP-SLAM</h2><p>Paper: <a href="https://arxiv.org/abs/2108.09481">DSP-SLAM: Object Oriented SLAM with Deep Shape Priors</a></p>
<p>Codes: <a href="https://github.com/JingwenWang95/DSP-SLAM">https://github.com/JingwenWang95/DSP-SLAM</a></p>
<p>Conference/Journal: 3DV, 2021</p>
<h2 id="1-3-SLOAM"><a href="#1-3-SLOAM" class="headerlink" title="1.3  SLOAM"></a>1.3  SLOAM</h2><p>Paper: <a href="https://ieeexplore.ieee.org/document/8949363">SLOAM: Semantic Lidar Odometry and Mapping in Forests</a></p>
<p>Codes: <a href="https://github.com/KumarRobotics/sloam">https://github.com/KumarRobotics/sloam</a></p>
<p>Conference/Journal: LRA, 2020</p>
<h2 id="1-4-SimVODIS"><a href="#1-4-SimVODIS" class="headerlink" title="1.4  SimVODIS"></a>1.4  SimVODIS</h2><p>Paper: <a href="https://arxiv.org/abs/1911.05939">Simvodis: Simultaneous visual odometry, object detection, and instance segmentation</a></p>
<p>Codes: <a href="https://github.com/Uehwan/SimVODIS">https://github.com/Uehwan/SimVODIS</a></p>
<p>Conference/Journal: PAMI, 2020</p>
<h2 id="1-5-SALSA"><a href="#1-5-SALSA" class="headerlink" title="1.5  SALSA"></a>1.5  SALSA</h2><p>Paper: <a href="https://github.com/heethesh/SALSA-Semantic-Assisted-SLAM/blob/master/docs/report.pdf">SALSA: Semantic Assisted Life-Long SLAM for Indoor Environments</a></p>
<p>Codes: <a href="https://github.com/heethesh/SALSA-Semantic-Assisted-SLAM">https://github.com/heethesh/SALSA-Semantic-Assisted-SLAM</a></p>
<p>Conference/Journal: Unknown</p>
<h2 id="1-6-object-map"><a href="#1-6-object-map" class="headerlink" title="1.6  object-map"></a>1.6  object-map</h2><p>Paper: <a href="https://ieeexplore.ieee.org/document/9190524">Representing and updating objects’ identities in semantic SLAM</a></p>
<p>Codes: <a href="https://github.com/or-tal-robotics/object_map">https://github.com/or-tal-robotics/object_map</a></p>
<p>Conference/Journal: FUSION, 2020</p>
<h2 id="1-7-Kimera"><a href="#1-7-Kimera" class="headerlink" title="1.7  Kimera"></a>1.7  Kimera</h2><p>Paper 1: <a href="https://arxiv.org/abs/1903.01067">Incremental Visual-Inertial 3D Mesh Generation with Structural Regularities</a></p>
<p>Conference/Journal: ICRA, 2019</p>
<p>Paper 2: <a href="https://arxiv.org/abs/1910.02490">Kimera: an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping</a></p>
<p>Conference/Journal: ICRA, 2020</p>
<p>Paper 3: <a href="https://arxiv.org/abs/2002.06289">3D Dynamic Scene Graphs: Actionable Spatial Perception with Places, Objects, and Humans</a></p>
<p>Conference/Journal: arXiv:2002.06289</p>
<p>Paper 4: <a href="https://arxiv.org/abs/2101.06894">Kimera: from SLAM to Spatial Perception with 3D Dynamic Scene Graphs</a></p>
<p>Conference/Journal: IJRR, 2021</p>
<p>Codes: <a href="https://github.com/MIT-SPARK/Kimera">https://github.com/MIT-SPARK/Kimera</a></p>
<h2 id="1-8-SuMa"><a href="#1-8-SuMa" class="headerlink" title="1.8  SuMa++"></a>1.8  SuMa++</h2><p>Paper: <a href="https://arxiv.org/abs/2105.11320">SuMa++: Efficient LiDAR-based Semantic SLAM</a></p>
<p>Codes: <a href="https://github.com/PRBonn/semantic_suma">https://github.com/PRBonn/semantic_suma</a></p>
<p>Conference/Journal: IROS, 2019</p>
<h2 id="1-9-EAO-SLAM"><a href="#1-9-EAO-SLAM" class="headerlink" title="1.9  EAO- SLAM"></a>1.9  EAO- SLAM</h2><p>Paper: <a href="https://ieeexplore.ieee.org/abstract/document/9341757">EAO-SLAM: Monocular Semi-Dense Object SLAM Based on Ensemble Data Association</a></p>
<p>Codes: <a href="https://github.com/yanmin-wu/EAO-SLAM">https://github.com/yanmin-wu/EAO-SLAM</a></p>
<p>Conference/Journal: IROS, 2020</p>
<h2 id="1-10-RS-SLAM"><a href="#1-10-RS-SLAM" class="headerlink" title="1.10  RS-SLAM"></a>1.10  RS-SLAM</h2><p>Title: <a href="https://ieeexplore.ieee.org/document/9494338">RS-SLAM: A Robust Semantic SLAM in Dynamic Environments Based on RGB-D Sensor</a></p>
<p>Codes: <a href="https://github.com/rantengsky/RS-SLAM">https://github.com/rantengsky/RS-SLAM</a></p>
<p>Conference/Jourcal: IEEE Sensors Journal, 2021</p>
<h1 id="2-Open-source-Projects"><a href="#2-Open-source-Projects" class="headerlink" title="2  Open-source Projects"></a>2  Open-source Projects</h1><h2 id="2-1-VIDO-SLAM"><a href="#2-1-VIDO-SLAM" class="headerlink" title="2.1  VIDO-SLAM"></a>2.1  VIDO-SLAM</h2><p>Codes: <a href="https://github.com/bxh1/VIDO-SLAM">https://github.com/bxh1/VIDO-SLAM</a></p>
<h2 id="2-2-ORB-SLAM2-SSD-Semantic"><a href="#2-2-ORB-SLAM2-SSD-Semantic" class="headerlink" title="2.2  ORB-SLAM2 SSD Semantic"></a>2.2  ORB-SLAM2 SSD Semantic</h2><p>Codes: <a href="https://github.com/Ewenwan/ORB_SLAM2_SSD_Semantic">https://github.com/Ewenwan/ORB_SLAM2_SSD_Semantic</a></p>
<h2 id="2-3-Semantic-SLAM"><a href="#2-3-Semantic-SLAM" class="headerlink" title="2.3  Semantic-SLAM"></a>2.3  Semantic-SLAM</h2><p>Codes: <a href="https://github.com/floatlazer/semantic_slam">https://github.com/floatlazer/semantic_slam</a></p>
<h2 id="2-4-ORB-SLAM2-with-Semantic-Label"><a href="#2-4-ORB-SLAM2-with-Semantic-Label" class="headerlink" title="2.4  ORB-SLAM2 with Semantic Label"></a>2.4  ORB-SLAM2 with Semantic Label</h2><p>Codes: <a href="https://github.com/qixuxiang/orb-slam2_with_semantic_label">https://github.com/qixuxiang/orb-slam2_with_semantic_label</a></p>
<h1 id="3-Paper"><a href="#3-Paper" class="headerlink" title="3  Paper"></a>3  Paper</h1><h2 id="3-1-Object-Aware-SLAM"><a href="#3-1-Object-Aware-SLAM" class="headerlink" title="3.1  Object-Aware SLAM"></a>3.1  Object-Aware SLAM</h2><p>Title: <a href="https://ieeexplore.ieee.org/document/9829272">Object-Aware SLAM Based on Efficient Quadric Initialization and Joint Data Association</a></p>
<p>Conference/Journal: LRA, 2022</p>
<h2 id="3-2-CDSFusion"><a href="#3-2-CDSFusion" class="headerlink" title="3.2  CDSFusion"></a>3.2  CDSFusion</h2><p>Title: <a href="https://www.mdpi.com/2072-4292/14/4/979/htm">CDSFusion: Dense Semantic SLAM for Indoor Environment Using CPU Computing</a></p>
<p>Conference/Journal: Remote Sensing, 2022</p>
<h2 id="3-3-Kimera-Multi-T-RO"><a href="#3-3-Kimera-Multi-T-RO" class="headerlink" title="3.3  Kimera-Multi (T-RO)"></a>3.3  Kimera-Multi (T-RO)</h2><p>Title: <a href="https://ieeexplore.ieee.org/abstract/document/9686955">Kimera-Multi: Robust, Distributed, Dense Metric-Semantic SLAM for Multi-Robot Systems</a></p>
<p>Conference/Journal: T-RO, 2022</p>
<h2 id="3-4-Kimera-Multi-ICRA"><a href="#3-4-Kimera-Multi-ICRA" class="headerlink" title="3.4  Kimera-Multi (ICRA)"></a>3.4  Kimera-Multi (ICRA)</h2><p>Title: <a href="https://ieeexplore.ieee.org/abstract/document/9561090">Kimera-Multi: a System for Distributed Multi-Robot Metric-Semantic Simultaneous Localization and Mapping</a></p>
<p>Conference/Journal: ICRA, 2021</p>
<h2 id="3-5-Evaluating-the-Impact-of-Semantic-Segmentation-and-Pose-Estimation"><a href="#3-5-Evaluating-the-Impact-of-Semantic-Segmentation-and-Pose-Estimation" class="headerlink" title="3.5  Evaluating the Impact of Semantic Segmentation and Pose Estimation"></a>3.5  Evaluating the Impact of Semantic Segmentation and Pose Estimation</h2><p>Title: <a href="https://arxiv.org/abs/2109.07748">Evaluating the Impact of Semantic Segmentation and Pose Estimation on Dense Semantic SLAM</a></p>
<p>Conference/Journal: IROS, 2021</p>
<h2 id="3-6-SOLO-SLAM"><a href="#3-6-SOLO-SLAM" class="headerlink" title="3.6  SOLO-SLAM"></a>3.6  SOLO-SLAM</h2><p>Title: <a href="https://www.mdpi.com/1424-8220/22/18/6977">SOLO-SLAM: A Parallel Semantic SLAM Algorithm for Dynamic Scenes</a></p>
<p>Conference/Journal: Sensors, 2022</p>
<h2 id="3-7-DP-SLAM"><a href="#3-7-DP-SLAM" class="headerlink" title="3.7  DP-SLAM"></a>3.7  DP-SLAM</h2><p>Ttile: <a href="https://www.sciencedirect.com/science/article/pii/S0020025520311841">DP-SLAM A visual SLAM with moving probability towards dynamic environments</a></p>
<p>Conference/Journal: Information Sciences, 2021</p>
<h2 id="3-8-Dynamic-Dense-CRF-Inference"><a href="#3-8-Dynamic-Dense-CRF-Inference" class="headerlink" title="3.8  Dynamic Dense CRF Inference"></a>3.8  Dynamic Dense CRF Inference</h2><p>Title: <a href="https://www.sciencedirect.com/science/article/pii/S0031320322005039">Dynamic dense CRF inference for video segmentation and semantic SLAM</a></p>
<p>Conference/Journal: Pattern Recognition, 2023</p>
<h2 id="3-9-Blitz-SLAM"><a href="#3-9-Blitz-SLAM" class="headerlink" title="3.9  Blitz-SLAM"></a>3.9  Blitz-SLAM</h2><p>Title: <a href="https://www.sciencedirect.com/science/article/pii/S0031320321004064">Blitz-SLAM: A semantic SLAM in dynamic environments</a></p>
<p>Conference/Journal: Pattern Recognition, 2022</p>
<h2 id="3-10-YOLO-SLAM"><a href="#3-10-YOLO-SLAM" class="headerlink" title="3.10  YOLO-SLAM"></a>3.10  YOLO-SLAM</h2><p>Title: <a href="https://link.springer.com/article/10.1007/s00521-021-06764-3">YOLO-SLAM: A semantic SLAM system towards dynamic environment with geometric constraint</a></p>
<p>Conference/Journal: Neural Computing and Applications, 2022</p>
<h2 id="3-11-Hysteretic-Mapping-and-Corridor-Semantic-Modeling"><a href="#3-11-Hysteretic-Mapping-and-Corridor-Semantic-Modeling" class="headerlink" title="3.11  Hysteretic Mapping and Corridor Semantic Modeling"></a>3.11  Hysteretic Mapping and Corridor Semantic Modeling</h2><p>Title: <a href="https://www.sciencedirect.com/science/article/pii/S0924271622000491">Hysteretic mapping and corridor semantic modeling using mobile LiDAR systems</a></p>
<p>Conference/Journal: ISPRS Journal of Photogrammetry and Remote Sensing, 2022</p>
<h2 id="3-12-Dynamic-Uncertainty-Semantic-SLAM"><a href="#3-12-Dynamic-Uncertainty-Semantic-SLAM" class="headerlink" title="3.12  Dynamic Uncertainty Semantic SLAM"></a>3.12  Dynamic Uncertainty Semantic SLAM</h2><p>Title: <a href="https://www.sciencedirect.com/science/article/pii/S0952197622003608">Three-dimensional dynamic uncertainty semantic SLAM method for a production workshop</a></p>
<p>Conference/Journal: Engineering Applications of Artificial Intelligence, 2022</p>
<h1 id="4-Blogs"><a href="#4-Blogs" class="headerlink" title="4  Blogs"></a>4  Blogs</h1><h2 id="4-1-动态-SLAM-合集"><a href="#4-1-动态-SLAM-合集" class="headerlink" title="4.1  动态 SLAM 合集"></a>4.1  动态 SLAM 合集</h2><p>Link: <a href="https://zhuanlan.zhihu.com/p/499391239">https://zhuanlan.zhihu.com/p/499391239</a></p>
<h2 id="4-2-Recent-SLAM-Research-2021"><a href="#4-2-Recent-SLAM-Research-2021" class="headerlink" title="4.2  Recent SLAM Research 2021"></a>4.2  Recent SLAM Research 2021</h2><p>Link: <a href="https://github.com/YiChenCityU/Recent_SLAM_Research">https://github.com/YiChenCityU/Recent_SLAM_Research</a></p>
<h2 id="4-3-Visual-SLAM-Related-Research"><a href="#4-3-Visual-SLAM-Related-Research" class="headerlink" title="4.3  Visual SLAM Related Research"></a>4.3  Visual SLAM Related Research</h2><p>Link: <a href="https://github.com/wuxiaolang/Visual_SLAM_Related_Research">https://github.com/wuxiaolang/Visual_SLAM_Related_Research</a></p>
]]></content>
      <categories>
        <category>Paper Collection</category>
      </categories>
      <tags>
        <tag>Semantic SLAM</tag>
        <tag>Collection</tag>
      </tags>
  </entry>
  <entry>
    <title>ATE &amp; RPE</title>
    <url>/summary/ATE%20&amp;%20RPE/</url>
    <content><![CDATA[<p>在 SLAM 相关论文中，有几个常用的算法评价指标，ATE（绝对轨迹误差）和 RPE（相对位姿误差）即为常见的指标。本文收集并整理了网络上的内容，对二者进行了总结。</p>
<span id="more"></span>
<center><b><font size="9">ATE & RPE</font></b></center>

<h1 id="1-定义"><a href="#1-定义" class="headerlink" title="1  定义"></a>1  定义</h1><ul>
<li>算法估计位姿：$P_1, …, P_n \in SE(3)$</li>
<li>真实位姿：$Q_1, …, Q_n \in SE(3)$</li>
<li>下标 $t$ 表示时间或帧，假设估计位姿和真实位姿各帧时间已经对齐，总帧数均为 $n$，$\Delta$ 表示帧间间隔时间。</li>
</ul>
<h1 id="2-相对位姿误差"><a href="#2-相对位姿误差" class="headerlink" title="2  相对位姿误差"></a>2  相对位姿误差</h1><p>相对位姿误差（Relative Pose Error，RPE）描述的是固定时间差 $\Delta$ 下，两帧位姿差相比真实位姿差的精度，相当于直接测量里程计的误差。第 $i$ 帧的 RPE 定义如下：</p>
<script type="math/tex; mode=display">
RPE_i = E_i \triangleq (Q_i^{-1} Q_{i+\Delta})^{-1} (P_i^{-1} P_{i + \Delta})
\tag{1}</script><p>已知总数 $n$ 与间隔 $\Delta$ 的情况下，可以得到 $m = n - \Delta$ 个 RPE，然后可以用均方根误差 RMSE 统计这个误差，得到一个总体值：</p>
<script type="math/tex; mode=display">
RMSE(E_{1:n}, \Delta) := \left( \frac{1}{m} \sum_{i=1}^m \left\| \ln \left( (Q_i^{-1} Q_{i+\Delta})^{-1} (P_i^{-1} P_{i + \Delta}) \right)^{\vee} \right\|^2 \right)^{\frac{1}{2}}
\tag{2}</script><p>下式为平移部分的误差：</p>
<script type="math/tex; mode=display">
RMSE(E_{1:n}, \Delta) := \left( \frac{1}{m} \sum_{i=1}^m \| \operatorname{trans}(E_i) \|^2 \right)^{\frac{1}{2}}
\tag{3}</script><p>其中 $\operatorname{trans}(E_i)$ 代表取相对位姿误差中的平移部分。通常使用 RPE 的平移部分进行评价。如果有需要，也可以使用旋转部分，其中：</p>
<script type="math/tex; mode=display">
\operatorname{rot}(E_i) = \ln \left( ({}^Q R_i^{-1} {}^Q R_{i+\Delta})^{-1} ({}^P R_i^{-1} {}^P R_{i + \Delta}) \right)^{\vee}
\tag{4}</script><p>其中 ${}^Q R$ 表示位姿 $Q$ 的旋转部分。</p>
<p>也可以不使用 RMSE，直接使用平均值、中位数等来描述相对误差情况。</p>
<p>实际情况中，$\Delta$ 的选择有多种，为了综合衡量，可以计算所有 $\Delta$ 的 RMSE 平均值：</p>
<script type="math/tex; mode=display">
RMSE(E_{1:n}) = \frac{1}{n} \sum_{\Delta=1}^n RMSE(E_{1:n}, \Delta)
\tag{5}</script><p>这样计算复杂度很高，因此 TUM 数据集在提供的工具中，通过计算固定数量的 RPE 样本计算近似值。</p>
<h1 id="3-绝对轨迹误差"><a href="#3-绝对轨迹误差" class="headerlink" title="3  绝对轨迹误差"></a>3  绝对轨迹误差</h1><p>绝对轨迹误差（Absolute Trajectory Error，ATE）是估计位姿和真实位姿的直接差值，可以直观反应算法精度和轨迹全局一致性。</p>
<p>需要注意的是，估计位姿和真实位姿通常不在同一坐标系中，因此需要先将两者对齐：</p>
<ul>
<li>对于双目 SLAM 和 RGB-D 为尺度统一，通过最小二乘法计算一个从估计位姿到真实位姿的转换矩阵 $S \in SE(3)$；</li>
<li>对于单目相机，具有尺度不确定性，需要计算估计位姿到真实位姿到相似转换矩阵 $S \in Sim(2)$。</li>
</ul>
<p>因此，第 $i$ 帧的 ATE 定义如下：</p>
<script type="math/tex; mode=display">
ATE_i = F_i \triangleq Q_i^{-1} S P_i
\tag{6}</script><p>与 RPE 相似，使用 RMSE 统计 ATE：</p>
<script type="math/tex; mode=display">
RMSE(F_{1:n}, \Delta) := \left( \frac{1}{m} \sum_{i=1}^m \| \operatorname{trans}(F_i) \|^2 \right)^{\frac{1}{2}}
\tag{7}</script><p>同 RPE，也可以使用平均值、中位数等来反应 ATE 等情况。上述只描述了平移误差部分，旋转误差也可以使用与 RPE 相同的方式计算。</p>
<h1 id="4-EVO"><a href="#4-EVO" class="headerlink" title="4  EVO"></a>4  EVO</h1><ul>
<li>Link：<a href="https://github.com/MichaelGrupp/evo">https://github.com/MichaelGrupp/evo</a></li>
</ul>
<h2 id="4-1-安装"><a href="#4-1-安装" class="headerlink" title="4.1  安装"></a>4.1  安装</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pip installation</span></span><br><span class="line">pip3 install evo --upgrade --no-library evo</span><br><span class="line"></span><br><span class="line"><span class="comment"># source code installation</span></span><br><span class="line">git <span class="built_in">clone</span> git@github.com:MichaelGrupp/evo.git</span><br><span class="line"><span class="built_in">cd</span> evo</span><br><span class="line">pip3 install --editable . --upgrade --no-library evo</span><br><span class="line"></span><br><span class="line"><span class="comment"># check successful installation</span></span><br><span class="line">evo_ate -h</span><br></pre></td></tr></table></figure>
<h2 id="4-2-使用"><a href="#4-2-使用" class="headerlink" title="4.2  使用"></a>4.2  使用</h2><p><strong>（1）轨迹可视化</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">evo_traj kitti KITTI_00_ORB.txt KITTI_oo_SPTAM.txt --ref=KITTI_00_gt.txt -p --plot_mode=xz</span><br></pre></td></tr></table></figure>
<p><strong>（2）APE</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">evo_ape kitti KITTI_00_gt.txt KITTI_00_ORB.txt -va --plot --plot_mode=xz</span><br></pre></td></tr></table></figure>
<p><strong>（3）RPE</strong></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">evo_rpe tum fr2_desk_groundtrutr2_desk_ORB.txt -va --plot --plot_mode=xyz</span><br></pre></td></tr></table></figure>
<p><strong>（4）参数</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><strong>选项</strong></th>
<th style="text-align:center"><strong>含义</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">-a, —align</td>
<td style="text-align:center">轨迹对齐</td>
</tr>
<tr>
<td style="text-align:center">-s, —correct_scale</td>
<td style="text-align:center">尺度对齐</td>
</tr>
<tr>
<td style="text-align:center">—sync</td>
<td style="text-align:center">通过时间戳关联轨迹</td>
</tr>
<tr>
<td style="text-align:center">-p, —plot</td>
<td style="text-align:center">可视化轨迹</td>
</tr>
<tr>
<td style="text-align:center">-v, —verbose</td>
<td style="text-align:center">命令行输出细节信息</td>
</tr>
<tr>
<td style="text-align:center">—plot_mode</td>
<td style="text-align:center">轨迹可视化模式：xyz、xy、xz 等</td>
</tr>
<tr>
<td style="text-align:center">—save_plot</td>
<td style="text-align:center">保存结果</td>
</tr>
</tbody>
</table>
</div>
<p><strong>（5）metrics 解析</strong></p>
<p>前面提到位姿误差包含平移和旋转，在 evo 设置输出的误差项对应 option为 <code>-r</code> 或 <code>--pose_relation</code>，对应如下模式：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">option</th>
<th style="text-align:center">功能</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">full</td>
<td style="text-align:center">同时考虑旋转与平移误差，无单位</td>
</tr>
<tr>
<td style="text-align:center">trans_part</td>
<td style="text-align:center">平移误差，单位为 m</td>
</tr>
<tr>
<td style="text-align:center">rot_part</td>
<td style="text-align:center">旋转误差，无单位</td>
</tr>
<tr>
<td style="text-align:center">angle_deg</td>
<td style="text-align:center">旋转误差，单位为 degree</td>
</tr>
<tr>
<td style="text-align:center">angle_rad</td>
<td style="text-align:center">旋转误差，单位为 rad</td>
</tr>
</tbody>
</table>
</div>
<p>无单位 full 和 rot_part 的误差指标使用了矩阵的二范数：</p>
<script type="math/tex; mode=display">
error_{full} = \| \delta T_i - E \| \\
error_{rot} = \| \delta R_i - E \|
\tag{8}</script><p>其中 $\delta T_i$ 表示 GT 和估计位姿之间的变换矩阵，$E$ 为单位阵。</p>
<h1 id="附件"><a href="#附件" class="headerlink" title="附件"></a>附件</h1><h2 id="罗德里格斯公式"><a href="#罗德里格斯公式" class="headerlink" title="罗德里格斯公式"></a>罗德里格斯公式</h2><ul>
<li>注：旋转向量的二范数为旋转角的大小。</li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}
&R = \cos \theta I + (1 - \cos \theta)n n^T + \sin \theta n^{\wedge} \\
\Rightarrow &\theta = \arccos \left( \frac{\operatorname{trace}(R) - 1}{2} \right)
\end{aligned}
\tag{9}</script><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://zhuanlan.zhihu.com/p/105428199">https://zhuanlan.zhihu.com/p/105428199</a></li>
<li><a href="https://www.pudn.com/news/6228d5059ddf223e1ad1d429.html">https://www.pudn.com/news/6228d5059ddf223e1ad1d429.html</a></li>
</ul>
]]></content>
      <categories>
        <category>Summary</category>
      </categories>
      <tags>
        <tag>Summary</tag>
        <tag>Knowledge</tag>
        <tag>SLAM</tag>
        <tag>Metrics</tag>
      </tags>
  </entry>
  <entry>
    <title>Bundle Adjustment</title>
    <url>/summary/Bundle%20Adjustment/</url>
    <content><![CDATA[<p>Bundle Adjustment（光束平差法、捆集调整等）是指从视觉重建中计算出最优的 3D 模型和相机参数（内参和外参）。从每个特征点反射出来的几束光线（bundles of light rays），在把相机姿态和特征点的位置做出最优的调整（adjustment）之后，最后收束到光心的这个过程，简称 BA。</p>
<p>本文总结网络资源，对 BA 进行了总结。</p>
<span id="more"></span>
<center><b><font size="9">Bundle Adjustment</font></b></center>

<h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1  介绍"></a>1  介绍</h1><p>Bundle Adjustment（光束平差法、捆集调整等）是指从视觉重建中计算出最优的 3D 模型和相机参数（内参和外参）。从每个特征点反射出来的几束光线（bundles of light rays），在把相机姿态和特征点的位置做出最优的调整（adjustment）之后，最后收束到光心的这个过程，简称 BA。</p>
<center>
    <img src="/summary/Bundle%20Adjustment/1 BA示意图.jpg">
    <div>
        图-1  BA 示意
    </div>
</center>



<h1 id="2-原理"><a href="#2-原理" class="headerlink" title="2  原理"></a>2  原理</h1><p>假设空间位置的 3D 点为：$X = \{ x_1, x_2, …,  x_n \}$，相机中心位姿为 $P =\{ p_1, p_2, …, p_m \}$。$u_i$ 为 $x_i$ 对应的像素位置，$K$ 为相机的内参矩阵，$s_i$ 为 $u_i$ 对应的深度值。如图，BA 要计算的误差是观测值和估计值之间的误差。</p>
<center>
    <img src="/summary/Bundle%20Adjustment/2 BA的计算目标.png">
    <div>
        图-2  BA 的计算目标
    </div>
</center>

<p>对于单个相机的 BA，可以构建重投影误差最小二乘如下：</p>
<script type="math/tex; mode=display">
\varepsilon (X, T) = \sum_{i=1}^{n} || u_i - \frac{1}{s_i} K T x_i ||^2</script><p>其中，变换矩阵 $T$ 满足约束：</p>
<script type="math/tex; mode=display">
T =
\left[
\begin{matrix}
R &t \\
\pmb{0}^T &1
\end{matrix}
\right]
\in SE(3),\ 
R^T R = I, \ det(R) = 1,\ t \in \mathbb{R}^3</script><p>对于有约束的变换矩阵在最小二乘中不便求解，因此转换为无约束的李群求解：</p>
<script type="math/tex; mode=display">
\varepsilon (X, T) = \sum_{i=1}^{n} || u_i - \frac{1}{s_i} K \cdot \exp (\xi^{\wedge}) \cdot x_i ||^2</script><p>定义误差函数为：</p>
<script type="math/tex; mode=display">
f (x_i, \xi) = u_i - K \cdot \exp (\xi^{\wedge}) \cdot x_i</script><p>对于高斯牛顿法，函数 $f(x)$ 有增量方程：</p>
<script type="math/tex; mode=display">
J(x)^T \cdot J(x) \cdot \Delta x = -J(x)^T \cdot f(x)</script><p>仅需要求 $f(x_i, \xi)$ 的一阶导（Jacobian 矩阵）。</p>
<h1 id="3-C-代码"><a href="#3-C-代码" class="headerlink" title="3  C++ 代码"></a>3  C++ 代码</h1><ul>
<li>优化相机内参及畸变系数，相机的位姿（6dof）和 landmark 代价函数写法如下：</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> CameraModel&gt;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BundleAdjustmentCostFunction</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="function"><span class="keyword">explicit</span> <span class="title">BundleAdjustmentCostFunction</span><span class="params">(<span class="type">const</span> Eigen::Vector2d&amp; point2D)</span></span></span><br><span class="line"><span class="function">      :observed_x_(point2D(<span class="number">0</span>)), observed_y_(point2D(<span class="number">1</span>)) &#123;</span>&#125;</span><br><span class="line">  <span class="comment">// 构造函数传入的是观测值</span></span><br><span class="line">  <span class="function"><span class="type">static</span> ceres::CostFunction* <span class="title">Create</span><span class="params">(<span class="type">const</span> Eigen::Vector2d&amp; point2D)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (<span class="keyword">new</span> ceres::AutoDiffCostFunction&lt;</span><br><span class="line">            BundleAdjustmentCostFunction&lt;CameraModel&gt;, <span class="number">2</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">3</span>,</span><br><span class="line">            CameraModel::kNumParams&gt;(<span class="keyword">new</span> <span class="built_in">BundleAdjustmentCostFunction</span>(point2D)));</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 优化量:2代表误差方程个数;4代表pose中的姿态信息,用四元数表示;3代表pose中的位置信息;3代表landmark自由度;CameraModel::kNumParams是相机内参和畸变系数,其取决于相机模型</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment">// opertator 重载函数的参数即是待优化的量</span></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">  <span class="function"><span class="type">bool</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="type">const</span> T* <span class="type">const</span> qvec, <span class="type">const</span> T* <span class="type">const</span> tvec,</span></span></span><br><span class="line"><span class="params"><span class="function">                  <span class="type">const</span> T* <span class="type">const</span> point3D, <span class="type">const</span> T* <span class="type">const</span> camera_params,</span></span></span><br><span class="line"><span class="params"><span class="function">                  T* residuals)</span> <span class="type">const</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Rotate and translate.</span></span><br><span class="line">    T projection[<span class="number">3</span>];</span><br><span class="line">    ceres::<span class="built_in">UnitQuaternionRotatePoint</span>(qvec, point3D, projection);</span><br><span class="line">    projection[<span class="number">0</span>] += tvec[<span class="number">0</span>];</span><br><span class="line">    projection[<span class="number">1</span>] += tvec[<span class="number">1</span>];</span><br><span class="line">    projection[<span class="number">2</span>] += tvec[<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Project to image plane.</span></span><br><span class="line">    projection[<span class="number">0</span>] /= projection[<span class="number">2</span>];</span><br><span class="line">    projection[<span class="number">1</span>] /= projection[<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Distort and transform to pixel space.</span></span><br><span class="line">    CameraModel::<span class="built_in">WorldToImage</span>(camera_params, projection[<span class="number">0</span>], projection[<span class="number">1</span>],</span><br><span class="line">                              &amp;residuals[<span class="number">0</span>], &amp;residuals[<span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Re-projection error.</span></span><br><span class="line">    residuals[<span class="number">0</span>] -= <span class="built_in">T</span>(observed_x_);</span><br><span class="line">    residuals[<span class="number">1</span>] -= <span class="built_in">T</span>(observed_y_);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">  <span class="type">const</span> <span class="type">double</span> observed_x_;</span><br><span class="line">  <span class="type">const</span> <span class="type">double</span> observed_y_;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<ul>
<li>ceres 自动求导，代码如下：</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">ceres::Problem problem;</span><br><span class="line">ceres::CostFunction* cost_function = <span class="literal">nullptr</span>; </span><br><span class="line">ceres::LossFunction * p_LossFunction =</span><br><span class="line">    ceres_options_.bUse_loss_function_ ?</span><br><span class="line">      <span class="keyword">new</span> ceres::<span class="built_in">HuberLoss</span>(<span class="built_in">Square</span>(<span class="number">4.0</span>))</span><br><span class="line">      : <span class="literal">nullptr</span>; <span class="comment">// 关于为何使用损失函数,因为现实中并不是所有观测过程中的噪声都服从 </span></span><br><span class="line">      <span class="comment">// gaussian noise的（或者可以说几乎没有），</span></span><br><span class="line">      <span class="comment">// 遇到有outlier的情况，这些方法非常容易挂掉，</span></span><br><span class="line">      <span class="comment">// 这时候就得用到robust statistics里面的</span></span><br><span class="line">      <span class="comment">// robust cost(*cost也可以叫做loss, 统计学那边喜欢叫risk) function了，</span></span><br><span class="line">      <span class="comment">// 比较常用的有huber, cauchy等等。</span></span><br><span class="line">cost_function = BundleAdjustmentCostFunction&lt;CameraModel&gt;::<span class="built_in">Create</span>(point2D.<span class="built_in">XY</span>()); </span><br><span class="line"><span class="comment">// 将优化量加入残差块</span></span><br><span class="line">problem_-&gt;<span class="built_in">AddResidualBlock</span>(cost_function, p_LossFunction, qvec_data,</span><br><span class="line">                                 tvec_data, point3D.<span class="built_in">XYZ</span>().<span class="built_in">data</span>(),</span><br><span class="line">                                 camera_params_data);</span><br></pre></td></tr></table></figure>
<ul>
<li>优化相机内参及畸变系数，pose subset parameterization（pose 信息部分参数优化）和 3D landmark，当只优化姿态信息时，problem 需要添加的代码如下：</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">map_poses[indexPose] = &#123;angleAxis[<span class="number">0</span>], angleAxis[<span class="number">1</span>], angleAxis[<span class="number">2</span>], <span class="built_in">t</span>(<span class="number">0</span>), <span class="built_in">t</span>(<span class="number">1</span>), <span class="built_in">t</span>(<span class="number">2</span>)&#125;;</span><br><span class="line"></span><br><span class="line"><span class="type">double</span> * parameter_block = &amp;map_poses.<span class="built_in">at</span>(indexPose)[<span class="number">0</span>];</span><br><span class="line">problem.<span class="built_in">AddParameterBlock</span>(parameter_block, <span class="number">6</span>);</span><br><span class="line">std::vector&lt;<span class="type">int</span>&gt; vec_constant_extrinsic;</span><br><span class="line">vec_constant_extrinsic.<span class="built_in">insert</span>(vec_constant_extrinsic.<span class="built_in">end</span>(), &#123;<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>&#125;);</span><br><span class="line"><span class="keyword">if</span> (!vec_constant_extrinsic.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">    <span class="comment">// 主要用到ceres的SubsetParameterization函数</span></span><br><span class="line">    ceres::SubsetParameterization *subset_parameterization =</span><br><span class="line">        <span class="keyword">new</span> ceres::<span class="built_in">SubsetParameterization</span>(<span class="number">6</span>, vec_constant_extrinsic);</span><br><span class="line">    problem.<span class="built_in">SetParameterization</span>(parameter_block, subset_parameterization);</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>
<ul>
<li>优化相机内参及畸变系数，pose subset parameterization（pose 信息部分参数优化）和 3D landmark，当只优化位置信息时，problem 需要添加的代码如下（同上，只需修改一行）：</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">map_poses[indexPose] = &#123;angleAxis[<span class="number">0</span>], angleAxis[<span class="number">1</span>], angleAxis[<span class="number">2</span>], <span class="built_in">t</span>(<span class="number">0</span>), <span class="built_in">t</span>(<span class="number">1</span>), <span class="built_in">t</span>(<span class="number">2</span>)&#125;;</span><br><span class="line"></span><br><span class="line"><span class="type">double</span> * parameter_block = &amp;map_poses.<span class="built_in">at</span>(indexPose)[<span class="number">0</span>];</span><br><span class="line">problem.<span class="built_in">AddParameterBlock</span>(parameter_block, <span class="number">6</span>);</span><br><span class="line">std::vector&lt;<span class="type">int</span>&gt; vec_constant_extrinsic;</span><br><span class="line">vec_constant_extrinsic.<span class="built_in">insert</span>(vec_constant_extrinsic.<span class="built_in">end</span>(), &#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>&#125;);</span><br><span class="line"><span class="keyword">if</span> (!vec_constant_extrinsic.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">    ceres::SubsetParameterization *subset_parameterization =</span><br><span class="line">        <span class="keyword">new</span> ceres::<span class="built_in">SubsetParameterization</span>(<span class="number">6</span>, vec_constant_extrinsic);</span><br><span class="line">    problem.<span class="built_in">SetParameterization</span>(parameter_block, subset_parameterization);</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>
<ul>
<li>优化相机内参及畸变系数，pose 是常量不优化和 3D landmark，代价函数写法如下：</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 相机模型</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> CameraModel&gt;  </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BundleAdjustmentConstantPoseCostFunction</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="built_in">BundleAdjustmentConstantPoseCostFunction</span>(<span class="type">const</span> Eigen::Vector4d&amp; qvec,</span><br><span class="line">                                           <span class="type">const</span> Eigen::Vector3d&amp; tvec,</span><br><span class="line">                                           <span class="type">const</span> Eigen::Vector2d&amp; point2D)</span><br><span class="line">      : <span class="built_in">qw_</span>(<span class="built_in">qvec</span>(<span class="number">0</span>)),</span><br><span class="line">        <span class="built_in">qx_</span>(<span class="built_in">qvec</span>(<span class="number">1</span>)),</span><br><span class="line">        <span class="built_in">qy_</span>(<span class="built_in">qvec</span>(<span class="number">2</span>)),</span><br><span class="line">        <span class="built_in">qz_</span>(<span class="built_in">qvec</span>(<span class="number">3</span>)),</span><br><span class="line">        <span class="built_in">tx_</span>(<span class="built_in">tvec</span>(<span class="number">0</span>)),</span><br><span class="line">        <span class="built_in">ty_</span>(<span class="built_in">tvec</span>(<span class="number">1</span>)),</span><br><span class="line">        <span class="built_in">tz_</span>(<span class="built_in">tvec</span>(<span class="number">2</span>)),</span><br><span class="line">        <span class="built_in">observed_x_</span>(<span class="built_in">point2D</span>(<span class="number">0</span>)),</span><br><span class="line">        <span class="built_in">observed_y_</span>(<span class="built_in">point2D</span>(<span class="number">1</span>)) &#123;&#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="type">static</span> ceres::CostFunction* <span class="title">Create</span><span class="params">(<span class="type">const</span> Eigen::Vector4d&amp; qvec,</span></span></span><br><span class="line"><span class="params"><span class="function">                                     <span class="type">const</span> Eigen::Vector3d&amp; tvec,</span></span></span><br><span class="line"><span class="params"><span class="function">                                     <span class="type">const</span> Eigen::Vector2d&amp; point2D)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (<span class="keyword">new</span> ceres::AutoDiffCostFunction&lt;</span><br><span class="line">            BundleAdjustmentConstantPoseCostFunction&lt;CameraModel&gt;, <span class="number">2</span>, <span class="number">3</span>,</span><br><span class="line">            CameraModel::kNumParams&gt;(</span><br><span class="line">        <span class="keyword">new</span> <span class="built_in">BundleAdjustmentConstantPoseCostFunction</span>(qvec, tvec, point2D)));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">  <span class="function"><span class="type">bool</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="type">const</span> T* <span class="type">const</span> point3D, <span class="type">const</span> T* <span class="type">const</span> camera_params,</span></span></span><br><span class="line"><span class="params"><span class="function">                  T* residuals)</span> <span class="type">const</span> </span>&#123;</span><br><span class="line">    <span class="type">const</span> T qvec[<span class="number">4</span>] = &#123;<span class="built_in">T</span>(qw_), <span class="built_in">T</span>(qx_), <span class="built_in">T</span>(qy_), <span class="built_in">T</span>(qz_)&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Rotate and translate.</span></span><br><span class="line">    T projection[<span class="number">3</span>];</span><br><span class="line">    ceres::<span class="built_in">UnitQuaternionRotatePoint</span>(qvec, point3D, projection);</span><br><span class="line">    projection[<span class="number">0</span>] += <span class="built_in">T</span>(tx_);</span><br><span class="line">    projection[<span class="number">1</span>] += <span class="built_in">T</span>(ty_);</span><br><span class="line">    projection[<span class="number">2</span>] += <span class="built_in">T</span>(tz_);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Project to image plane.</span></span><br><span class="line">    projection[<span class="number">0</span>] /= projection[<span class="number">2</span>];</span><br><span class="line">    projection[<span class="number">1</span>] /= projection[<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Distort and transform to pixel space.</span></span><br><span class="line">    CameraModel::<span class="built_in">WorldToImage</span>(camera_params, projection[<span class="number">0</span>], projection[<span class="number">1</span>],</span><br><span class="line">                              &amp;residuals[<span class="number">0</span>], &amp;residuals[<span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Re-projection error.</span></span><br><span class="line">    residuals[<span class="number">0</span>] -= <span class="built_in">T</span>(observed_x_);</span><br><span class="line">    residuals[<span class="number">1</span>] -= <span class="built_in">T</span>(observed_y_);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">  <span class="type">const</span> <span class="type">double</span> qw_;</span><br><span class="line">  <span class="type">const</span> <span class="type">double</span> qx_;</span><br><span class="line">  <span class="type">const</span> <span class="type">double</span> qy_;</span><br><span class="line">  <span class="type">const</span> <span class="type">double</span> qz_;</span><br><span class="line">  <span class="type">const</span> <span class="type">double</span> tx_;</span><br><span class="line">  <span class="type">const</span> <span class="type">double</span> ty_;</span><br><span class="line">  <span class="type">const</span> <span class="type">double</span> tz_;</span><br><span class="line">  <span class="type">const</span> <span class="type">double</span> observed_x_;</span><br><span class="line">  <span class="type">const</span> <span class="type">double</span> observed_y_;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<ul>
<li>接下来 problem 加入残差块代码如下:</li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">ceres::Problem problem;</span><br><span class="line">ceres::CostFunction* cost_function = <span class="literal">nullptr</span>; </span><br><span class="line">ceres::LossFunction * p_LossFunction =</span><br><span class="line">    ceres_options_.bUse_loss_function_ ?</span><br><span class="line">      <span class="keyword">new</span> ceres::<span class="built_in">HuberLoss</span>(<span class="built_in">Square</span>(<span class="number">4.0</span>))</span><br><span class="line">      : <span class="literal">nullptr</span>; <span class="comment">// 关于为何使用损失函数,因为现实中并不是所有观测过程中的噪声都服从 </span></span><br><span class="line">      <span class="comment">// gaussian noise的（或者可以说几乎没有），</span></span><br><span class="line">      <span class="comment">// 遇到有outlier的情况，这些方法非常容易挂掉，</span></span><br><span class="line">      <span class="comment">// 这时候就得用到robust statistics里面的</span></span><br><span class="line">      <span class="comment">// robust cost(*cost也可以叫做loss, 统计学那边喜欢叫risk) function了，</span></span><br><span class="line">      <span class="comment">// 比较常用的有huber, cauchy等等。</span></span><br><span class="line">cost_function = BundleAdjustmentConstantPoseCostFunction&lt;CameraModel&gt;::<span class="built_in">Create</span>(</span><br><span class="line">            image.<span class="built_in">Qvec</span>(), image.<span class="built_in">Tvec</span>(), point2D.<span class="built_in">XY</span>()); <span class="comment">// 观测值输入  </span></span><br><span class="line"><span class="comment">// 将优化量加入残差块</span></span><br><span class="line">problem_-&gt;<span class="built_in">AddResidualBlock</span>(cost_function, loss_function,</span><br><span class="line">                           point3D.<span class="built_in">XYZ</span>().<span class="built_in">data</span>(), camera_params_data); <span class="comment">// 被优化量加入残差-3D点和相机内参</span></span><br></pre></td></tr></table></figure>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://zhuanlan.zhihu.com/p/344766723">https://zhuanlan.zhihu.com/p/344766723</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/332663887（C++">https://zhuanlan.zhihu.com/p/332663887（C++</a> 代码）</li>
<li><a href="https://blog.csdn.net/optsolution/article/details/64442962">https://blog.csdn.net/optsolution/article/details/64442962</a></li>
<li><a href="https://blog.csdn.net/shyjhyp11/article/details/104108926">https://blog.csdn.net/shyjhyp11/article/details/104108926</a></li>
</ul>
]]></content>
      <categories>
        <category>Summary</category>
      </categories>
      <tags>
        <tag>Summary</tag>
        <tag>Knowledge</tag>
        <tag>SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>CLINS</title>
    <url>/papernote/CLINS/</url>
    <content><![CDATA[<p>关于论文 CLINS：Continuous-Time Trajectory Estimation for LiDAR-Inertial System (IROS 2021) 的阅读总结。</p>
<span id="more"></span>
<center><b><font size="9">CLINS</font></b></center>

<h1 id="0-论文简介"><a href="#0-论文简介" class="headerlink" title="0  论文简介"></a>0  论文简介</h1><ul>
<li>标题：CLINS：Continuous-Time Trajectory Estimation for LiDAR-Inertial System</li>
<li>作者：Jiajun Lv，Kewei Hu，Jinhong Xu，Yong Liu，Xiushui Ma，Xingxing Zuo</li>
<li>会议：IROS 2021</li>
<li>源码：<a href="https://github.com/APRIL-ZJU/clins">https://github.com/APRIL-ZJU/clins</a></li>
</ul>
<h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1  介绍"></a>1  介绍</h1><p>基于 discrete-time 的方法通过差值的方法将 LiDAR 点去畸变为扫描的起始时刻。IMU 数据被差值和融合来形成两个离散 scan 的相对位姿约束。但是这种方法存在一下的缺陷：</p>
<ul>
<li>在实际中，传感器之间不是以相同的频率获取测量数据，这将产生不可忽视的误差。</li>
<li>直接使用原始 LiDAR 测量数据和 IMU 数据是困难的。原始 LiDAR 点数据需要被去畸变至特定时刻以产生 LiDAR scan，而原始 IMU 数据则用于生成相对位姿。</li>
</ul>
<p>这些问题是由于难以处理高频传感器产生的原始数据而形成的，直接使用这些数据需要大量的位姿变量。上述问题可被归纳为离散的位姿表达无法满足系统的高分辨率要求。</p>
<p>目前出现了 continuous-time 的方法，通过将轨迹建模为时间的函数并支持查询任何时间戳下的位姿，解决了集成异步和高频数据的问题。CLINS 提出了一种 LiDAR 惯性系统的完全基于 continuous-time 轨迹估计框架，主要贡献如下：</p>
<ul>
<li>提出了 continuous-time 轨迹估计子（estimator），可以支持 3D LiDAR 点和惯性数据的融合，并且能容易地拓展以融合其他不同频率的异步传感器产生的数据。</li>
<li>提出了一种两阶段的 continuous-time 轨迹修正方法来高效解决回环检测问题。</li>
</ul>
<h1 id="2-连续时间轨迹的表达方式"><a href="#2-连续时间轨迹的表达方式" class="headerlink" title="2  连续时间轨迹的表达方式"></a>2  连续时间轨迹的表达方式</h1><p>定义 6-DoF 刚性变换 ${}_A^B \pmb{T} \in SE(3) \in \mathbb{R}^{4 \times 4}$，将帧 $\{ A\}$ 中的点 ${}^A \pmb{p} \in \mathbb{R}^3$ 变换到帧 $\{ B\}$ 中。变换矩阵为：</p>
<script type="math/tex; mode=display">
{}_A^B \pmb{T} = 
\left[
\begin{matrix}
{}_A^B \pmb{R} &{}^B \pmb{p}_A \\
{}^A \pmb{0} &1
\end{matrix}
\right]</script><p>其中旋转部分 ${}_A^B \pmb{R} \in SO(3)$，平移部分 ${}^B \pmb{p}_A \in \mathbb{R}^3$。为简化，将变换过程写为其次方程：</p>
<script type="math/tex; mode=display">
{}^B \pmb{p} = {}_A^B \pmb{T} {}^A \pmb{p}</script><p>$Exp(\cdot)$ 函数将 $\mathbb{R}^3$ 中的向量映射到李群 $SO(3)$，$Log(\cdot)$ 为此映射的反过程，$(\cdot)_{\vee}$ 将李代数中的元素映射到向量。</p>
<p>B 样条曲线（B-spline）是光滑且局部连续的，其具有封闭形式的解析导数，使得很容易与 IMU 测量数据匹配。为此，CLINS 使用了两个分离的 B-spline 群来参数化 3D 平移和 3D 旋转，$\pmb{p}(t) \in \mathbb{R}^3$ 和 $\pmb{R}(t) \in SO(3)$。均匀 B 样条曲线的总结如下表：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Uniform B-Spline Basics</th>
<th style="text-align:center"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Order</td>
<td style="text-align:center">$k$</td>
</tr>
<tr>
<td style="text-align:center">Knot distance</td>
<td style="text-align:center">$\Delta t$</td>
</tr>
<tr>
<td style="text-align:center">Coeff. Vec. (1)</td>
<td style="text-align:center">$\phi(u) = \pmb{M}^{(k)} \pmb{u}$</td>
</tr>
<tr>
<td style="text-align:center">Cumul. Coeff. Vec. (1,2)</td>
<td style="text-align:center">$\pmb{\lambda}(u) = \tilde{\pmb{M}}^{(k)} \pmb{u}$</td>
</tr>
<tr>
<td style="text-align:center"><strong>Position in Cumulative Form</strong></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">Control point</td>
<td style="text-align:center">$\pmb{\Phi}_p = {\pmb{p}_i} \in \mathbb{R}^3,\ i \in [0,n]$</td>
</tr>
<tr>
<td style="text-align:center">Distance</td>
<td style="text-align:center">$d_j^i = \pmb{p}_{i+j} - \pmb{p}_{i + j - 1} \in \mathbb{R}^3$</td>
</tr>
<tr>
<td style="text-align:center">Position</td>
<td style="text-align:center">$\pmb{p}(u) = \pmb{p}_i + \sum_{j=1}^{k-1} \lambda_j \cdot \pmb{d}_j^i$</td>
</tr>
<tr>
<td style="text-align:center">Velocity</td>
<td style="text-align:center">$\pmb{v}(u) = \sum_{j=1}^{k-1} \dot{\lambda}_j(u) \cdot \pmb{d}_j^i$</td>
</tr>
<tr>
<td style="text-align:center">Acceleration</td>
<td style="text-align:center">$\pmb{a}(u) = \sum_{j=1}^{k-1} \ddot{\lambda}_j(u) \cdot \pmb{d}_j^i$</td>
</tr>
<tr>
<td style="text-align:center"><strong>Orientation in Cumulative Form</strong></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">Control point</td>
<td style="text-align:center">$\pmb{\Phi}_R = {\pmb{R}_i} \in SO(3),\ i \in [0,n]$</td>
</tr>
<tr>
<td style="text-align:center">Distance</td>
<td style="text-align:center">$d_j^i = Log(\pmb{R}_{i+j-1}^{-1} \pmb{R}_{i+j}) \in \mathbb{R}^3$</td>
</tr>
<tr>
<td style="text-align:center">Position</td>
<td style="text-align:center">$\pmb{R}(u) = \pmb{R}_i \cdot \prod_{j=1}^{k-1} Exp(\lambda_j(u) \cdot \pmb{d}_j^i)$</td>
</tr>
<tr>
<td style="text-align:center">Velocity</td>
<td style="text-align:center">$\pmb{\omega}(u) = (\pmb{R}^\top \dot{\pmb{R}})_{\vee}$</td>
</tr>
</tbody>
</table>
</div>
<p>其中：</p>
<ul>
<li>(1) $t \in [t_i, t_{i+1}),\ u(t) = s(t) - i, s(t) = (t - t_0) / \Delta t$</li>
<li>(2) 注意 $\lambda_0(u) \equiv 1$</li>
</ul>
<p>特别的，样条矩阵 $M^{(k)}$ 和累积矩阵 $\tilde{M}^{(k)}$ 对于 B 样条曲线来说是常量。对于 $\mathbb{R}^n$ 中的平移轨迹，B 样条曲线的基本形式和累积形式是等价可相互转换的，而在非欧氏空间 $SO(3)$ 中这是不成立的。旋转轨迹可以使用累积形式表示，上表列出了 $\mathbb{R}^3$ 和 $SO(3)$ 中的累积形式。取样条曲线对时间的倒数，可以得到速度和加速度。上表中的线速度 $\pmb{v}(t)$ 和线加速度 $\pmb{a}(t)$ 是在全局坐标系中的，而角速度 $\pmb{\omega}(t)$ 则在局部坐标系。</p>
<p>全局坐标系 $\{G\}$ 下 IMU 的 continuous-time 轨迹变换为：</p>
<script type="math/tex; mode=display">
{}_I^G \pmb{T}(t) = 
\left[
\begin{matrix}
\pmb{R}(t) &\pmb{p}(t) \\
\pmb{0} &1
\end{matrix}
\right]</script><p>由于已知 IMU 和 LiDAR 之间的变换（外参）${}_L^I \pmb{T}$，因此 LiDAR 的轨迹变换为：</p>
<script type="math/tex; mode=display">
{}_L^G \pmb{T}(t) = {}_I^G \pmb{T}(t) {}_L^I \pmb{T}</script><p>由于 B 样条曲线的局部性，对于 $t \in [t_i, t_{i+1})$，${}_I^G \pmb{T}$ 只受到 $\{ t_i, t_{i+1}, \cdots, t_{i+k-1} \}$ 的 knots 及与其相关的 control points 集 $\pmb{\Phi}(t_i, t_{i+1})$ 的控制，其中：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\pmb{\Phi}(t_i, t_{i+1}) 
&= \pmb{\Phi}_R (t_i, t_{i+1}) \cup \pmb{\Phi}_p (t_i, t_{i+1}) \\
&= \{ \pmb{R}_i, \cdots, \pmb{R}_{i+k-1} \} \cup \{ \pmb{p}_i, \cdots, \pmb{p}_{i+k-1} \}
\end{aligned}</script><h1 id="3-方法"><a href="#3-方法" class="headerlink" title="3  方法"></a>3  方法</h1><p>如图是所提出的 LiDAR 惯性系统 continuous-time 轨迹估计所涉及的定义。根据当前 LiDAR 扫描数据的采集时间间隔来定义 active segments。Active segments 对应的控制点序列 $\pmb{\Phi}_{active}$ 记为活动控制点（active control points，蓝点和红点）。对应于 $\pmb{\Phi}_{active}$ 的基函数子集表示为活动基函数（蓝色曲线和红色曲线）。Active 轨迹（蓝色和红色）是由局部区域的活动控制点决定。此外，除了 active segments 的时间，与活动基函数有关的时间段被标记为 static segments。据此定义了静态控制点 $\pmb{\Phi}_{static}$（static control points，绿色点）和静态基函数（绿色曲线）。</p>
<center>
    <img src="/papernote/CLINS/1 系统框架.png">
    <div>
        图-1  系统框架
    </div>
</center>

<p>该系统在一次新的 LiDAR 扫描后，在状态向量中加入新的控制点（红点），对扩展轨迹进行参数化。利用集成的 IMU 姿态来初始化这些新的控制点。随后，从当前扫描帧中提取边缘点和平面点等 LOAM 特征与局部子地图进行数据关联。最后，给定原始 IMU 测量在局部窗口和相关的 LiDAR 特征，使用批量优化的方式估计 continuous-time 轨迹。状态估计问题可以表述为最大后验概率（MAP）问题。基于传感器测量数据存在独立高斯噪声的假设，可以通过求解非线性最小二乘（NLLS）问题来估计 continuous-time 轨迹。</p>
<h2 id="3-1-初始化"><a href="#3-1-初始化" class="headerlink" title="3.1  初始化"></a>3.1  初始化</h2><p>当新的 LiDAR 帧到达后，新的 control points 被加入以扩展现有的轨迹。融合离散的 IMU 数据来获取时间 $t_m$ 的高频的旋转 $\pmb{R}_{I_m}$、平移 $\pmb{p}_{I_m}$ 和线速度 $\pmb{v}_{I_m}$ 估计。可以最小化一下损失函数来初始化新加入的 control points $\pmb{\Phi}_{new}$：</p>
<script type="math/tex; mode=display">
\underset{\pmb{\Phi}_{new}}{\operatorname{argmin}} \sum (|| Log(\pmb{R}_{I_m}^{\top}) \pmb{R}(t_m) || + || \pmb{p}(t_m) - \pmb{p}_{I_m} || + || \pmb{v}(t_m) - \pmb{v}_{I_m} ||)</script><h2 id="3-2-局部窗口的非网格注册"><a href="#3-2-局部窗口的非网格注册" class="headerlink" title="3.2  局部窗口的非网格注册"></a>3.2  局部窗口的非网格注册</h2><p>对于新到来的帧 $S_k$，其在时间间隔 $[t_k, t_k + \Delta T]$（$t_k$ 为 $S_k$ 中第一个点的时间戳，$\Delta T$ 为完成一次 LiDAR 扫描的周期）收集得到，如果在扫描过程中传感器存在外部移动，那么 $S_k$ 中的非网格点云就十分重要。因此传统计算两个网格点云的相对变换的注册算法，将不适用于这种点云。为解决这个问题，此处提出了一个非网格注册的方法。</p>
<p>特别地，$S_k$ 中的每一个点变换到一个统一的坐标系 $\{ L_k \}$ 中：</p>
<script type="math/tex; mode=display">
{}^{L_k} \pmb{x}_{kj} = {}^G_L T(t_k)^\top {}^G_LT(t_k + \tau_j) {}^{L_{kj}} \pmb{x}_{kj}</script><p>$\tau_j$ 为点 ${}^{L_{kj}} \pmb{x}_{kj}$ 相对当前帧起始时间点时间戳。为使计算易处理，通过计算曲率从原始点云中提取出面特征（planar features）和边特征（edge features）。</p>
<p>考虑到 LiDAR 数据在一些非结构化的环境中会退化，同时高频的 IMU 数据不受影响，因此将 IMU 测量同 LiDAR 特征紧耦合以约束轨迹。于是非网格注册问题可以被定义为：给定当前帧 active segment 和 static segment 对应的 LiDAR 特征和惯性数据，估计轨迹的活动控制点 $\pmb{\Phi} (t_k, t_k + \Delta T)$ 和 IMU 的偏差。具体地，可以通过求解下目标函数来解决这个问题：</p>
<script type="math/tex; mode=display">
\underset{\mathcal{X}}{\operatorname{argname}} \sum ||\pmb{r}_{\mathcal{L}}||_{\pmb{\Sigma}_{\mathcal{L}}} + ||\pmb{r}_a||_{\pmb{\Sigma}_{\mathcal{a}}} + ||\pmb{r}_w||_{\pmb{\Sigma}_{\mathcal{w}}}</script><p>其中 $\mathcal{X} = \{ \pmb{\Phi}(t_k, t_k + \Delta T), \pmb{b}_a, \pmb{b}_g \}$，$\pmb{b}_a,\pmb{b}_{g}$ 分别表示加速度计和陀螺仪的偏差，$\pmb{r}_{\mathcal{L}}, \pmb{r}_a, \pmb{r}_w$ 表示 LiDAR 特征和 IMU 测量的残差，$\pmb{\Sigma}_{\mathcal{L}}, \pmb{\Sigma}_a, \pmb{\Sigma}_w$ 为响应的协方差矩阵。残差的定义如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\pmb{r}_{\mathcal{L}} &= \pi ({}^G_L T(t_k + \tau_j) {}^{L_{kj}}\pmb{x}_{kj}) \\
\pmb{r}_a &= {}_L^G \pmb{R}^\top (t_m) (\pmb{a}(t_m) - {}^G \pmb{g}) - \pmb{a}_m + \pmb{b}_a \\
\pmb{r}_w &= \pmb{\omega} (t_m) - \pmb{\omega}_m + \pmb{b}_w
\end{aligned}</script><p>其中投影函数 $\pi(\cdot)$ 对面特征进行点到平面投影，对边特征进行点到线投影。$\pmb{a}_m,\pmb{\omega}_m$ 为时间 $t_m$ 的惯性测量数据。注意到静态控制点被包含在优化过程中，但在优化过程中保持不变。使用 Ceres 中的 LM 法求解上述非线性最小二乘问题。</p>
<h2 id="3-3-轨迹修正"><a href="#3-3-轨迹修正" class="headerlink" title="3.3  轨迹修正"></a>3.3  轨迹修正</h2><p>里程系统的累积漂移误差是无法避免的，本文在出现回环时进行修正以减缓累积误差。考虑到在有许多 knot 的连续轨迹上使用全局优化是费时的，因此此处提出了一个两阶段的轨迹优化方法降低时间复杂度和计算开销：</p>
<ul>
<li>第一阶段，当检测到回环时，在关键帧的离散位姿上进行位姿图优化来限制累积误差。</li>
<li>第二阶段，使用更新过的关键帧的位姿尝试更新控制点。这个过程可以被理解为将连续的轨迹投影到某些锚位姿上并保留轨迹的局部形状。</li>
</ul>
<p>一般来说，局部的惯导估计是准确的，因此在某一确切时刻的原始 continuous-time 轨迹上计算局部线速度和角速度。因此，第二阶段的问题可以被形式化为求解如下函数：</p>
<script type="math/tex; mode=display">
\underset{\pmb{\Phi}_{update}}{\operatorname{argname}} \sum (||Log(\hat{\pmb{R}}_k^\top \pmb{R}(t_k))|| + ||\pmb{p}(t_k) - \hat{\pmb{p}}_k||) + \sum (||\pmb{R}(t_j)^\top \pmb{v}(t_j) - \hat{\pmb{v}}_j|| + ||\pmb{\omega}(t_j) - \hat{\pmb{\omega}}_j||)</script><p>其中 $\hat{\pmb{R}}_k, \hat{\pmb{p}}_k$ 表示位姿优化后时间 $t_k$ 更新后的关键帧的位姿，$\hat{\pmb{v}}_j, \hat{\pmb{\omega}}_j$ 为修正前时间 $t_j$ 的线速度和角速度。下Í图为两阶段方法的示意图：</p>
<center>
    <img src="/papernote/CLINS/2 两阶段修正.png">
    <div>
        图-2  两阶段修正
    </div>
</center>



<h1 id="4-试验结果"><a href="#4-试验结果" class="headerlink" title="4  试验结果"></a>4  试验结果</h1><h2 id="4-1-数据集"><a href="#4-1-数据集" class="headerlink" title="4.1  数据集"></a>4.1  数据集</h2><center>
    <img src="/papernote/CLINS/3 数据集.png">
    <div>
        图-3  数据集
    </div>
</center>



<h2 id="4-2-RMSE"><a href="#4-2-RMSE" class="headerlink" title="4.2  RMSE"></a>4.2  RMSE</h2><center>
    <img src="/papernote/CLINS/4 RMSE.png">
    <div>
        图-4  平移和旋转的 RMSE
    </div>
</center>



<h2 id="4-3-与其他模型的对比"><a href="#4-3-与其他模型的对比" class="headerlink" title="4.3  与其他模型的对比"></a>4.3  与其他模型的对比</h2><p>其中 LIO-SAM(odom) 和 CLINS(odom) 表示去除回环修正：</p>
<center>
    <img src="/papernote/CLINS/5 模型对比.png">
    <div>
        图-5  与其他模型的对比
    </div>
</center>



<h2 id="4-4-可视化结果"><a href="#4-4-可视化结果" class="headerlink" title="4.4  可视化结果"></a>4.4  可视化结果</h2><center>
    <img src="/papernote/CLINS/6 可视化.png">
    <div>
        图-6  结果可视化
    </div>
</center>



<h1 id="5-B-样条曲线"><a href="#5-B-样条曲线" class="headerlink" title="5  B 样条曲线"></a>5  B 样条曲线</h1><h2 id="5-1-参考"><a href="#5-1-参考" class="headerlink" title="5.1  参考"></a>5.1  参考</h2><ul>
<li><a href="https://zhuanlan.zhihu.com/p/144042470">https://zhuanlan.zhihu.com/p/144042470</a></li>
<li><a href="https://blog.csdn.net/qq_40597317/article/details/81155571">https://blog.csdn.net/qq_40597317/article/details/81155571</a></li>
<li><a href="https://www.jianshu.com/p/49aff913104c">https://www.jianshu.com/p/49aff913104c</a></li>
</ul>
]]></content>
      <categories>
        <category>Paper Note</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Paper Note</tag>
        <tag>IO</tag>
      </tags>
  </entry>
  <entry>
    <title>CT-ICP</title>
    <url>/papernote/CT-ICP/</url>
    <content><![CDATA[<p>关于论文 CT-ICP: Real-time Elastic LiDAR Odometry with Loop Closure (ICRA 2022) 的阅读总结。</p>
<span id="more"></span>
<p><img src="/papernote/CT-ICP/幻灯片1.png" alt><br><img src="/papernote/CT-ICP/幻灯片2.png" alt><br><img src="/papernote/CT-ICP/幻灯片3.png" alt><br><img src="/papernote/CT-ICP/幻灯片4.png" alt><br><img src="/papernote/CT-ICP/幻灯片5.png" alt><br><img src="/papernote/CT-ICP/幻灯片6.png" alt><br><img src="/papernote/CT-ICP/幻灯片7.png" alt><br><img src="/papernote/CT-ICP/幻灯片8.png" alt><br><img src="/papernote/CT-ICP/幻灯片9.png" alt><br><img src="/papernote/CT-ICP/幻灯片10.png" alt><br><img src="/papernote/CT-ICP/幻灯片11.png" alt><br><img src="/papernote/CT-ICP/幻灯片12.png" alt><br><img src="/papernote/CT-ICP/幻灯片13.png" alt><br><img src="/papernote/CT-ICP/幻灯片14.png" alt><br><img src="/papernote/CT-ICP/幻灯片15.png" alt><br><img src="/papernote/CT-ICP/幻灯片16.png" alt><br><img src="/papernote/CT-ICP/幻灯片17.png" alt><br><img src="/papernote/CT-ICP/幻灯片18.png" alt><br><img src="/papernote/CT-ICP/幻灯片19.png" alt><br><img src="/papernote/CT-ICP/幻灯片20.png" alt><br><img src="/papernote/CT-ICP/幻灯片21.png" alt><br><img src="/papernote/CT-ICP/幻灯片22.png" alt><br><img src="/papernote/CT-ICP/幻灯片23.png" alt></p>
]]></content>
      <categories>
        <category>Paper Note</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Paper Note</tag>
        <tag>ICP</tag>
        <tag>LiDAR Odometry</tag>
      </tags>
  </entry>
  <entry>
    <title>CLINS</title>
    <url>/summary/Convex%20Hull/</url>
    <content><![CDATA[<p>在一个实数向量空间 $\mathcal{V}$ 中，对于给定集合 $X$，所有包含 $X$ 的凸集的交集 $S$ 被称为 $X$ 的凸包。$X$ 的凸包可以用 $X$ 内所有点 $(x_1,x_2,…,x_n)$ 的线性组合来构造。在二维欧氏空间中，凸包可以想象为一条刚好包括所有点的橡皮圈。</p>
<p>以不严谨的语言来描述，给定二维平面上的点集，凸包就是将最外层的点连接起来构成的凸多边形，它能包含点集中的所有点。如图-1，假设平面共有 $p_0 \sim p_{12}$ 共 13 个点，过某些点作一个多边形，使这个多边形能把所有点都“包”起来。当这个多边形是凸多边形的时候，就叫它“凸包”。</p>
<span id="more"></span>
<center><b><font size="9">凸包算法</font></b></center>

<h1 id="1-概念"><a href="#1-概念" class="headerlink" title="1  概念"></a>1  概念</h1><p>在一个实数向量空间 $\mathcal{V}$ 中，对于给定集合 $X$，所有包含 $X$ 的凸集的交集 $S$ 被称为 $X$ 的凸包。$X$ 的凸包可以用 $X$ 内所有点 $(x_1,x_2,…,x_n)$ 的线性组合来构造。在二维欧氏空间中，凸包可以想象为一条刚好包括所有点的橡皮圈。</p>
<p>以不严谨的语言来描述，给定二维平面上的点集，凸包就是将最外层的点连接起来构成的凸多边形，它能包含点集中的所有点。如图-1，假设平面共有 $p_0 \sim p_{12}$ 共 13 个点，过某些点作一个多边形，使这个多边形能把所有点都“包”起来。当这个多边形是凸多边形的时候，就叫它“凸包”。 </p>
<center>
    <img src="/summary/Convex%20Hull/1 凸包示意.png">
    <div>
        图-1 凸包示意
    </div>
</center>




<h1 id="2-解法一（Graham-扫描法）"><a href="#2-解法一（Graham-扫描法）" class="headerlink" title="2  解法一（Graham 扫描法）"></a>2  解法一（Graham 扫描法）</h1><ul>
<li>时间复杂度：$O(n\log n)$</li>
</ul>
<h2 id="2-1-计算步骤"><a href="#2-1-计算步骤" class="headerlink" title="2.1  计算步骤"></a>2.1  计算步骤</h2><p>Graham 扫描的思想是先找到凸包上的一个点，然后从那个点开始按逆时针方向逐个找凸包上的点，实际上就是进行极角排序，然后对其查询使用。 以图-2 为例，Graham 扫描法计算步骤为：</p>
<center>
    <img src="/summary/Convex%20Hull/2 Graham扫描初始图.png">
    <div>
        图-2 Graham扫描初始图
    </div>
</center>


<ol>
<li>把所有点放在二维坐标系中，则纵坐标最小的点一定是凸包上的点，如图-2 中 $P_0$。</li>
<li>将所有点的坐标平移，使 $P_0$ 作为原点。</li>
<li>计算各个点相对于 $P_0$ 的幅角 $\alpha$ ，按从小到大的顺序对各个点排序。当 $\alpha$ 相同时，距离 $P_0$ 比较近的排在前面。如图-2 得到的结果为  $P_1,P_2,P_3,P_4,P_5,P_6,P_7,P_8$。由几何知识可以知道，结果中第一个点 $P_1$ 和最后一个点 $P_8$ 一定是凸包上的点。以上，已经知道了凸包上的第一个点 $P_0$ 和第二个点 $P_1$，把它们放在栈 $S$ 里。把 $P_2$ 压入栈 $S$，假设 $S$ 的栈底元素为 $S[0]$，栈顶元素为 $S[t]$。</li>
<li>对 $i$ 遍历 3 到 8（所有点的最大下标）：</li>
<li>连接 $S[t-1]$ 和 $S[t]$ 得向量 $v_1$，连接 $S[t-1]$ 和 $P_i$ 得向量 $v_2$。</li>
<li>若 $v_2$ 的方向相对于 $v_1$ 为左转，则把 $P_i$ 压入 $S$，继续步骤 4，否则进入步骤 7。</li>
<li>$S[t]$ 不满足凸包要求，把 $S[t]$ 弹出，继续步骤 5。</li>
</ol>
<p>最后，栈中的元素就是凸包上的点。计算的动态过程如图-3。</p>
<center>
    <img src="/summary/Convex%20Hull/3 Graham扫描法计算过程.gif">
    <div>
        图-3 Graham扫描法计算过程
    </div>
</center>




<h2 id="2-2-算法"><a href="#2-2-算法" class="headerlink" title="2.2  算法"></a>2.2  算法</h2><center>
    <img src="/summary/Convex%20Hull/4 Graham扫描法算法.png">
    <div>
        图-4 Graham扫描法算法
    </div>
</center>




<h2 id="2-3-CPP-代码"><a href="#2-3-CPP-代码" class="headerlink" title="2.3  CPP 代码"></a>2.3  CPP 代码</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cmath&gt;</span></span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">mpoint</span> &#123;</span><br><span class="line"><span class="comment">//class point(x, y)</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="type">double</span> x;</span><br><span class="line">    <span class="type">double</span> y;</span><br><span class="line">    <span class="built_in">mpoint</span>(<span class="type">double</span> xx = <span class="number">0</span>, <span class="type">double</span> yy = <span class="number">0</span>) &#123;</span><br><span class="line">        x = xx;</span><br><span class="line">        y = yy;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">&#125;;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">get_miny_point_id</span><span class="params">(mpoint *points, <span class="type">int</span> size)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//get the point with min_y</span></span><br><span class="line">    <span class="type">int</span> min_id = <span class="number">0</span>;</span><br><span class="line">    <span class="type">double</span> miny = <span class="number">10000</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; size; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (points[i].y &lt; miny) &#123;</span><br><span class="line">            miny = points[i].y;</span><br><span class="line">            min_id = i;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> min_id;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">get_cos</span><span class="params">(mpoint *points, <span class="type">double</span> *mcos, <span class="type">int</span> id, <span class="type">int</span> size)</span> </span>&#123;  </span><br><span class="line">    <span class="comment">//get point&#x27;s cos</span></span><br><span class="line">    <span class="type">double</span> coss;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; size; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (i == id) &#123;</span><br><span class="line">            mcos[i] = <span class="number">2</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            coss = (points[i].x - points[id].x) / <span class="built_in">sqrt</span>((points[i].x - points[id].x) * (points[i].x - points[id].x) + (points[i].y - points[id].y) * (points[i].y - points[id].y));</span><br><span class="line">            mcos[i] = coss;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">sort_points</span><span class="params">(mpoint *points, <span class="type">double</span> *mcos, <span class="type">int</span> size)</span> </span>&#123;   </span><br><span class="line">    <span class="comment">//sort the points</span></span><br><span class="line">    <span class="type">int</span> i, j;</span><br><span class="line">    <span class="type">double</span> temp_cos;</span><br><span class="line">    mpoint temp_point;</span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; size; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; size - i - <span class="number">1</span>; j++) &#123;      </span><br><span class="line">            <span class="comment">//bubble sorting</span></span><br><span class="line">            <span class="keyword">if</span> (mcos[j] &lt; mcos[j + <span class="number">1</span>]) &#123;</span><br><span class="line">                temp_cos = mcos[j];</span><br><span class="line">                mcos[j] = mcos[j + <span class="number">1</span>];</span><br><span class="line">                mcos[j + <span class="number">1</span>] = temp_cos;</span><br><span class="line">                </span><br><span class="line">                temp_point = points[j];</span><br><span class="line">                points[j] = points[j + <span class="number">1</span>];</span><br><span class="line">                points[j + <span class="number">1</span>] = temp_point;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">ccw</span><span class="params">(mpoint a, mpoint b, mpoint c)</span> </span>&#123;          </span><br><span class="line">    <span class="comment">//judge if it is couter-colockwise</span></span><br><span class="line">    <span class="type">double</span> area = (b.x-a.x) * (c.y-a.y) - (b.y-a.y) * (c.x-a.x);</span><br><span class="line">    <span class="keyword">if</span> (area &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>; <span class="comment">// clockwise</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (area &gt; <span class="number">0</span>) <span class="keyword">return</span> <span class="number">1</span>; <span class="comment">// counter-clockwise</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">0</span>; <span class="comment">// collinear</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">get_outpoint</span><span class="params">(mpoint *points, <span class="type">int</span> size)</span> </span>&#123;    </span><br><span class="line">    <span class="comment">//get points in stack</span></span><br><span class="line">    vector&lt;mpoint&gt; outpoint;</span><br><span class="line">    outpoint.<span class="built_in">push_back</span>(points[<span class="number">0</span>]);</span><br><span class="line">    outpoint.<span class="built_in">push_back</span>(points[<span class="number">1</span>]);</span><br><span class="line">    <span class="type">int</span> i = <span class="number">2</span>;</span><br><span class="line">    <span class="keyword">while</span> (i != size) &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">ccw</span>(outpoint[outpoint.<span class="built_in">size</span>() - <span class="number">2</span>], outpoint[outpoint.<span class="built_in">size</span>() - <span class="number">1</span>], points[i]) &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            outpoint.<span class="built_in">push_back</span>(points[i]);</span><br><span class="line">            i = i + <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            outpoint.<span class="built_in">pop_back</span>();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;The outpoints are: &quot;</span> &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">0</span>; k &lt; outpoint.<span class="built_in">size</span>(); k++) &#123;</span><br><span class="line">        cout &lt;&lt; outpoint[k].x &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; outpoint[k].y &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> size = <span class="number">4</span>;</span><br><span class="line">    <span class="type">double</span> px, py;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;Please input the size: &quot;</span>;</span><br><span class="line">    cin &gt;&gt; size;</span><br><span class="line">    mpoint *points;</span><br><span class="line">    <span class="type">int</span> miny_point_id;</span><br><span class="line">    <span class="type">double</span> *mcos;</span><br><span class="line">    points = <span class="keyword">new</span> mpoint[size];</span><br><span class="line">    mcos = <span class="keyword">new</span> <span class="type">double</span>[size];</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; size; i++) &#123;</span><br><span class="line">        cin &gt;&gt; px;</span><br><span class="line">        cin &gt;&gt; py;</span><br><span class="line">        points[i].x = px;</span><br><span class="line">        points[i].y = py;</span><br><span class="line">    &#125;</span><br><span class="line">    miny_point_id = <span class="built_in">get_miny_point_id</span>(points, size);</span><br><span class="line">    <span class="built_in">get_cos</span>(points, mcos, miny_point_id, size);</span><br><span class="line">    <span class="built_in">sort_points</span>(points, mcos, size);</span><br><span class="line">    <span class="built_in">get_outpoint</span>(points, size);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="2-4-左转的判断"><a href="#2-4-左转的判断" class="headerlink" title="2.4  左转的判断"></a>2.4  左转的判断</h2><p>对于前面的 $v_2$ 相对 $v_1$ 是左转还是右转，计算方式可以使用叉乘：</p>
<script type="math/tex; mode=display">
v_1 \times v_2 =
\left[
\begin{matrix}
x_1 &x_2 \\
y_1 &y_2
\end{matrix}
\right]
=x_1 y_2 - x_2 y_1</script><p>若值为正，则 $v_1$ 在 $v_2$ 的右侧（顺时针方向），若为负则 $v_1$ 在 $v_2$ 的左侧（逆时针方向）。</p>
<h1 id="3-解法二（Javis-March-算法或-Gift-Wrapping-算法）"><a href="#3-解法二（Javis-March-算法或-Gift-Wrapping-算法）" class="headerlink" title="3  解法二（Javis-March 算法或 Gift Wrapping 算法）"></a>3  解法二（Javis-March 算法或 Gift Wrapping 算法）</h1><ul>
<li>时间复杂度：$O(NM)$，$N$ 为点的数量，$M$ 为凸包的顶点数。</li>
</ul>
<h2 id="3-1-计算步骤"><a href="#3-1-计算步骤" class="headerlink" title="3.1  计算步骤"></a>3.1  计算步骤</h2><ol>
<li>先确定凸包边界上的点 $p_1$ 和与下一个点 $p_2$（可以利用 Graham 扫描法中的方法）；</li>
<li>在点集里去寻找下一个点 $p_3$，使得 $p_1,p_3,p_2$ 满足 CCW（Counterclockwise，以逆时针方向旋转）；</li>
<li>如果满足，这就说明 $p_3$ 是更外围的点，把 $p_3$ 的值覆盖 $p_2$，即 $p_2 = p_3$；</li>
<li>重复第 3 步，直到所有点都访问过，即为找到了下一个最外边的点；</li>
<li>将 $p_2$ 的值覆盖 $p_1$，即 $p_1 = p_2$；</li>
<li>重复第 2 步，直到下一个点回到起点。</li>
</ol>
<h2 id="3-2-CPP-代码"><a href="#3-2-CPP-代码" class="headerlink" title="3.2  CPP 代码"></a>3.2  CPP 代码</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> vector&lt;pair&lt;<span class="type">double</span>, <span class="type">double</span>&gt;&gt; vpdd;</span><br><span class="line"><span class="keyword">typedef</span> pair&lt;<span class="type">double</span>, <span class="type">double</span>&gt; pdd;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">ccw</span><span class="params">(pdd a, pdd b, pdd c)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 叉乘</span></span><br><span class="line">    <span class="keyword">return</span> ((c.first - a.first) * (b.second - a.second) - (c.second - a.second) * (b.first - a.first)) &lt; <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">jarvis_march</span><span class="params">(<span class="type">const</span> vpdd &amp;input)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> n = input.<span class="built_in">size</span>();</span><br><span class="line">    <span class="type">int</span> left = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (input[i].first &lt; input[left].first) &#123;</span><br><span class="line">            left = i;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> first_point = left;</span><br><span class="line">    <span class="type">int</span> third_point;</span><br><span class="line">    <span class="keyword">do</span> &#123;</span><br><span class="line">        hull_jmarch.<span class="built_in">push_back</span>(input[first_point]);</span><br><span class="line">        third_point = (first_point + <span class="number">1</span>) % n;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (<span class="built_in">ccw</span>(input[first_point], input[i], input[third_point])) &#123;</span><br><span class="line">                third_point = i;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; </span><br><span class="line">        first_point = third_point;</span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">while</span> (first_point != left);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://blog.csdn.net/viafcccy/article/details/87483567">https://blog.csdn.net/viafcccy/article/details/87483567</a></li>
<li><a href="https://blog.csdn.net/u012328159/article/details/50808360">https://blog.csdn.net/u012328159/article/details/50808360</a></li>
<li><a href="https://blog.csdn.net/shungry/article/details/104340363">https://blog.csdn.net/shungry/article/details/104340363</a></li>
</ul>
]]></content>
      <categories>
        <category>Summary</category>
      </categories>
      <tags>
        <tag>Summary</tag>
        <tag>Knowledge</tag>
      </tags>
  </entry>
  <entry>
    <title>Corridor-Walker</title>
    <url>/papernote/Corridor-Walker/</url>
    <content><![CDATA[<p>关于论文 Corridor-Walker: Mobile Indoor Walking Assistance for Blind People to Avoid Obstacles and Recognize Intersections(ACM on Human-Computer Interaction) 的阅读总结。</p>
<span id="more"></span>
<center><b><font size="9">Corridor-Walker</font></b></center>

<h1 id="论文情况"><a href="#论文情况" class="headerlink" title="论文情况"></a>论文情况</h1><ul>
<li>标题：Corridor-Walker: Mobile Indoor Walking Assistance for Blind People to Avoid Obstacles and Recognize Intersections</li>
<li>作者：MASAKI KURIBAYASHI, SEITA KAYUKAWA, JAYAKORN VONGKULBHISAL, CHIEKO ASAKAWA, DAISUKE SATO, HIRONOBU TAKAGI, SHIGEO MORISHIMA</li>
<li>期刊：ACM on Human-Computer Interaction</li>
<li>源码：未开源</li>
</ul>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1  Introduction"></a>1  Introduction</h1><ul>
<li>应用场景：盲人面临的室内走廊导航的挑战。室内除了地面上还有壁挂式的障碍物，现有的一些系统仅能判断是否有障碍物，而无法规划避开路径。使用导盲犬可以解决问题，但是盲人并不一定都喜欢，且导盲犬十分稀有。</li>
<li>面临的挑战：<ul>
<li>现有系统多数只能识别障碍物，无法规避障碍物；</li>
<li>盲人可以使用导盲杖探测障碍物，但是在交叉路口时会遇到无法辨别方向的问题；</li>
<li>室内转弯导航系统可以传递正确的交叉路口信息，但是需要额外的基础设施。</li>
</ul>
</li>
</ul>
<p>Corridor-Walker：</p>
<ul>
<li><p>基本思想：通过使用空间化音频和文本到语音（TTS）反馈来引导用户跟踪避障路径。对于交叉路口，通过震动和 TTS 反馈用户交叉路口的存在和形状。</p>
</li>
<li><p>基本设计：使用 iPhone 12 Pro 的 LiDAR 传感器构建周围环境的 2D 占用网格地图（Occupancy Grid Map）；然后利用 A* 路径规划算法在网格图上规划一条避障路径；同时使用 YOLO v3 检测交叉路口。</p>
</li>
</ul>
<center>
    <img src="/papernote/Corridor-Walker/1 Corridor-Walker.jpg">
    <div>
        图-1  Corridor-Walker 工作方式
    </div>
</center>



<h1 id="2-System-Overview"><a href="#2-System-Overview" class="headerlink" title="2  System Overview"></a>2  System Overview</h1><center>
    <img src="/papernote/Corridor-Walker/2 System.jpg">
    <div>
        图-2  系统框架
    </div>
</center>

<p>使用智能手机的激光雷达测量与物体之间的距离。盲人使用导盲杖避开障碍物并识别交叉路口。</p>
<h2 id="2-1-Avoiding-Obstacles"><a href="#2-1-Avoiding-Obstacles" class="headerlink" title="2.1  Avoiding Obstacles"></a>2.1  Avoiding Obstacles</h2><p>一些障碍物会依墙放置，因此系统生成一条尽可能与墙保持距离的路径，并引导用户在不转向的情况下沿着路径行走。如果前方出现障碍物，则系统生成一条绕开障碍物的路径。</p>
<h2 id="2-2-Detecting-Intersections"><a href="#2-2-Detecting-Intersections" class="headerlink" title="2.2  Detecting Intersections"></a>2.2  Detecting Intersections</h2><p>盲人需要感知经过的交叉路口，同时由于用户不会依墙行走，因此系统会在交叉路口出现时对用户发出提醒，防止用户错过交叉路口。</p>
<h1 id="3-Implementation"><a href="#3-Implementation" class="headerlink" title="3  Implementation"></a>3  Implementation</h1><h2 id="3-1-Grid-Map-Construction"><a href="#3-1-Grid-Map-Construction" class="headerlink" title="3.1  Grid Map Construction"></a>3.1  Grid Map Construction</h2><p>2D 占用网格地图通过 LiDAR 获取的点云构建，最大感知范围为 5m。利用 ARKit 提供的定位算法，对每个时间帧的网格信息进行累积。用户需要以图-2 左中的方式手持手机，以便于获取网格图。</p>
<p>为计算每个网格是否可步行，需要计算每个点的法向量：</p>
<ul>
<li>先用 RANSAC（Random Sample Consensus）提取地面点云；</li>
<li>如果一个点的法向量与重力方向平行，且高度与地面相差小于 0.1m，则为 walkable 点，其他则为 non-walkable 点。</li>
</ul>
<p>确定每个点投影到 xy 平面后的网格。每个网格的大小设置为 0.15m：</p>
<ul>
<li>若网格中可行走点多于不可行走点，则网格标记为 walkable 网格（图-2A 中白色区域），否则网格标记为 non-walkable 网格（图-2A 中黑色区域）。</li>
<li>若单元格中不包含点，则标记为 no-information 网格（图-2A 中灰色区域）。</li>
</ul>
<p>系统以 10FPS 的速率处理上述算法，每次观察网格时，都会对网格进行更新，这使得系统能够处理动态障碍物。一旦障碍物被移走，系统就能将 non-walkable 网格更新为 walkable 网格。</p>
<h2 id="3-2-Path-Planning-and-Obstacle-Avoidance"><a href="#3-2-Path-Planning-and-Obstacle-Avoidance" class="headerlink" title="3.2  Path Planning and Obstacle Avoidance"></a>3.2  Path Planning and Obstacle Avoidance</h2><h3 id="3-2-1-Assigning-Cost-to-Each-Cell"><a href="#3-2-1-Assigning-Cost-to-Each-Cell" class="headerlink" title="3.2.1  Assigning Cost to Each Cell"></a>3.2.1  Assigning Cost to Each Cell</h3><p>首先，系统为每个 walkable 网格分配 $0$ 到 $\beta$ 之间的损失值。系统因此可以获得损失图，便于规划原理 non-walkable 网格的路线。</p>
<p>每个 walkable 网格的损失值计算：</p>
<ul>
<li><p>设 $\delta_i$ 为 walkable 网格 $i$ 到其最近 non-walkable 网格的距离。walkable 网格 $i$ 的损失则定义为：</p>
<script type="math/tex; mode=display">
cost_i = 
\begin{cases}
    \beta (1 - \frac{\delta_i - 1}{\alpha}),\ &\mathrm{if}\ 1 \le \delta_i \le \alpha \\
    0,\ &\mathrm{if}\ \delta_i > \alpha
\end{cases}</script><p>其中 $\alpha$ 为距离上界，以使得损失值为正数。离 non-walkable 越近的点将会得到越大的损失。</p>
</li>
<li><p>图-2A 中，损失值高的点被渲染为暗红色，较低的则为浅红色。</p>
</li>
<li><p>根据观察，取 $\alpha = 3, \beta = 50$。</p>
</li>
</ul>
<h3 id="3-2-2-Path-Planning-Algorithm"><a href="#3-2-2-Path-Planning-Algorithm" class="headerlink" title="3.2.2  Path Planning Algorithm"></a>3.2.2  Path Planning Algorithm</h3><p>首先，系统需要寻找一个执行路径规划算法的目的地：</p>
<ul>
<li>系统先在前方 $\gamma$m 内 $100 \degree$ 的扇形区域内进行采样；</li>
<li>然后系统选择采样点最长连续采样空间的中点作为目的地；</li>
<li>如果目的地落在 non-walkable 网格中，则缩短 0.5m 并重复上述过程，直到变为 0m；</li>
<li>最后系统使用 A* 路径规划算法计算到该点的路径。</li>
</ul>
<p>根据观察，取 $\gamma = 3.5$。</p>
<h3 id="3-2-3-Obstacle-Detection"><a href="#3-2-3-Obstacle-Detection" class="headerlink" title="3.2.3  Obstacle Detection"></a>3.2.3  Obstacle Detection</h3><p>尽管路径规划能找到避开障碍物的路径，但仍然需要通知用户存在障碍物以进行绕道。</p>
<p>为确定 non-walkable 是墙还是障碍物，系统在 2D 占用网格图上使用 RANSAC 算法进行平面检测，所有被 RANSAC 检测出来的网格则标记为 wall 网格，其他为 obstacle 网格。</p>
<p>然后，在用户前方 $2$m 内 $30\degree$ 的扇形区域内的网格上检测是否有障碍物：</p>
<ul>
<li>如果 obstacle 网格数量超过 30%，则系统认为前方存在障碍物，并通知用户。</li>
</ul>
<h3 id="3-2-4-Veering-Detection"><a href="#3-2-4-Veering-Detection" class="headerlink" title="3.2.4  Veering Detection"></a>3.2.4  Veering Detection</h3><p>为避免用户改变生成的路径，系统还要检测是否用户面向了正确的前进方向：</p>
<ul>
<li>首先，系统使用 ARKit 中的定位算法在占用网格地图中计算系统方向；</li>
<li>系统计算系统方向与路径规划的前进方向的夹角 $\theta$：<ul>
<li>如果 $\theta &gt; 10\degree$，则视为用户偏离了规划好的方向，系统将发出通知。</li>
</ul>
</li>
</ul>
<h2 id="3-3-Intersection-Detection"><a href="#3-3-Intersection-Detection" class="headerlink" title="3.3  Intersection Detection"></a>3.3  Intersection Detection</h2><p>系统使用 YOLO v3 检测交叉口，其在 iPhone 12 Pro 上的运行速率约为 70FPS。系统使用 2D 占用网格地图作为检测输入，避免了 RGB 相机拍照产生的运动模糊问题，检测结果为图-1 中的蓝色方框，表示此处出现交叉口。</p>
<h3 id="3-3-1-Image-Processing"><a href="#3-3-1-Image-Processing" class="headerlink" title="3.3.1  Image Processing"></a>3.3.1  Image Processing</h3><p>由于网格地图本身不包含关于用户前进方向的信息，因此对网格图像进行预处理，使这些信息变得明显：</p>
<ul>
<li>系统旋转网格地图的图像使用户的前进方向朝上（图-2C），用户的前进方向则根据过去 4s 内的位置计算得到。</li>
<li>然后移动图像，使用户在图像的中心，预处理后的图像（$128 \times 128$ 像素）用作 YOLO v3 的输入。</li>
</ul>
<h3 id="3-3-2-Training-the-YOLOv3-Detector"><a href="#3-3-2-Training-the-YOLOv3-Detector" class="headerlink" title="3.3.2  Training the YOLOv3 Detector"></a>3.3.2  Training the YOLOv3 Detector</h3><ul>
<li>目的：训练 YOLOv3 检测器使得能够识别即将到来的交叉口。</li>
<li>数据：从环境中收集 9940 张预处理好的走廊图像，并标注交叉口的位置和标签（交叉口的方向）。</li>
<li>标签：<ul>
<li>例：如果交叉口只引导用户去左边，那么标记为 “Left, Back”。</li>
<li>“Left, Back” 和 “Right, Back” 由于有相同的拓扑形状，因此定义为 “L-Shaped”，同理其他交叉口被分类为 “T-Shaped”、“Rotated T-Shaped” 和 “X-Shaped”。</li>
</ul>
</li>
<li>阈值：将 YOLO v3 检测器的置信度阈值设为 $0.2$。</li>
</ul>
<h3 id="3-3-3-Detetermining-the-Distance-to-Intersection"><a href="#3-3-3-Detetermining-the-Distance-to-Intersection" class="headerlink" title="3.3.3  Detetermining the Distance to Intersection"></a>3.3.3  Detetermining the Distance to Intersection</h3><p>如图-2D，用户到交叉口的距离定义为 bounding box 下边界到用户的像素数。由于一个像素（网格）在真实世界中对应 0.15m，因此像素数乘以 0.15m 即为用户到交叉口的距离。</p>
<p>注意：当 bounding box 包含图像中心点时，说明用户行走到了检测到的交叉口。</p>
<h3 id="3-3-4-Evaluation"><a href="#3-3-4-Evaluation" class="headerlink" title="3.3.4  Evaluation"></a>3.3.4  Evaluation</h3><center>
    <img src="/papernote/Corridor-Walker/3 Result of Intersection Detection.jpg">
    <div>
        图-3  交叉口检测结果
    </div>
</center>
从获取的数据集中取出 1215 张预处理的图像作为训练集。评估参数：

- 在不同距离时的准确率和召回率，如图-3A。
- 检测到交叉口标签的最远距离，如图-3B。其中 “L” 表示 “Left”，“R” 表示 “Right”，“B” 表示 “Back”，“F” 表示 “Front”。平均上，在距离 2.47m 时能够识别到交叉口形状。



### 3.3.5  Confirming the Existence of an Intersection

如果走廊有不均匀的结构，如 “凹陷”，则检测器很有可能将其识别为交叉口。因此提出了一种判断算法：

- 当用户处于检测到的交叉口时，系统通过要求用户扫描十字路口可能通向的两侧（左/右），测量交叉口两侧最远的 walkable 网格单元。如果边界框的最近一侧与这个最远 walkable 网格距离超过阈值 $\epsilon$m，则系统认为此处有一条路径。取 $\epsilon = 1.5$。



## 3.4  Intersection of Corridor-Walker

<center>
    <img src="/papernote/Corridor-Walker/4 Interface of System.jpg">
    <div>
        图-4  系统接口
    </div>
</center>

<p>系统使用 TTS，空间化的音频和震动反馈。系统尽量少使用 TTS 来避免用户较高的认知符合。设计上：</p>
<ul>
<li>TTS 反馈用于传达交叉口的形状，告知用户绕行。</li>
<li>空间化音频指导用户避免偏离轨迹。</li>
<li>震动反馈用于告知用户路口的存在，并提醒即将发生碰撞。</li>
</ul>
<h3 id="3-4-1-Conveying-Intersection-Related-Information"><a href="#3-4-1-Conveying-Intersection-Related-Information" class="headerlink" title="3.4.1  Conveying Intersection-Related Information"></a>3.4.1  Conveying Intersection-Related Information</h3><ul>
<li><p>当系统检测到前方有交叉口时，系统就会发出震动。将震动设计为单脉冲震动，脉冲持续时间为 0.1s，间隔 0.5s。</p>
</li>
<li><p>当用户进入交叉口区域时，系统使用 TTS 告知用户扫描特定一侧（左/右）的信息，例如：若检测到 “Left, Back” 或 “Left, Front, Back”，系统将提醒用户扫描左侧。</p>
<p>此指令的目的在于让用户确认检测到的交叉口是否是正确，一旦检测成功，系统将告知交叉口通往哪条路，否则系统保持沉默。当用户进入 “Left, Right, Back” 交叉口时，指令示例：</p>
<ol>
<li>用户进入交叉口：<em>Scan Left and Right</em>；</li>
<li>用户扫描结束，发现仅有左侧可以通行：(Intersection to) Left。</li>
</ol>
</li>
</ul>
<h3 id="3-4-2-Conveying-Veering-Related-Information"><a href="#3-4-2-Conveying-Veering-Related-Information" class="headerlink" title="3.4.2  Conveying Veering-Related Information"></a>3.4.2  Conveying Veering-Related Information</h3><p>系统使用空间化音频反馈用户正确的旋转角度：</p>
<ul>
<li>当用户偏离预定路径时，系统提供反馈以修正用户行走路径。</li>
<li>如果用户面向左（右），而实际应该面向右（左），系统将用骨传导耳机在右（左）侧发出正弦音（持续 0.25s，间隔 0.25s，频率 400Hz）。若听不到正弦音，说明面向了正确的方向。</li>
</ul>
<h3 id="3-4-3-Conveying-Obstacle-Related-Information"><a href="#3-4-3-Conveying-Obstacle-Related-Information" class="headerlink" title="3.4.3  Conveying Obstacle-Related Information"></a>3.4.3  Conveying Obstacle-Related Information</h3><p>当在 2m 内检测到障碍物时，系统将使用 TTS 告知用户使用哪条路绕行：</p>
<ul>
<li>例如：当障碍物出现在左侧的墙边，系统将提示 ”<em>Make a detour to the right</em>“。</li>
</ul>
<p>如果障碍物（包括墙）在用户前方 1m 内，系统将持续震动，知道用户面对安全的方向。</p>
<h1 id="4-User-Study"><a href="#4-User-Study" class="headerlink" title="4  User Study"></a>4  User Study</h1><p>招募了盲人进行有效性实验，对比使用导盲杖和系统相结合（system-aided）、仅使用导盲杖（cane-only）的实验结果。</p>
<h2 id="4-1-实验设置"><a href="#4-1-实验设置" class="headerlink" title="4.1  实验设置"></a>4.1  实验设置</h2><ul>
<li>参与者：使用电子邮件招募了 14 名盲人，他们有超过 2 年的日常生活主要使用导盲杖作为导航工具和智能手机的经历。</li>
</ul>
<center>
    <img src="/papernote/Corridor-Walker/表-1 Paticipants.jpg">
    <div>
        表-1  实验参与者信息和实验评价
    </div>
</center>

<ul>
<li>实验任务：<ul>
<li>（1）识别并在交叉口转弯：参与者在交叉口转向至特定方向，然后回答交叉口的形状。</li>
<li>（2）障碍物躲避</li>
<li>（3）在有障碍物的长走廊进行导航</li>
</ul>
</li>
<li>评价指标：<ul>
<li>正确回答交叉口形状</li>
<li>不同任务下的完成时间</li>
<li>不同任务下的导盲杖碰到障碍物或者墙壁的次数</li>
</ul>
</li>
</ul>
<center>
    <img src="/papernote/Corridor-Walker/5 Routes.jpg">
    <div>
        图-5  任务 2 和 3 的实验路线
    </div>
</center>



<h1 id="5-Results"><a href="#5-Results" class="headerlink" title="5  Results"></a>5  Results</h1><ul>
<li>正确回答交叉口形状（L，T，Rotated T，X）：<ul>
<li>cane-only：71.4%，21.4%，28.6%，0%</li>
<li>system-aided：92.9%，92.9%，100%，50%</li>
</ul>
</li>
<li>完成时间，导盲杖碰墙次数：</li>
</ul>
<center>
    <img src="/papernote/Corridor-Walker/表-2 Quantitative Evaluation.jpg">
    <div>
        表-2  各项任务的完成时间与导盲杖碰墙（障碍物）次数
    </div>
</center>
]]></content>
      <categories>
        <category>Paper Note</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Paper Note</tag>
        <tag>Indoor Navidation</tag>
      </tags>
  </entry>
  <entry>
    <title>Cylinder3D</title>
    <url>/papernote/Cylinder3D/</url>
    <content><![CDATA[<p>关于论文 Cylinder3D: An Effective 3D Framework for Driving-scene LiDAR Semantic Segmentation (CVPR 2021) 的阅读总结。</p>
<span id="more"></span>
<center><b><font size="9">Cylinder3D</font></b></center>

<h1 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h1><ul>
<li>题目：Cylinder3D: An Effective 3D Framework for Driving-scene LiDAR Semantic Segmentation</li>
<li>作者：Hui Zhou, Xinge Zhu, Xiao Song, Yuexin Ma, Zhe Wang, Hongsheng Li,Dahua Lin</li>
<li>期刊/会议：CVPR 2021</li>
</ul>
<h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1  介绍"></a>1  介绍</h1><p>Cylinder3D 是一个使用三维激光雷达对 3D 点云进行语义分割的算法，其主要贡献有：</p>
<ul>
<li>研究了最先进的网络架构和不同的点特征表示，揭示了直接处理点云而不需要 3D 到 2D 投影是获得卓越分割性能的关键。</li>
<li>提出了一种柱面分割点云编码方案，该方案更好地遵循了三维驾驶场景点云的固有分布规律，并开发了一种基于三维卷积的框架。其中，设计了非对称残差块作为基本模块，并提出了一种新的维分解块，以循序渐进的方式探索上下文。</li>
<li>提出的 LiDAR 分割算法在驾驶场景语义分割基准上的性能优于目前最先进的分割算法，有 6% mIoU 的增益。</li>
</ul>
<h1 id="2-Methodology"><a href="#2-Methodology" class="headerlink" title="2  Methodology"></a>2  Methodology</h1><h2 id="2-1-3D-点云的学习"><a href="#2-1-3D-点云的学习" class="headerlink" title="2.1  3D 点云的学习"></a>2.1  3D 点云的学习</h2><p>三维点表示研究室外场景点云与室内场景点云有显著差异：</p>
<ol>
<li><p>一个驾驶场景点云可能覆盖非常大的区域，可达 100 多米。</p>
</li>
<li><p>通常包含较多的点（ &gt; 10 万个点），但比室内场景稀疏很多。</p>
</li>
</ol>
<p>因此，针对稠密点和固定数量点的室内分割方法难以适应点密度变化巨大的驾驶场景。现有的户外激光雷达分割方法主要是通过投影将 3D 点云转化为 2D 表示，包括球面投影和鸟瞰投影，然后采用 2D 卷积处理 2D 网格表示。</p>
<p>然而，如图-1（右），2D 网格表示中的局部空间格局并不能很好地捕捉三维几何结构。2D 网格中的红色框代表了分布在不同空间位置的点，3D-to- 2D 投影方法可能无法对某些 3D 几何结构进行编码，导致模式提取不准确。</p>
<p>对 2D、2.5D 和 3D 之间的各种分区和网络进行了广泛的实验，从结果来看，一致的性能增益表明了 3D 分区和 3D 网络的有效性。</p>
<center>
    <img src="/papernote/Cylinder3D/1 3D-to-2D.png">
    <div>
        图-1  3D-to-2D 方法的结果
    </div>
</center>



<h2 id="2-2-具体框架"><a href="#2-2-具体框架" class="headerlink" title="2.2  具体框架"></a>2.2  具体框架</h2><p>室外点云覆盖了大量不同的城市场景，语义分割的目的是为点云中的每个点分配一个语义标签。通过投影获得的 2D 表示会放弃许多可用的 3D 结构。因此，提出了一种新的基于 3D 表示和神经网络的户外激光雷达分割方法。</p>
<p>如图-2 所示，该框架由两个组件组成：</p>
<ul>
<li>3D 柱面分区（获取 3D 表示）；</li>
<li>3D U-Net（处理 3D 表示）。</li>
</ul>
<p>特别地，设计了两个模块来适应户外点云的特性：</p>
<ul>
<li>非对称残块（Asymmetrical Residual Block）：匹配经常出现在驾驶场景中的长方体状的物体（汽车、卡车、摩托车等）；</li>
<li>基于维度分解的上下文建模模块（Dimension-decomposition based Context Modeling Module，DDCM）：以分解聚合的方式对点云中的高阶上下文信息进行挖掘。</li>
</ul>
<center>
    <img src="/papernote/Cylinder3D/2 pipeline.png">
    <div>
        图-2  整体框架
    </div>
</center>


<h2 id="2-3-Cylinder-Partition"><a href="#2-3-Cylinder-Partition" class="headerlink" title="2.3  Cylinder Partition"></a>2.3  Cylinder Partition</h2><p>室外场景 LiDAR 点云具有区域密度变化巨大的特性，附近区域的密度远大于远区。因此，使用圆柱坐标系来代替笛卡尔网格划分：利用逐渐增大的网格覆盖较远的区域，使点在不同区域的分布更加均匀，与室外点的分布相匹配。</p>
<p>此外，与基于投影的方法将点投影到 2D 视图不同，圆柱坐标系保留了 3D 网格表示，从而保留了物体的几何结构。</p>
<p>流程如图-3：</p>
<ol>
<li><p>先将直角坐标系上的点变换到柱面坐标系，在柱面坐标系下计算半径 $\rho$ 和方位角 $\theta$，变换点 $(x, y, z)$ 到点 $(\rho, \theta, z)$；</p>
</li>
<li><p>柱面分割是为了均匀地分割这三个维度（注意：这种分割表示区域越远，体素越大）。</p>
</li>
<li><p>将这些柱面网格表示输入基于 MLP 的 PointNet 以获得柱面 point-wise 特征。</p>
</li>
</ol>
<p>经过这些步骤，可以得到 3D 柱面表示 $R \in C \times H \times W \times L$，其中 $C$ 表示特征维数，$H, W, L$ 表示某一圆柱块。</p>
<center>
    <img src="/papernote/Cylinder3D/3 Cylinder Partition.png">
    <div>
        图-3  Cylinder Partition 流程
    </div>
</center>


<h2 id="2-4-非对称残差块"><a href="#2-4-非对称残差块" class="headerlink" title="2.4  非对称残差块"></a>2.4  非对称残差块</h2><center>
    <img src="/papernote/Cylinder3D/4 非对称残差块和DDCM模块.png">
    <div>
        图-4  非对称残差块和 DDCM 模块
    </div>
</center>

<p>在自动驾驶场景中，存在大量的长方体物体，包括汽车、卡车、公共汽车和摩托车。通过采用不对称卷积核对矩形目标区域进行匹配，因此设计了非对称残差块来满足这类长方体对象的特性，同时显著降低了传统 3D 卷积核的计算成本。</p>
<p>具体地，使用内核 $= 3 \times 1 \times 3$ 的卷积后，使用内核 $= 1 \times 3 \times 3$ 的卷积。相当于滑动两层网络，其接受域与内核 $= 3 \times 3 \times 3$ 的卷积相同，但比一个输出相同数量结果的 $3 \times 3 \times 3$ 卷积与减少了 33% 的计算成本。</p>
<p>非对称残差块是下采样块和上采样块的基本组成部分（如图-2）：</p>
<ul>
<li>下采样块由一个非对称残差块和一个 $stride=2$ 的 3D 卷积组成。</li>
<li>上采样块融合了低阶特征，并用非对称残块对融合特征进行处理。</li>
</ul>
<h2 id="2-5-DDCM-模块"><a href="#2-5-DDCM-模块" class="headerlink" title="2.5  DDCM 模块"></a>2.5  DDCM 模块</h2><p>由于上下文的差异较大（对于 3D 空间，其上下文在点云与点云之间存在巨大的差异），因此上下文张量应该是 high-rank（高阶）的，才有足够的容量对上下文信息进行编码。</p>
<p>为这个上下文特征建模需要巨大的成本，特别是在 3D 空间中（上下文的高阶性）。受高阶矩阵分解理论的启发，可以将高阶上下文分解为几个低阶表示：</p>
<ul>
<li>这个高阶上下文可以被划分为三个维度：高度、宽度和深度，而三个片段都是低阶的。</li>
<li>然后，使用这些片段构建完整的高级上下文。</li>
</ul>
<p>通过这种分解-聚合策略处理了基于低阶约束的不同视角的高阶困难。如图-2（下）所示，三个秩为 $1$ 的核（$3 \times 1 \times 1$，$1 \times 3 \times 1$，$1 \times 1 \times 3$）在所有三个维度上生成这些低阶编码。然后 Sigmoid 函数对卷积结果进行调制，生成每个维度的权值，其中基于不同视图的秩 1 张量挖掘共现上下文信息。聚合所有三个低阶上下文的激活结果，获得表示完整上下文特征的整体。</p>
<h2 id="2-6-Point-wise-细化模块"><a href="#2-6-Point-wise-细化模块" class="headerlink" title="2.6  Point-wise 细化模块"></a>2.6  Point-wise 细化模块</h2><p>区块检测的方法是目前常用的方法，但不管是基于立方体还是基于圆柱体的方法都有一个不可避免的缺点，即不同类别的点可能被划分到同一个 cell（voxel）中，这将导致导致信息的丢失而影响精度。而采用块点结合的方式，能够有效的缓解错误 cell-label 编码的干扰，而提高识别的精度。</p>
<p><strong>具体的实现方式：</strong>首先，基于 point-voxel 映射表将 voxel-wise 特征投影到 point-wise。然后，point-wise 模块将 3D 卷积网络前后的点特征作为输入，并将它们融合在一起以细化输出。</p>
<p>在得到 voxel-wise 的 feature 之后，可以映射回 point-wise，然后再使用 MLP 得到 point-wise 的 label。</p>
<p>获得 voxel 的 label，然后将当前 voxel 中的所有点预测为统一个 label。</p>
<h2 id="2-7-网络优化"><a href="#2-7-网络优化" class="headerlink" title="2.7  网络优化"></a>2.7  网络优化</h2><p>柱面划分利用具有 BatchNorm 和 ReLU 的 4 层 MLP 网络提取每个点的点特征，并在体素内，做 channel-wise 的 max 提取出体素特征。3D 分割 backbone 来源于 UNet，其中 3D 卷积是改编的稀疏卷积。如上所述，用非对称残块代替传统残块，并在最终预测前插入 DDCM 模块。</p>
<p>分割主干的输入为 $C \times H \times W \times L$ 张量。分割头采用一个 $3 \times 3 \times 3$ 核的 3D 卷积层作为轻量级的分割头。经过整个 pipeline，得到基于体素的预测，其大小为 $Class \times H \times W \times L$（$Class$ 为类别数量）。</p>
<p>对于网络优化，使用加权交叉熵损失和 lovasz-softmax 损失，以最大限度地提高类的点精度和交叉-联合得分。两种损失的权重相同。因此，总损失是：</p>
<script type="math/tex; mode=display">
\zeta_{total} = \zeta_{iou} + \zeta_{acc}</script><p>在优化器中，使用初始学习率为 $0.001$ 的 Adam。</p>
<h1 id="3-实验"><a href="#3-实验" class="headerlink" title="3  实验"></a>3  实验</h1><h2 id="3-1-数据集和评估"><a href="#3-1-数据集和评估" class="headerlink" title="3.1  数据集和评估"></a>3.1  数据集和评估</h2><ul>
<li><p>数据集：SemanticKITTI。</p>
</li>
<li><p>评估：在所有语义类别上使用 $mIoU$（平均交并比）度量，其中</p>
<script type="math/tex; mode=display">
  IoU_i = \frac{TP_i}{TP_i + FP_i + FN_i}</script><p>  表示在类别 $i$ 上的交并比，$mIoU$ 为所有 $IoU_i$ 的均值。</p>
</li>
</ul>
<h2 id="3-2-试验结果可视化"><a href="#3-2-试验结果可视化" class="headerlink" title="3.2  试验结果可视化"></a>3.2  试验结果可视化</h2><center>
    <img src="/papernote/Cylinder3D/5 实验结果.png">
    <div>
        图-5  实验结构
    </div>
</center>



<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://zhuanlan.zhihu.com/p/500643228">https://zhuanlan.zhihu.com/p/500643228</a></li>
<li><a href="https://juejin.cn/post/7069015256495980552">https://juejin.cn/post/7069015256495980552</a></li>
</ul>
<h1 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h1><ul>
<li><a href="https://github.com/xinge008/Cylinder3D">https://github.com/xinge008/Cylinder3D</a></li>
</ul>
]]></content>
      <categories>
        <category>Paper Note</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Paper Note</tag>
        <tag>Semantic Segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title>DETR</title>
    <url>/papernote/DETR/</url>
    <content><![CDATA[<p>关于论文 End-to-End Object Detection with Transformers (ECCV 2020) 的阅读总结。</p>
<span id="more"></span>
<p><img src="/papernote/DETR/幻灯片1.png" alt><br><img src="/papernote/DETR/幻灯片2.png" alt><br><img src="/papernote/DETR/幻灯片3.png" alt><br><img src="/papernote/DETR/幻灯片4.png" alt><br><img src="/papernote/DETR/幻灯片5.png" alt><br><img src="/papernote/DETR/幻灯片6.png" alt><br><img src="/papernote/DETR/幻灯片7.png" alt><br><img src="/papernote/DETR/幻灯片8.png" alt><br><img src="/papernote/DETR/幻灯片9.png" alt><br><img src="/papernote/DETR/幻灯片10.png" alt><br><img src="/papernote/DETR/幻灯片11.png" alt><br><img src="/papernote/DETR/幻灯片12.png" alt><br><img src="/papernote/DETR/幻灯片13.png" alt><br><img src="/papernote/DETR/幻灯片14.png" alt><br><img src="/papernote/DETR/幻灯片15.png" alt><br><img src="/papernote/DETR/幻灯片16.png" alt><br><img src="/papernote/DETR/幻灯片17.png" alt><br><img src="/papernote/DETR/幻灯片18.png" alt><br><img src="/papernote/DETR/幻灯片19.png" alt><br><img src="/papernote/DETR/幻灯片20.png" alt><br><img src="/papernote/DETR/幻灯片21.png" alt><br><img src="/papernote/DETR/幻灯片22.png" alt><br><img src="/papernote/DETR/幻灯片23.png" alt><br><img src="/papernote/DETR/幻灯片24.png" alt><br><img src="/papernote/DETR/幻灯片25.png" alt><br><img src="/papernote/DETR/幻灯片26.png" alt></p>
]]></content>
      <categories>
        <category>Paper Note</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Paper Note</tag>
        <tag>Object Detection</tag>
        <tag>Transformers</tag>
      </tags>
  </entry>
  <entry>
    <title>DGS (Distributed Gauss-Siedel)</title>
    <url>/papernote/DGS%20(Distributed%20Gauss-Siedel)/</url>
    <content><![CDATA[<p>关于论文 Distributed Trajectory Estimation with Privacy and Communication Constraints: a Two-Stage Distributed Gauss-Seidel Approach (ICRA 2016) 的阅读总结。</p>
<span id="more"></span>
<p><img src="/papernote/DGS%20(Distributed%20Gauss-Siedel)/DGS (Distributed Gauss-Siedel" alt>/幻灯片1.png)<br><img src="/papernote/DGS%20(Distributed%20Gauss-Siedel)/DGS (Distributed Gauss-Siedel" alt>/幻灯片2.png)<br><img src="/papernote/DGS%20(Distributed%20Gauss-Siedel)/DGS (Distributed Gauss-Siedel" alt>/幻灯片3.png)<br><img src="/papernote/DGS%20(Distributed%20Gauss-Siedel)/DGS (Distributed Gauss-Siedel" alt>/幻灯片4.png)<br><img src="/papernote/DGS%20(Distributed%20Gauss-Siedel)/DGS (Distributed Gauss-Siedel" alt>/幻灯片5.png)<br><img src="/papernote/DGS%20(Distributed%20Gauss-Siedel)/DGS (Distributed Gauss-Siedel" alt>/幻灯片6.png)<br><img src="/papernote/DGS%20(Distributed%20Gauss-Siedel)/DGS (Distributed Gauss-Siedel" alt>/幻灯片7.png)<br><img src="/papernote/DGS%20(Distributed%20Gauss-Siedel)/DGS (Distributed Gauss-Siedel" alt>/幻灯片8.png)<br><img src="/papernote/DGS%20(Distributed%20Gauss-Siedel)/DGS (Distributed Gauss-Siedel" alt>/幻灯片9.png)<br><img src="/papernote/DGS%20(Distributed%20Gauss-Siedel)/DGS (Distributed Gauss-Siedel" alt>/幻灯片10.png)<br><img src="/papernote/DGS%20(Distributed%20Gauss-Siedel)/DGS (Distributed Gauss-Siedel" alt>/幻灯片11.png)<br><img src="/papernote/DGS%20(Distributed%20Gauss-Siedel)/DGS (Distributed Gauss-Siedel" alt>/幻灯片12.png)<br><img src="/papernote/DGS%20(Distributed%20Gauss-Siedel)/DGS (Distributed Gauss-Siedel" alt>/幻灯片13.png)<br><img src="/papernote/DGS%20(Distributed%20Gauss-Siedel)/DGS (Distributed Gauss-Siedel" alt>/幻灯片14.png)<br><img src="/papernote/DGS%20(Distributed%20Gauss-Siedel)/DGS (Distributed Gauss-Siedel" alt>/幻灯片15.png)<br><img src="/papernote/DGS%20(Distributed%20Gauss-Siedel)/DGS (Distributed Gauss-Siedel" alt>/幻灯片16.png)<br><img src="/papernote/DGS%20(Distributed%20Gauss-Siedel)/DGS (Distributed Gauss-Siedel" alt>/幻灯片17.png)<br><img src="/papernote/DGS%20(Distributed%20Gauss-Siedel)/DGS (Distributed Gauss-Siedel" alt>/幻灯片18.png)<br><img src="/papernote/DGS%20(Distributed%20Gauss-Siedel)/DGS (Distributed Gauss-Siedel" alt>/幻灯片19.png)<br><img src="/papernote/DGS%20(Distributed%20Gauss-Siedel)/DGS (Distributed Gauss-Siedel" alt>/幻灯片20.png)<br><img src="/papernote/DGS%20(Distributed%20Gauss-Siedel)/DGS (Distributed Gauss-Siedel" alt>/幻灯片21.png)<br><img src="/papernote/DGS%20(Distributed%20Gauss-Siedel)/DGS (Distributed Gauss-Siedel" alt>/幻灯片22.png)<br><img src="/papernote/DGS%20(Distributed%20Gauss-Siedel)/DGS (Distributed Gauss-Siedel" alt>/幻灯片23.png)<br><img src="/papernote/DGS%20(Distributed%20Gauss-Siedel)/DGS (Distributed Gauss-Siedel" alt>/幻灯片24.png)<br><img src="/papernote/DGS%20(Distributed%20Gauss-Siedel)/DGS (Distributed Gauss-Siedel" alt>/幻灯片25.png)<br><img src="/papernote/DGS%20(Distributed%20Gauss-Siedel)/DGS (Distributed Gauss-Siedel" alt>/幻灯片26.png)<br><img src="/papernote/DGS%20(Distributed%20Gauss-Siedel)/DGS (Distributed Gauss-Siedel" alt>/幻灯片27.png)<br><img src="/papernote/DGS%20(Distributed%20Gauss-Siedel)/DGS (Distributed Gauss-Siedel" alt>/幻灯片28.png)<br><img src="/papernote/DGS%20(Distributed%20Gauss-Siedel)/DGS (Distributed Gauss-Siedel" alt>/幻灯片29.png)<br><img src="/papernote/DGS%20(Distributed%20Gauss-Siedel)/DGS (Distributed Gauss-Siedel" alt>/幻灯片30.png)<br><img src="/papernote/DGS%20(Distributed%20Gauss-Siedel)/DGS (Distributed Gauss-Siedel" alt>/幻灯片31.png)<br><img src="/papernote/DGS%20(Distributed%20Gauss-Siedel)/DGS (Distributed Gauss-Siedel" alt>/幻灯片32.png)<br><img src="/papernote/DGS%20(Distributed%20Gauss-Siedel)/DGS (Distributed Gauss-Siedel" alt>/幻灯片33.png)<br><img src="/papernote/DGS%20(Distributed%20Gauss-Siedel)/DGS (Distributed Gauss-Siedel" alt>/幻灯片34.png)</p>
]]></content>
      <categories>
        <category>Paper Note</category>
      </categories>
      <tags>
        <tag>Paper Note</tag>
        <tag>LiDAR SLAM</tag>
        <tag>Distributed SLAM</tag>
        <tag>Gauss Siedel</tag>
      </tags>
  </entry>
  <entry>
    <title>Data-Driven Navigation/Bluetooth Indoor Localization</title>
    <url>/papernote/Data-Driven%20Navigation%20Bluetooth%20Indoor%20Localization/</url>
    <content><![CDATA[<p>论文 A Data-Driven Inertial Navigation/Bluetooth Fusion Algorithm for Indoor Localization (IEEE Sensors Journal, 2022) 阅读笔记。</p>
<span id="more"></span>
<center><b><font size="9">Data-Driven Navigation/Bluetooth Indoor Localization</font></b></center>

<h1 id="论文介绍"><a href="#论文介绍" class="headerlink" title="论文介绍"></a>论文介绍</h1><ul>
<li>标题：A Data-Driven Inertial Navigation/Bluetooth Fusion Algorithm for Indoor Localization</li>
<li>作者：Jianfan Chen, Baoding Zhou, Shaoqian Bao, et al.</li>
<li>期刊：IEEE Sensors Journal 2022</li>
<li>源码：未开源</li>
</ul>
<h1 id="1-Introductio"><a href="#1-Introductio" class="headerlink" title="1  Introductio"></a>1  Introductio</h1><p>室内 Location-based Services（LBS）的传统方法存在的问题：</p>
<ul>
<li>目前手机上都配备了小型、廉价而高效的 IMU，但是这些 IMU 存在着噪声和偏置，使得惯导受到影响。</li>
<li>传感器+相机结合的方法，但是相机会造成额外的计算成本和电池消耗。</li>
<li>基于步数的行人航迹推算（PDR，step-based pedestrian dead reckoning）使用惯性测量进行步长计算、步长分割、步长估计和步向估计。通过步长更新，虽然系统能解决惯性偏移问题，但行走习惯会影响计算结果。此外，步长检测中的周期性假设是不成立的，如果手机放在推车上，则无法检测到步数。</li>
</ul>
<p>数据驱动的定位方法：</p>
<ul>
<li>定义：使用短时间内的 IMU 数据和 GT 运动轨迹回归运动参数（如速度和方向），Vicon、Apple ARKit、Google Tango 等用于获取 GT 数据进行训练。</li>
<li>效果较好的方法：Robust IMU double integration（RIDI）[1]、IONet[2]、AI-IMU[3]、Backdrop Kalman Filter[4]。</li>
<li>缺点：<ul>
<li>当起点未知时，惯导只能获取全局坐标系的相对坐标而不是绝对坐标；</li>
<li>惯性追踪的不断进行，系统不可避免的会产生漂移。</li>
</ul>
</li>
<li>改进方法：使用 Wi-Fi、BLE 进行定位，BLE 相对更加精准。</li>
</ul>
<p>BLE-based 定位：</p>
<ul>
<li>ranging-based：三角化。</li>
<li>non-ranging-based：指纹算法。</li>
<li>缺点：障碍物会影响定位精度。</li>
</ul>
<p>融合 sensor-based 和 BLE-based 的解决方法：</p>
<ul>
<li>可用方法：EKF、UKF、Particle Filter。</li>
<li>对 BLE 的校准：使用 Channel State Information（CSI），但仍然面对着较大的误差。</li>
</ul>
<p>本文聚焦在探索数据驱动的惯导和 BLE-based 定位融合，使用如下贡献达到更好的室内定位效果：</p>
<ul>
<li>提出了一种利用粒子滤波融合数据驱动的惯导和 BLE-based 定位进行室内定位的方法。</li>
<li>有效地结合了利用重力稳定 IMU 数据的方法（坐标系对齐），使网络对行人运动特征的回归更具鲁棒性。</li>
<li>给出了基于 GT 和 IMU 传感器测量跨多个设备的视觉惯性里程计数据集。</li>
</ul>
<h1 id="2-Methodology"><a href="#2-Methodology" class="headerlink" title="2  Methodology"></a>2  Methodology</h1><center>
    <img src="/papernote/Data-Driven%20Navigation%20Bluetooth%20Indoor%20Localization/Figure-1.png">
</center>

<p>如 Fig.1 为本文方法的主要框架，由 3 个关键部分组成：</p>
<ul>
<li>数据驱动的惯性导航：使用 IMU 和训练好的 DNN 预测行人运动的距离和方向（得到运动向量）。</li>
<li>基于 BLE 的定位：将 RSS（Received Signal Strength）和阈值比较，选择稳定的方法进行 BLE-based 定位。</li>
<li>粒子滤波：融合运动向量和 BLE 定位结果，产生最终定位输出。</li>
</ul>
<h2 id="2-1-Data-Driven-Inertial-Navigation"><a href="#2-1-Data-Driven-Inertial-Navigation" class="headerlink" title="2.1  Data-Driven Inertial Navigation"></a>2.1  Data-Driven Inertial Navigation</h2><p>本文提出的数据驱动惯性导航方法包含三个关键步骤：</p>
<ol>
<li>获取序列学习所需的训练数据；</li>
<li>通过坐标对齐方法提取与手机携带方式无关的数据；</li>
<li>利用深度神经网络学习数据特征与运动特征之间的关系进行距离和航向回归。</li>
</ol>
<h3 id="2-1-1-Data-Acquisition"><a href="#2-1-1-Data-Acquisition" class="headerlink" title="2.1.1  Data Acquisition"></a>2.1.1  Data Acquisition</h3><ul>
<li>使用监督学习的方法：利用 GT 运动轨迹和 IMU 数据回归运动参数。</li>
</ul>
<p>假设用户在平面上运动，则运动参数的回归是在 2D 平面上进行的。文章考虑 4 种用户携带手机的方式：翻看（texting）、手持（swinging）、打电话（calling）、放包中（pocket）。用户一只手持 Tango 拍摄运动轨迹的 GT，另一只手携带 Samsung Galaxy S8 来获取 IMU 数据。</p>
<center>
    <img src="/papernote/Data-Driven%20Navigation%20Bluetooth%20Indoor%20Localization/Figure-2.png">
</center>

<ul>
<li>对于 GT 运动轨迹，从 Tango 上的 VIO 利用 RIDI 中的方法[5]得到 3D 相机位姿。Tango 可以提供低漂移的运动轨迹，这可以作为伪 GT。</li>
<li>对于 IMU 数据，实际的数据获取实验发现惯性数据会被附上时间戳。</li>
<li>TangoIMURecorder 产生位姿的频率为 200Hz，因此将 IMU 数据通过线性插值与 Tango 位姿进行同步。</li>
</ul>
<h3 id="2-1-2-Coordinate-Frame-Alignment"><a href="#2-1-2-Coordinate-Frame-Alignment" class="headerlink" title="2.1.2  Coordinate Frame Alignment"></a>2.1.2  Coordinate Frame Alignment</h3><p>加速度计和陀螺仪数据用于提取运动数据，但是由于携带手机的方式很多样（如 Fig.2），使得手机内部的 IMU 坐标系很不稳定。这种手机携带方法的任意性导致了仅依靠 IMU 难以获取准确的运动向量。因此，RIDI 提出了一种稳定的坐标框架，从 IMU 传感器数据中提取与手机方向无关的数据。</p>
<center>
    <img src="/papernote/Data-Driven%20Navigation%20Bluetooth%20Indoor%20Localization/Figure-3.png">
</center>

<p>通过 Android API 记录原始加速度计、陀螺仪和重力测量结果。将手机的 y 轴与负重力方向对齐，加速度计和陀螺仪的数据可以在固定的坐标系中表示，如 Fig.3。</p>
<p>为了抑制高频噪声，将对齐的加速度计和陀螺仪数据应用 $\sigma = 2.0$ 的高斯平滑。此外，由于 Tango 从 Android API 中提供全局坐标系下的位姿，因此要将速度矢量转换到稳定的坐标系下：</p>
<ul>
<li>假设用户和智能手机对齐到同一个坐标系。当用户在平坦的地面上行走，即垂直位移为零时，可以将位姿数据转换为稳定的航向和位移。</li>
</ul>
<p>这些对准的加速度计和陀螺仪数据与运动特征相结合，构成 DNN 的训练数据。</p>
<h3 id="2-1-3-Distance-and-Heading-Regression"><a href="#2-1-3-Distance-and-Heading-Regression" class="headerlink" title="2.1.3  Distance and Heading Regression"></a>2.1.3  Distance and Heading Regression</h3><p>DNN 的结构如下：</p>
<center>
    <img src="/papernote/Data-Driven%20Navigation%20Bluetooth%20Indoor%20Localization/Figure-4.png">
</center>

<p>网络的输入为数秒内的 IMU 数据特征（对齐后的加速度计和陀螺仪数据），输出为运动特征（距离和方向）。</p>
<p>使用加速度计、陀螺仪、重力和 3D 相机在坐标系对齐模块。在大小为 $n$ 的时间窗口上执行回归任务，使网络学习定位距离 $\Delta l$ 和方向 $\Delta \psi$：</p>
<script type="math/tex; mode=display">
(\Delta l, \Delta \psi) = RNN (\{ (a_i, \omega_i) \}_{i=1}^n)</script><p>使用两层双向 LSTM，每层 LSTM 有 256 个隐藏状态。为避免过拟合，在每个 LSTM 后面接概率为 0.5 的 dropout 层，随机将输入元素变为 0。最后连接一个全连接层，执行对位置距离变化和方向变化的回归。预测和 GT 的均方误差（mean square error）用于作为损失函数。选择 Adam 优化器和 TensorFlow 进行网络搭建。</p>
<center>
    <img src="/papernote/Data-Driven%20Navigation%20Bluetooth%20Indoor%20Localization/Figure-5.png">
</center>

<p>如 Fig.5，实验室验证了窗口大小为 200、400、600 帧的实验结果，发现窗口大小为 400 时可以快速下降损失（因此选择 $n = 400$，即向网络输入大小为 2400 的来自加速度计和陀螺仪的特征数据）。在通过网络获得 $\Delta l$ 和 $\Delta \psi$ 后，运动的下一个位置则可以描述为</p>
<script type="math/tex; mode=display">
\begin{cases}
x = x_0 + \Delta l \times \cos (\psi_0 + \Delta \psi) \\
y = y_0 + \Delta l \times \sin (\psi_0 + \Delta \psi)
\end{cases}</script><p>其中 $(x_0, y_0)$ 表示前一次的位置，$\psi_0$ 表示前一次的方向。</p>
<h2 id="2-2-BLE-Based-Localization"><a href="#2-2-BLE-Based-Localization" class="headerlink" title="2.2  BLE-Based Localization"></a>2.2  BLE-Based Localization</h2><p>通过三边测量（trilateration）使用 RSS 进行定位。且离 BLE beacon 越近，RSS 值越大：</p>
<script type="math/tex; mode=display">
RSS(\lambda) = RSS(\lambda_0) - 10 \eta \log (\frac{\lambda}{\lambda_0}) + X_{\sigma}</script><p>其中 $\lambda$ 为 beacon 到设备的距离，$RSS(\lambda)$ 为 beacon 的 RSS，$RSS(\lambda_0)$ 为在参考距离 $\lambda_0$ 处的 RSS，$\eta$ 为路径损耗指数，$X_{\sigma}$ 为均值为 0，方差为 $\sigma^2$ 的高斯噪声。</p>
<center>
    <img src="/papernote/Data-Driven%20Navigation%20Bluetooth%20Indoor%20Localization/Figure-6.png">
</center>

<p>通过将数个（三个或以上）的 RSS 转换为设备到 beacon 的距离，三边测量使用圆形几何计算设备的位置，如 Fig.6。具体地，假设已知 BLE beacons 的位置，记设备位置 $(x,y)$ 和 beacon 位置 $(x_i, y_i)$，则可以得到 $N$ 个等式：</p>
<script type="math/tex; mode=display">
(x - x_i)^2 + (y - y_i)^2 = \lambda_i^2,\ i = 1, 2, \dots, N</script><p>求解上述 $N$ 个式子可以得到设备的位置：</p>
<ul>
<li>理想条件下（Fig.6a)，三个圆交于一个公共点，则可以求得唯一解。</li>
<li>现实条件下（Fig.6b 和 Fig.6c），三个圆无公共交点或均不相交。使用 Line Intersection-based Trilateration[6] 来寻找近似解。</li>
</ul>
<p>对于 Fig.6c 中的情况，三边测量结果并不准确，因此使用 RSS 与 radio map 进行对比，显示地将 RSS 转换为距离，即 location fingerprinting。</p>
<p>BLE fingerprinting-based positioning：</p>
<ul>
<li>offline training phases：选择一系列确定点为参考点 RP，在每个 RP 处手机所有检测到的 BLE beacon 的 RSS。这些在 RP 处收集的 RSS 则称为指纹（fingerprint），这些指纹形成了 radio map。</li>
<li>online positioning phases：将采样到的 RSS 与 radio map 中的指纹进行匹配，得到当前位置的估计。</li>
</ul>
<p>设置 RSS 阈值（-50dBm）来选择 BLE-based 定位的方法：</p>
<ul>
<li>如果所有观测到的 BLE beacon 的 RSS 小于阈值，则使用基于指纹的定位方法，否则使用三角化的定位方法。</li>
</ul>
<p>但是仅依靠 BLE 进行定位，精度可能仍然不够，因此本文采用了数据驱动和基于 BLE 的融合方法。</p>
<h2 id="2-3-Particle-Filter"><a href="#2-3-Particle-Filter" class="headerlink" title="2.3  Particle Filter"></a>2.3  Particle Filter</h2><p>使用粒子滤波融合数据驱动的惯导方法和 BLE-based 的定位方法：BLE-based 的定位提供设备位置的观测值，数据驱动的惯导用于建模用户的移动。</p>
<h3 id="2-3-1-Initialization"><a href="#2-3-1-Initialization" class="headerlink" title="2.3.1  Initialization"></a>2.3.1  Initialization</h3><p>假设粒子集合为 $H = \{ X^i | i = 1, 2, …, N \}$，每个粒子有 3 维联合概率分布，即 $X^i = (x^i, y^i, \theta^i)$。在时间 $t=0$，利用高斯分布，随机从 BLE-based 定位得到的初始点 $(x_0^i, y_0^i)$ 周围选取粒子的位置。每个粒子的初始权重为 $\omega_0^i = \frac{1}{N}$。</p>
<h3 id="2-3-2-Particle-Movement"><a href="#2-3-2-Particle-Movement" class="headerlink" title="2.3.2  Particle Movement"></a>2.3.2  Particle Movement</h3><p>利用数据驱动的惯导来建模用户的移动：</p>
<script type="math/tex; mode=display">
\begin{cases}
x_t^i = x_{t-1}^i + l_t \times \cos \psi_t \\
y_t^i = y_{t-1}^i + l_t \times \sin \psi_t
\end{cases}</script><p>$l_t$ 和 $\psi_t$ 为数据驱动的惯导的结果。</p>
<h3 id="2-3-3-Particle-Weight-Update-and-Resampling"><a href="#2-3-3-Particle-Weight-Update-and-Resampling" class="headerlink" title="2.3.3  Particle Weight Update and Resampling"></a>2.3.3  Particle Weight Update and Resampling</h3><p>在时间 $t$，当从 BLE-based 定位获得一个新的观测数据（位置），基于粒子的位置相距 BLE 定位的结果的距离，所有粒子的权重发生更新。</p>
<p>同时，采用重采样，保留权重大的粒子，丢弃权重小的粒子。</p>
<h3 id="2-3-4-Target-Location-Estimation"><a href="#2-3-4-Target-Location-Estimation" class="headerlink" title="2.3.4  Target Location Estimation"></a>2.3.4  Target Location Estimation</h3><p>最终的定位结果为加权和：</p>
<script type="math/tex; mode=display">
\begin{cases}
x_t = \sum_{i=0}^N x_t^i \omega_t^i \\\
y_t = \sum_{i=0}^N y_t^i \omega_t^i
\end{cases}</script><h1 id="3-Experiment-and-Results"><a href="#3-Experiment-and-Results" class="headerlink" title="3  Experiment and Results"></a>3  Experiment and Results</h1><h2 id="3-1-实验设置"><a href="#3-1-实验设置" class="headerlink" title="3.1  实验设置"></a>3.1  实验设置</h2><p>实验环境：所有的 beacon 离地面 2.5m 高且为相同规格，beacon 间的水平距离约为 3m。选择了 Fig.7 中的 3 条行走路径。</p>
<center>
    <img src="/papernote/Data-Driven%20Navigation%20Bluetooth%20Indoor%20Localization/Figure-7.png">
</center>

<p>数据：使用 Galaxy S8 和 Tango 手机了 116 个数据序列，考虑 Tango 的精度，每个序列长度小于 200m。每 100 个 IMU 采样产生一个训练采样。</p>
<center>
    <img src="/papernote/Data-Driven%20Navigation%20Bluetooth%20Indoor%20Localization/Table-1.png">
</center>



<h2 id="3-2-实验结果"><a href="#3-2-实验结果" class="headerlink" title="3.2  实验结果"></a>3.2  实验结果</h2><p>数据驱动方法的短距离实验误差：</p>
<center>
    <img src="/papernote/Data-Driven%20Navigation%20Bluetooth%20Indoor%20Localization/Table-2.png">
</center>

<p>轨迹重建结果：</p>
<center>
    <img src="/papernote/Data-Driven%20Navigation%20Bluetooth%20Indoor%20Localization/Figure-8.png">
</center>

<p>粒子数量对算法融合的影响：</p>
<center>
    <img src="/papernote/Data-Driven%20Navigation%20Bluetooth%20Indoor%20Localization/Figure-10.png">
</center>

<p>融合算法的定位结果：</p>
<center>
    <img src="/papernote/Data-Driven%20Navigation%20Bluetooth%20Indoor%20Localization/Table-3.png">
</center>

<p>融合算法的轨迹重建结果：</p>
<center>
    <img src="/papernote/Data-Driven%20Navigation%20Bluetooth%20Indoor%20Localization/Figure-11.png">
</center>

<p>与现有方法的对比：</p>
<center>
    <img src="/papernote/Data-Driven%20Navigation%20Bluetooth%20Indoor%20Localization/Figure-12.png">
</center>



<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li>[1] H. Yan, Q. Shan, and Y. Furukawa, “RIDI: Robust IMU double integra- tion,” in <em>Proc. Eur. Conf. Comput. Vis. (ECCV)</em>, Sep. 2018, pp. 621–636.</li>
<li>[2] C. Chen, X. Lu, A. Markham, and N. Trigoni, “IONet: Learning to cure the curse of drift in inertial odometry,” in <em>Proc. AAAI Conf. Artif. Intell.</em>, Apr. 2018, vol. 32, no. 1, pp. 6468-6476.</li>
<li>[3] M. Brossard, A. Barrau, and S. Bonnabel, “AI-IMU dead-reckoning,” <em>IEEE Trans. Intell. Vehicles</em>, vol. 5, no. 4, pp. 585–595, Dec. 2020.</li>
<li>[4] T. Haarnoja, A. Ajay, S. Levine, and P. Abbeel, “Backprop KF: Learning discriminative deterministic state estimators,” in <em>Proc. 30th Conf. Neural Inf. Process. Syst.</em>, 2016, pp. 1–11. [Online]. Available: <a href="http://arxiv.org/abs/1605.07148">http://arxiv.org/abs/1605.07148</a></li>
<li>[5] <a href="https://github.com/higerra/TangoIMURecorder">https://github.com/higerra/TangoIMURecorder</a></li>
<li>[6] S. Pradhan, Y. Bae, J.-Y. Pyun, N. Y. Ko, and S.-S. Hwang, “Hybrid TOA trilateration algorithm based on line intersection and comparison approach of intersection distances,” <em>Energies</em>, vol. 12, no. 9, p. 1668, May 2019.</li>
</ul>
]]></content>
      <categories>
        <category>Paper Note</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Paper Note</tag>
        <tag>Indoor Localization</tag>
        <tag>Indoor Navigation</tag>
        <tag>Bluetooth</tag>
      </tags>
  </entry>
  <entry>
    <title>Deformable DETR</title>
    <url>/papernote/Deformable%20DETR/</url>
    <content><![CDATA[<p>关于论文 Deformable DETR: Deformable Transformers for End-to-End Object Detection (arXiv:2010.04159v4 [cs.CV]) 的阅读总结。</p>
<span id="more"></span>
<p><img src="/papernote/Deformable%20DETR/幻灯片1.png" alt><br><img src="/papernote/Deformable%20DETR/幻灯片2.png" alt><br><img src="/papernote/Deformable%20DETR/幻灯片3.png" alt><br><img src="/papernote/Deformable%20DETR/幻灯片4.png" alt><br><img src="/papernote/Deformable%20DETR/幻灯片5.png" alt><br><img src="/papernote/Deformable%20DETR/幻灯片6.png" alt><br><img src="/papernote/Deformable%20DETR/幻灯片7.png" alt><br><img src="/papernote/Deformable%20DETR/幻灯片8.png" alt><br><img src="/papernote/Deformable%20DETR/幻灯片9.png" alt><br><img src="/papernote/Deformable%20DETR/幻灯片10.png" alt><br><img src="/papernote/Deformable%20DETR/幻灯片11.png" alt><br><img src="/papernote/Deformable%20DETR/幻灯片12.png" alt><br><img src="/papernote/Deformable%20DETR/幻灯片13.png" alt><br><img src="/papernote/Deformable%20DETR/幻灯片14.png" alt><br><img src="/papernote/Deformable%20DETR/幻灯片15.png" alt><br><img src="/papernote/Deformable%20DETR/幻灯片16.png" alt><br><img src="/papernote/Deformable%20DETR/幻灯片17.png" alt><br><img src="/papernote/Deformable%20DETR/幻灯片18.png" alt><br><img src="/papernote/Deformable%20DETR/幻灯片19.png" alt><br><img src="/papernote/Deformable%20DETR/幻灯片20.png" alt><br><img src="/papernote/Deformable%20DETR/幻灯片21.png" alt></p>
]]></content>
      <categories>
        <category>Paper Note</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Paper Note</tag>
        <tag>Object Detection</tag>
        <tag>Transformers</tag>
      </tags>
  </entry>
  <entry>
    <title>DiSCo-SLAM</title>
    <url>/papernote/DiSCo-SLAM/</url>
    <content><![CDATA[<p>关于论文 DiSCo-SLAM: Distributed Scan Context-Enabled Multi-Robot LiDAR SLAM with Two-Stage Global-Local Graph Optimization (IEEE ROBOTICS AND AUTOMATION LETTERS, 2021) 的阅读总结。</p>
<span id="more"></span>
<p><img src="/papernote/DiSCo-SLAM/幻灯片1.png" alt><br><img src="/papernote/DiSCo-SLAM/幻灯片2.png" alt><br><img src="/papernote/DiSCo-SLAM/幻灯片3.png" alt><br><img src="/papernote/DiSCo-SLAM/幻灯片4.png" alt><br><img src="/papernote/DiSCo-SLAM/幻灯片5.png" alt><br><img src="/papernote/DiSCo-SLAM/幻灯片6.png" alt><br><img src="/papernote/DiSCo-SLAM/幻灯片7.png" alt><br><img src="/papernote/DiSCo-SLAM/幻灯片8.png" alt><br><img src="/papernote/DiSCo-SLAM/幻灯片9.png" alt><br><img src="/papernote/DiSCo-SLAM/幻灯片10.png" alt><br><img src="/papernote/DiSCo-SLAM/幻灯片11.png" alt><br><img src="/papernote/DiSCo-SLAM/幻灯片12.png" alt><br><img src="/papernote/DiSCo-SLAM/幻灯片13.png" alt><br><img src="/papernote/DiSCo-SLAM/幻灯片14.png" alt><br><img src="/papernote/DiSCo-SLAM/幻灯片15.png" alt><br><img src="/papernote/DiSCo-SLAM/幻灯片16.png" alt><br><img src="/papernote/DiSCo-SLAM/幻灯片17.png" alt><br><img src="/papernote/DiSCo-SLAM/幻灯片18.png" alt><br><img src="/papernote/DiSCo-SLAM/幻灯片19.png" alt><br><img src="/papernote/DiSCo-SLAM/幻灯片20.png" alt><br><img src="/papernote/DiSCo-SLAM/幻灯片21.png" alt><br><img src="/papernote/DiSCo-SLAM/幻灯片22.png" alt><br><img src="/papernote/DiSCo-SLAM/幻灯片23.png" alt><br><img src="/papernote/DiSCo-SLAM/幻灯片24.png" alt></p>
]]></content>
      <categories>
        <category>Paper Note</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Paper Note</tag>
        <tag>LiDAR SLAM</tag>
        <tag>Distributed SLAM</tag>
        <tag>Scan Context</tag>
      </tags>
  </entry>
  <entry>
    <title>向量（矩阵）求导</title>
    <url>/summary/Derivative%20of%20Vector%20or%20Matrix/</url>
    <content><![CDATA[<p>向量（矩阵）对求导运算总结。</p>
<span id="more"></span>
<center><b><font size="9">向量（矩阵）求导</font></b></center>

<h1 id="1-向量（矩阵）对元素求导"><a href="#1-向量（矩阵）对元素求导" class="headerlink" title="1  向量（矩阵）对元素求导"></a>1  向量（矩阵）对元素求导</h1><h2 id="1-1-行向量对元素求导"><a href="#1-1-行向量对元素求导" class="headerlink" title="1.1  行向量对元素求导"></a>1.1  行向量对元素求导</h2><p>设 $\pmb{y}^T = [y_1, \cdots, y_n]$ 为 $n$ 维行向量，$x$ 为元素，则</p>
<script type="math/tex; mode=display">
\frac{\partial \pmb{y}^T}{\partial x} = [\frac{\partial y_1}{\partial x}, \cdots, \frac{\partial y_n}{\partial x}]</script><h2 id="1-2-列向量对元素求导"><a href="#1-2-列向量对元素求导" class="headerlink" title="1.2  列向量对元素求导"></a>1.2  列向量对元素求导</h2><p>设 $\pmb{y} = [y_1, \cdots, y_m]^T$ 为 $m$ 维列向量，$x$ 为元素，则</p>
<script type="math/tex; mode=display">
\frac{\partial \pmb{y}}{\partial x} = 
\left[
\begin{matrix}
\frac{\partial y_1}{\partial x} \\
\vdots \\
\frac{\partial y_m}{\partial x}
\end{matrix}
\right]</script><h2 id="1-3-矩阵对元素求导"><a href="#1-3-矩阵对元素求导" class="headerlink" title="1.3  矩阵对元素求导"></a>1.3  矩阵对元素求导</h2><p>设 $Y = [y_{ij}]$ 为 $m\times n$ 矩阵，$x$ 为元素，则</p>
<script type="math/tex; mode=display">
\frac{\partial Y}{\partial x} = 
\left[
\begin{matrix}
\frac{\partial y_{11}}{\partial x} &\cdots &\frac{\partial y_{1n}}{\partial x} \\
\vdots &\vdots &\vdots \\
\frac{\partial y_{m1}}{\partial x} &\cdots &\frac{\partial y_{mn}}{\partial x}
\end{matrix}
\right]</script><h1 id="2-元素对向量（矩阵）求导"><a href="#2-元素对向量（矩阵）求导" class="headerlink" title="2  元素对向量（矩阵）求导"></a>2  元素对向量（矩阵）求导</h1><h2 id="2-1-元素对行向量求导"><a href="#2-1-元素对行向量求导" class="headerlink" title="2.1  元素对行向量求导"></a>2.1  元素对行向量求导</h2><p>设 $y$ 为元素，$\pmb{x}^T = [x_1, \cdots, x_q]$ 为 $q$ 维行向量，则</p>
<script type="math/tex; mode=display">
\frac{\partial y}{\partial \pmb{x}^T} = [\frac{\partial y}{\partial x_1}, \cdots, \frac{\partial y}{\partial x_q}]</script><h2 id="2-2-元素对列向量求导"><a href="#2-2-元素对列向量求导" class="headerlink" title="2.2  元素对列向量求导"></a>2.2  元素对列向量求导</h2><p>设 $y$ 为元素，$\pmb{x} = [x_1, \cdots, x_p]^T$ 为 $p$ 维行向量，则</p>
<script type="math/tex; mode=display">
\frac{\partial y}{\partial \pmb{x}} = 
\left[
\begin{matrix}
\frac{\partial y}{\partial x_1} \\
\vdots \\
\frac{\partial y}{\partial x_p}
\end{matrix}
\right]</script><h2 id="2-3-元素对矩阵求导"><a href="#2-3-元素对矩阵求导" class="headerlink" title="2.3  元素对矩阵求导"></a>2.3  元素对矩阵求导</h2><p>设 $y$ 为元素，$X = [x_{ij}]$ 为 $p\times q$ 矩阵，则</p>
<script type="math/tex; mode=display">
\frac{\partial y}{\partial X} = 
\left[
\begin{matrix}
\frac{\partial y}{\partial x_{11}} &\cdots &\frac{\partial y}{\partial x_{1q}} \\
\vdots &\vdots &\vdots \\
\frac{\partial y}{\partial x_{p1}} &\cdots &\frac{\partial y}{\partial x_{pq}}
\end{matrix}
\right]</script><h1 id="3-行（列）向量对列（行）向量求导"><a href="#3-行（列）向量对列（行）向量求导" class="headerlink" title="3  行（列）向量对列（行）向量求导"></a>3  行（列）向量对列（行）向量求导</h1><h2 id="3-1-行向量对列向量求导"><a href="#3-1-行向量对列向量求导" class="headerlink" title="3.1  行向量对列向量求导"></a>3.1  行向量对列向量求导</h2><p>设 $\pmb{y}^T = [y_1, \cdots, y_n]$ 为 $n$ 维行向量，$\pmb{x} = [x_1, \cdots, x_p]^T$ 为 $p$ 维行向量，则</p>
<script type="math/tex; mode=display">
\frac{\partial \pmb{y}^T}{\partial \pmb{x}} = 
\left[
\begin{matrix}
\frac{\partial y_1}{\partial x_1} &\cdots &\frac{\partial y_n}{\partial x_1} \\
\vdots &\vdots &\vdots \\
\frac{\partial y_1}{\partial x_p} &\cdots &\frac{\partial y_n}{\partial x_p}
\end{matrix}
\right]</script><h2 id="3-2-列向量对行向量求导"><a href="#3-2-列向量对行向量求导" class="headerlink" title="3.2  列向量对行向量求导"></a>3.2  列向量对行向量求导</h2><p>设 $\pmb{y} = [y_1, \cdots, y_m]^T$ 为 $m$ 维列向量，$\pmb{x}^T = [x_1, \cdots, x_q]$ 为 $q$ 维行向量，则</p>
<script type="math/tex; mode=display">
\frac{\partial \pmb{y}}{\partial \pmb{x}^T} = 
\left[
\begin{matrix}
\frac{\partial y_1}{\partial x_1} &\cdots &\frac{\partial y_1}{\partial x_q} \\
\vdots &\vdots &\vdots \\
\frac{\partial y_m}{\partial x_1} &\cdots &\frac{\partial y_m}{\partial x_q}
\end{matrix}
\right]</script><h1 id="4-行（列）向量对行（列）向量求导"><a href="#4-行（列）向量对行（列）向量求导" class="headerlink" title="4  行（列）向量对行（列）向量求导"></a>4  行（列）向量对行（列）向量求导</h1><h2 id="4-1-行向量对行向量求导"><a href="#4-1-行向量对行向量求导" class="headerlink" title="4.1  行向量对行向量求导"></a>4.1  行向量对行向量求导</h2><p>设 $\pmb{y}^T = [y_1, \cdots, y_n]$ 为 $n$ 维行向量，$\pmb{x}^T = [x_1, \cdots, x_q]$ 为 $q$ 维行向量，则</p>
<script type="math/tex; mode=display">
\frac{\partial \pmb{y}^T}{\partial \pmb{x}^T} = [\frac{\partial \pmb{y}^T}{\partial x_1}, \cdots, \frac{\partial \pmb{y}^T}{\partial x_q}]</script><h2 id="4-2-列向量对列向量求导"><a href="#4-2-列向量对列向量求导" class="headerlink" title="4.2  列向量对列向量求导"></a>4.2  列向量对列向量求导</h2><p>设 $\pmb{y} = [y_1, \cdots, y_m]^T$ 为 $m$ 维列向量，$\pmb{x} = [x_1, \cdots, x_p]^T$ 为 $p$ 维行向量，则</p>
<script type="math/tex; mode=display">
\frac{\partial \pmb{y}}{\partial \pmb{x}} = 
\left[
\begin{matrix}
\frac{\partial y_1}{\partial \pmb{x}} \\
\vdots \\
\frac{\partial y_m}{\partial \pmb{x}}
\end{matrix}
\right]</script><h1 id="5-矩阵对行（列）向量求导"><a href="#5-矩阵对行（列）向量求导" class="headerlink" title="5  矩阵对行（列）向量求导"></a>5  矩阵对行（列）向量求导</h1><h2 id="5-1-矩阵对行向量求导"><a href="#5-1-矩阵对行向量求导" class="headerlink" title="5.1  矩阵对行向量求导"></a>5.1  矩阵对行向量求导</h2><p>设 $Y = [y_{ij}]$ 为 $m\times n$ 矩阵，$\pmb{x}^T = [x_1, \cdots, x_q]$ 为 $q$ 维行向量，则</p>
<script type="math/tex; mode=display">
\frac{\partial Y}{\partial \pmb{x}^T} = [\frac{\partial Y}{\partial x_1}, \cdots, \frac{\partial Y}{\partial x_q}] \in \mathbb{R}^{m \times nq}</script><p>记 $Y = [\pmb{y}_1^T, \cdots, \pmb{y}_m^T]^T$，则可以视为多个行向量对单个行向量求导</p>
<script type="math/tex; mode=display">
\frac{\partial Y}{\partial \pmb{x}^T} =
\left[
\begin{matrix}
\frac{\partial \pmb{y}_1^T}{\partial \pmb{x}^T} \\
\vdots \\
\frac{\partial \pmb{y}_m^T}{\partial \pmb{x}^T}
\end{matrix}
\right]
=
\left[
\begin{matrix}
\frac{\partial \pmb{y}_1^T}{\partial x_1} &\cdots &\frac{\partial \pmb{y}_1^T}{\partial x_q} \\
\vdots &\vdots &\vdots \\
\frac{\partial \pmb{y}_m^T}{\partial x_1} &\cdots &\frac{\partial \pmb{y}_m^T}{\partial x_q}
\end{matrix}
\right]
\in \mathbb{R}^{m \times nq}</script><h2 id="5-2-矩阵对列向量求导"><a href="#5-2-矩阵对列向量求导" class="headerlink" title="5.2  矩阵对列向量求导"></a>5.2  矩阵对列向量求导</h2><p>设 $Y = [y_{ij}]$ 为 $m\times n$ 矩阵，$\pmb{x} = [x_1, \cdots, x_p]^T$ 为 $p$ 维列向量，则</p>
<script type="math/tex; mode=display">
\frac{\partial Y}{\partial \pmb{x}} = 
\left[
\begin{matrix}
\frac{\partial y_{11}}{\partial \pmb{x}} &\cdots &\frac{\partial y_{1n}}{\partial \pmb{x}} \\
\vdots &\vdots &\vdots \\
\frac{\partial y_{m1}}{\partial \pmb{x}} &\cdots &\frac{\partial y_{mn}}{\partial \pmb{x}}
\end{matrix}
\right]
\in \mathbb{R}^{mp \times n}</script><p>记 $Y = [\pmb{y}_1, \cdots, \pmb{y}_n]$，则可以视为多个列向量对单个列向量求导</p>
<script type="math/tex; mode=display">
\frac{\partial Y}{\partial \pmb{x}^T} = [\frac{\partial \pmb{y_1}}{\partial \pmb{x}}, \cdots, \frac{\partial \pmb{y_n}}{\partial \pmb{x}}] \in \mathbb{R}^{mp \times n}</script><h1 id="6-向量（矩阵）对矩阵求导"><a href="#6-向量（矩阵）对矩阵求导" class="headerlink" title="6  向量（矩阵）对矩阵求导"></a>6  向量（矩阵）对矩阵求导</h1><h2 id="6-1-行向量对矩阵求导"><a href="#6-1-行向量对矩阵求导" class="headerlink" title="6.1  行向量对矩阵求导"></a>6.1  行向量对矩阵求导</h2><p>设 $\pmb{y}^T = [y_1, \cdots, y_n]$ 为 $n$ 维行向量，$X = [x_{ij}]$ 为 $p\times q$ 矩阵，则</p>
<script type="math/tex; mode=display">
\frac{\partial \pmb{y}^T}{\partial X} = 
\left[
\begin{matrix}
\frac{\partial \pmb{y}^T}{\partial x_{11}} &\cdots &\frac{\partial \pmb{y}^T}{\partial x_{1q}} \\
\vdots &\vdots &\vdots \\
\frac{\partial \pmb{y}^T}{\partial x_{p1}} &\cdots &\frac{\partial \pmb{y}^T}{\partial x_{pq}}
\end{matrix}
\right]
\in \mathbb{R}^{p \times nq}</script><p>记 $X = [\pmb{x}_1^T, \cdots, \pmb{x}_p^T]^T$，则可以视为单个行向量对多个行向量求导</p>
<script type="math/tex; mode=display">
\frac{\partial \pmb{y}^T}{\partial X} = 
\left[
\begin{matrix}
\frac{\partial \pmb{y}^T}{\partial \pmb{x}_1^T} \\
\vdots \\
\frac{\partial \pmb{y}^T}{\partial \pmb{x}_p^T}
\end{matrix}
\right]
\in \mathbb{R}^{p \times nq}</script><h2 id="6-2-列向量对矩阵求导"><a href="#6-2-列向量对矩阵求导" class="headerlink" title="6.2  列向量对矩阵求导"></a>6.2  列向量对矩阵求导</h2><p>设 $\pmb{y} = [y_1, \cdots, y_m]^T$ 为 $m$ 维列向量，$X = [x_{ij}]$ 为 $p\times q$ 矩阵，则</p>
<script type="math/tex; mode=display">
\frac{\partial \pmb{y}}{\partial X} = 
\left[
\begin{matrix}
\frac{\partial y_1}{\partial X} \\
\vdots \\
\frac{\partial y_m}{\partial X}
\end{matrix}
\right]
\in \mathbb{R}^{mp \times q}</script><p>记 $X = [\pmb{x}_1, \cdots, \pmb{x}_q]$，则可以视为单个列向量对多个列向量求导</p>
<script type="math/tex; mode=display">
\frac{\partial \pmb{y}}{\partial X} = [\frac{\partial \pmb{y}}{\partial \pmb{x}_1}, \cdots, \frac{\partial \pmb{y}}{\partial \pmb{x}_q}]
=
\left[
\begin{matrix}
\frac{\partial y_1}{\partial \pmb{x}_1} &\cdots &\frac{\partial y_1}{\partial \pmb{x}_q} \\
\vdots &\vdots &\vdots \\
\frac{\partial y_m}{\partial \pmb{x}_1} &\cdots &\frac{\partial y_m}{\partial \pmb{x}_q}
\end{matrix}
\right]
\in \mathbb{R}^{mp \times q}</script><h2 id="6-3-矩阵对矩阵求导"><a href="#6-3-矩阵对矩阵求导" class="headerlink" title="6.3  矩阵对矩阵求导"></a>6.3  矩阵对矩阵求导</h2><p>设 $Y = [y_{ij}]$ 为 $m\times n$ 矩阵，$X = [x_{ij}]$ 为 $p\times q$ 矩阵，则</p>
<script type="math/tex; mode=display">
\frac{\partial Y}{\partial X} =
\left[
\begin{matrix}
\frac{\partial \pmb{y}_1^T}{\partial \pmb{x}_1} &\cdots &\frac{\partial \pmb{y}_1^T}{\partial \pmb{x}_q} \\
\vdots &\vdots &\vdots \\
\frac{\partial \pmb{y}_m^T}{\partial \pmb{x}_1} &\cdots &\frac{\partial \pmb{y}_m^T}{\partial \pmb{x}_q}
\end{matrix}
\right]
\in \mathbb{R}^{mp \times nq}</script><p>先将 $X$ 看成列向量，再将 $Y$ 看成行向量。</p>
<h1 id="7-几个推广公式"><a href="#7-几个推广公式" class="headerlink" title="7  几个推广公式"></a>7  几个推广公式</h1><script type="math/tex; mode=display">
\frac{\partial Ax}{\partial x^T} = A \\
\frac{\partial (Ax)^T}{\partial x} = A^T</script><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://blog.csdn.net/zhangdamengcsdn/article/details/80200059">https://blog.csdn.net/zhangdamengcsdn/article/details/80200059</a></li>
<li><a href="https://www.cnblogs.com/yanghh/p/13758243.html">https://www.cnblogs.com/yanghh/p/13758243.html</a></li>
</ul>
]]></content>
      <categories>
        <category>Summary</category>
      </categories>
      <tags>
        <tag>Summary</tag>
        <tag>Mathematics</tag>
      </tags>
  </entry>
  <entry>
    <title>Distributed Ranging SLAM for Multiple Robots with UWB</title>
    <url>/papernote/Distributed%20Ranging%20SLAM%20for%20Multiple%20Robots%20with%20UWB/</url>
    <content><![CDATA[<p>关于论文 Distributed Ranging SLAM for Multiple Robots with Ultra-WideBand and Odometry Measurements (IROS, 2022) 的阅读总结。</p>
<span id="more"></span>
<p><img src="/papernote/Distributed%20Ranging%20SLAM%20for%20Multiple%20Robots%20with%20UWB/幻灯片1.png" alt><br><img src="/papernote/Distributed%20Ranging%20SLAM%20for%20Multiple%20Robots%20with%20UWB/幻灯片2.png" alt><br><img src="/papernote/Distributed%20Ranging%20SLAM%20for%20Multiple%20Robots%20with%20UWB/幻灯片3.png" alt><br><img src="/papernote/Distributed%20Ranging%20SLAM%20for%20Multiple%20Robots%20with%20UWB/幻灯片4.png" alt><br><img src="/papernote/Distributed%20Ranging%20SLAM%20for%20Multiple%20Robots%20with%20UWB/幻灯片5.png" alt><br><img src="/papernote/Distributed%20Ranging%20SLAM%20for%20Multiple%20Robots%20with%20UWB/幻灯片6.png" alt><br><img src="/papernote/Distributed%20Ranging%20SLAM%20for%20Multiple%20Robots%20with%20UWB/幻灯片7.png" alt><br><img src="/papernote/Distributed%20Ranging%20SLAM%20for%20Multiple%20Robots%20with%20UWB/幻灯片8.png" alt><br><img src="/papernote/Distributed%20Ranging%20SLAM%20for%20Multiple%20Robots%20with%20UWB/幻灯片9.png" alt><br><img src="/papernote/Distributed%20Ranging%20SLAM%20for%20Multiple%20Robots%20with%20UWB/幻灯片10.png" alt><br><img src="/papernote/Distributed%20Ranging%20SLAM%20for%20Multiple%20Robots%20with%20UWB/幻灯片11.png" alt><br><img src="/papernote/Distributed%20Ranging%20SLAM%20for%20Multiple%20Robots%20with%20UWB/幻灯片12.png" alt><br><img src="/papernote/Distributed%20Ranging%20SLAM%20for%20Multiple%20Robots%20with%20UWB/幻灯片13.png" alt><br><img src="/papernote/Distributed%20Ranging%20SLAM%20for%20Multiple%20Robots%20with%20UWB/幻灯片14.png" alt><br><img src="/papernote/Distributed%20Ranging%20SLAM%20for%20Multiple%20Robots%20with%20UWB/幻灯片15.png" alt><br><img src="/papernote/Distributed%20Ranging%20SLAM%20for%20Multiple%20Robots%20with%20UWB/幻灯片16.png" alt><br><img src="/papernote/Distributed%20Ranging%20SLAM%20for%20Multiple%20Robots%20with%20UWB/幻灯片17.png" alt><br><img src="/papernote/Distributed%20Ranging%20SLAM%20for%20Multiple%20Robots%20with%20UWB/幻灯片18.png" alt><br><img src="/papernote/Distributed%20Ranging%20SLAM%20for%20Multiple%20Robots%20with%20UWB/幻灯片19.png" alt><br><img src="/papernote/Distributed%20Ranging%20SLAM%20for%20Multiple%20Robots%20with%20UWB/幻灯片20.png" alt><br><img src="/papernote/Distributed%20Ranging%20SLAM%20for%20Multiple%20Robots%20with%20UWB/幻灯片21.png" alt><br><img src="/papernote/Distributed%20Ranging%20SLAM%20for%20Multiple%20Robots%20with%20UWB/幻灯片22.png" alt><br><img src="/papernote/Distributed%20Ranging%20SLAM%20for%20Multiple%20Robots%20with%20UWB/幻灯片23.png" alt><br><img src="/papernote/Distributed%20Ranging%20SLAM%20for%20Multiple%20Robots%20with%20UWB/幻灯片24.png" alt><br><img src="/papernote/Distributed%20Ranging%20SLAM%20for%20Multiple%20Robots%20with%20UWB/幻灯片25.png" alt></p>
]]></content>
      <categories>
        <category>Paper Note</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Paper Note</tag>
        <tag>LiDAR SLAM</tag>
        <tag>Distributed SLAM</tag>
        <tag>Scan Context</tag>
      </tags>
  </entry>
  <entry>
    <title>CT-ICP</title>
    <url>/summary/Dynamic%20Convolution/</url>
    <content><![CDATA[<p>传统 CNN 中，训练完成后所有 kernal 参数则固定。对于任意的输入，所有的 kernal 都对他们同等对待。所以为了提高模型 capacity，大多数方法堆叠卷积层（深度）或者增加卷积层的 channel 数（宽度），这种做法一定程度上可以提升模型能力，但会降低计算效率。</p>
<p>为了压缩模型，在增加模型 capacity 的同时不会增加太多参数和计算量，动态卷积在训练结束后，kernal 不再是一个定值，而是一个由 input 决定的变量。因此 kernal 相当于一个以 input 为自变量的函数。这种做法变相增加了模型 capacity，而模型参数和计算量是非常小的。</p>
<span id="more"></span>
<center><b><font size="9">动态卷积</font></b></center>

<h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1  简介"></a>1  简介</h1><p>随着模型参数的不断增加，计算成本也越来越高，对于一些对 latency 有较高要求的任务，显然是一种挑战。在传统 CNN 中，训练完成后所有 kernal 参数则固定。对于任意的输入，所有的 kernal 都对他们同等对待。所以为了提高模型 capacity，大多数方法堆叠卷积层（深度）或者增加卷积层的 channel 数（宽度），这种做法一定程度上可以提升模型能力，但会降低计算效率。</p>
<p>为了压缩模型，在增加模型 capacity 的同时不会增加太多参数和计算量，动态卷积在训练结束后，kernal 不再是一个定值，而是一个由 input 决定的变量。因此 kernal 相当于一个以 input 为自变量的函数。这种做法变相增加了模型 capacity，而模型参数和计算量是非常小的。</p>
<h1 id="2-动态感知机"><a href="#2-动态感知机" class="headerlink" title="2  动态感知机"></a>2  动态感知机</h1><p>通过感知机模型引出动态卷积，设感知机模型如下，$W,b,g$ 分别表示权重、偏置和激活函数：</p>
<script type="math/tex; mode=display">
y = g(W^T x + b)</script><p>动态感知机的定义如下，动态感知机模型如 $y$ 式，聚合权重（$\tilde{W}(x)$）和聚合偏置（$\tilde{b}(x)$）通过 $W$ 式和 $b$ 式中多个权重和偏置加权求和得到。需要满足的条件约束是权重系数和为 1，这也说明了权重系数不固定，随着输入数据的变化而变化。</p>
<script type="math/tex; mode=display">
y = g(\tilde{W}^T(x) x + \tilde{b}(x)) \\
\tilde{W}(x) = \sum_{k=1}^K \pi_k (x) \tilde{W}_k \\
\tilde{b}(x) = \sum_{k=1}^K \pi_k (x) \tilde{b}_k \\
\mathrm{s.t.} \ 0 \le \pi_k (x) \le 1, \ \sum_{k=1}^K \pi_k (x) = 1</script><p>式中 $\pi_k$ 表示第 $k$ 个线性函数 $\tilde{W}_k^T x + \tilde{b}_k$ 的 attention 权重，这个权重在输入 $x$ 不同的情况下是不同的。因此在给定 input 的情况下，动态感知器代表了该 input 的最佳线性函数组合。</p>
<h1 id="3-动态卷积"><a href="#3-动态卷积" class="headerlink" title="3  动态卷积"></a>3  动态卷积</h1><center>
    <img src="/summary/Dynamic%20Convolution/1 动态卷积.jpg">
    <div>
        图-1  动态卷积
    </div>
</center>

<p>动态卷积有 $K$ 个卷积核，这些卷积核有相同的大小，相同的输入和输出通道。</p>
<h2 id="3-1-注意力"><a href="#3-1-注意力" class="headerlink" title="3.1  注意力"></a>3.1  注意力</h2><p>动态卷积采用压缩激励操作来计算卷积核注意力 $\pi_k(x)$。首先，全局信息通过平均池化来进行压缩。然后，使用两个全连接层（之间采用 ReLU 激活函数）和 softmax 函数，来生成 $K$ 个卷积核的归一化注意力权重。其中，第一个全连接层用于降维。</p>
<h2 id="3-2-聚合卷积核"><a href="#3-2-聚合卷积核" class="headerlink" title="3.2  聚合卷积核"></a>3.2  聚合卷积核</h2><p>由于核的尺寸较小，聚合卷积核的计算效率较高。同时，在 softmax 中采用了以恶很大的温度来平衡注意力：</p>
<script type="math/tex; mode=display">
\pi_k = \frac{\exp (z_k / \gamma)}{\sum_j \exp (z_j / \gamma)}</script><p>$z_k$ 是注意力分支中的第二个全连接层的输出，$\gamma$ 是温度，原始 softmax 中 $\gamma = 1$。采用退火技巧，在前 10 次迭代中将 $\gamma$ 从 30 变成 1，可以进一步提高准确度。</p>
<h1 id="4-代码实现样例（PyTorch）"><a href="#4-代码实现样例（PyTorch）" class="headerlink" title="4  代码实现样例（PyTorch）"></a>4  代码实现样例（PyTorch）</h1><h2 id="4-1-attention-实现"><a href="#4-1-attention-实现" class="headerlink" title="4.1  attention 实现"></a>4.1  attention 实现</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Attention2d</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_planes, K</span>):</span><br><span class="line">        <span class="built_in">super</span>(Attention2d, self).__init__()</span><br><span class="line">        self.avgpool = nn.AdaptiveAvgPool2d(<span class="number">1</span>)</span><br><span class="line">        self.fc1 = nn.Conv2d(in_planes, K, <span class="number">1</span>)</span><br><span class="line">        self.fc2 = nn.Conv2d(K, K, <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.avgpool(x)</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.fc2(x).view(x.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> F.softmax(x, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h2 id="4-2-Dynamic-Convolution-实现"><a href="#4-2-Dynamic-Convolution-实现" class="headerlink" title="4.2  Dynamic Convolution 实现"></a>4.2  Dynamic Convolution 实现</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DynamicConvolution2d</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_planes, out_planes, kernel_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>,</span></span><br><span class="line"><span class="params">                 dilation=<span class="number">1</span>, groups=<span class="number">1</span>, bias=<span class="literal">True</span>, K=<span class="number">4</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(DynamicConvolution2d, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> in_planes % groups == <span class="number">0</span></span><br><span class="line">        self.in_planes = in_planes</span><br><span class="line">        self.out_planes = out_planes</span><br><span class="line">        self.kernel_size = kernel_size</span><br><span class="line">        self.stride = stride</span><br><span class="line">        self.padding = padding</span><br><span class="line">        self.dilation = dilation</span><br><span class="line">        self.groups = groups</span><br><span class="line">        self.bias = bias</span><br><span class="line">        self.K = K</span><br><span class="line">        self.attention = Attention2d(in_planes, K)</span><br><span class="line">        </span><br><span class="line">        self.weight = nn.Parameter(torch.Tensor(K, out_planes, </span><br><span class="line">                                                in_planes//groups, kernel_size, </span><br><span class="line">                                                kernel_size), </span><br><span class="line">                                   requires_grad=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">if</span> bias:</span><br><span class="line">            self.bias = nn.Parameter(torch.Tensor(K, out_planes))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.bias = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 将 batch 视作维度变量进行组卷积，因为组卷积的权重不同，动态卷积的权重也不同</span></span><br><span class="line">        softmax_attention = self.attention(x)</span><br><span class="line">        batch_size, in_planes, height, width = x.size()</span><br><span class="line">        x = x.view(<span class="number">1</span>, -<span class="number">1</span>, height, width)</span><br><span class="line">        weight = self.weight.view(self.K, -<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 动态卷积权重的生成，生成 batch_size 个卷积参数</span></span><br><span class="line">        aggregate_weight = torch.mm(sofrmax_attention, </span><br><span class="line">                                    weight).view(self.out_planes, -<span class="number">1</span>, </span><br><span class="line">                                                 self.kernel_size, </span><br><span class="line">                                                 self.kernel_size)</span><br><span class="line">        <span class="keyword">if</span> self.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            aggregate_bias = torch.mm(softmax_attention, slef.bias).view(-<span class="number">1</span>)</span><br><span class="line">            output = F.conv2d(x, weight=aggregate_weight, bias=aggreagate_bias, </span><br><span class="line">                              stride=self.stride, padding=self.padding, </span><br><span class="line">                              dilation=self.dilation, </span><br><span class="line">                              groups=self.groups*batch_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            output = F.conv2d(x, weight=aggregate_weight, bias=<span class="literal">None</span>, </span><br><span class="line">                              stride=self.stride, padding=self.padding, </span><br><span class="line">                              dilation=self.dilation, </span><br><span class="line">                              groups=self.groups*batch_size)</span><br><span class="line">        </span><br><span class="line">        output = output.view(batch_size, self.out_planes, </span><br><span class="line">                             output.size(-<span class="number">2</span>), output.size(-<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://zhuanlan.zhihu.com/p/149805261">https://zhuanlan.zhihu.com/p/149805261</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/157174227">https://zhuanlan.zhihu.com/p/157174227</a></li>
<li><a href="https://blog.csdn.net/m0_45447650/article/details/123992367">https://blog.csdn.net/m0_45447650/article/details/123992367</a></li>
</ul>
<h1 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h1><ul>
<li><a href="https://github.com/kaijieshi7/Dynamic-convolution-Pytorch">https://github.com/kaijieshi7/Dynamic-convolution-Pytorch</a></li>
</ul>
]]></content>
      <categories>
        <category>Summary</category>
      </categories>
      <tags>
        <tag>Summary</tag>
        <tag>Deep Learning</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>CT-ICP</title>
    <url>/papernote/Dynamic%20SLAM-The%20Need%20For%20Speed/</url>
    <content><![CDATA[<p>关于论文 Dynamic SLAM: The Need For Speed (ICRA, 202) 的阅读总结。</p>
<span id="more"></span>
<center><b><font size="9">Dynamic SLAM: The Need For Speed</font></b></center>

<h1 id="论文情况"><a href="#论文情况" class="headerlink" title="论文情况"></a>论文情况</h1><ul>
<li>标题：Dynamic SLAM: The Need For Speed</li>
<li>作者：Mina Henein, Jun Zhang, Robert Mahony and Viorela Ila</li>
<li>期刊：ICRA 2020</li>
<li>源码：<a href="https://github.com/halajun/VDO_SLAM（与">https://github.com/halajun/VDO_SLAM（与</a> VDO-SLAM 出自同一个组）</li>
</ul>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1  Introduction"></a>1  Introduction</h1><p>大量 SLAM 问题基于静态世界假设，这限制了现存算法在包含动态物体的真实世界的精确度。</p>
<p>DL 的发展提供了几乎实时目标检测和分割的可靠算法：</p>
<ul>
<li>将 DL 融入几何 SLAM，除了检测和分割，还需要 3D 模型是可用的，或者前段显示提供对象的位姿。对 3D 模型的精确度要求限制了潜在的应用领域。</li>
<li>同时，多目标跟踪和 3D 位姿仍然是一大挑战。需要一种算法，可以利用现有 DL 强大的检测和分割能力，而不依赖额外的位姿估计或运动模型先验。</li>
</ul>
<p>本文提出来一个无模型、目标感知的基于点的动态 SLAM 模型：</p>
<ul>
<li>利用基于图像的语义信息进行机器人定位、绘制静态结构、估计动态物体的完整 $SE(3)$ 位姿变换、构建世界的动态表示。</li>
<li>利用物体的刚体运动来提取场景中物体的速度信息，如 Fig.1。</li>
</ul>
<center>
    <img src="/papernote/Dynamic%20SLAM-The%20Need%20For%20Speed/Figure-1.png">
</center>

<p>本文关键创新：</p>
<ul>
<li>一种新颖的姿态变化表示，用于对给定刚体上点集合的运动进行建模，并将模型集成到 SLAM 优化框架。只要对象的语义检测和分割可以被跟踪，生成的模型与对象的底层 3D 模型无关。</li>
<li>本文是第一个能够估计相机位姿、静态和动态结构、场景中每个刚性物体的完整 $SE(3)$ 位姿变化的工作。</li>
</ul>
<h1 id="2-Accounting-for-Dynamic-Obects-in-SLAM"><a href="#2-Accounting-for-Dynamic-Obects-in-SLAM" class="headerlink" title="2  Accounting for Dynamic Obects in SLAM"></a>2  Accounting for Dynamic Obects in SLAM</h1><p>问题描述：SLAM 下机器人的感知范围内存在相对较大的动态刚体。SLAM 前端能在不同时刻识别并提取同一物体上的点，这些点共享在一个潜在运动约束上，用于提高 SLAM 性能。</p>
<h2 id="2-1-Problem-Formulation"><a href="#2-1-Problem-Formulation" class="headerlink" title="2.1  Problem Formulation"></a>2.1  Problem Formulation</h2><p>动态物体 SLAM：</p>
<ul>
<li>使用因子图，获取静态和动态 3D 结构、位姿变换约束。</li>
</ul>
<p>考虑高斯噪声，问题变为非线性最小二乘问题：</p>
<ul>
<li>机器人位姿 $\pmb{x} = \{ x_0, …, x_{n_x} \}, x_k \in SE(3), k \in \{ 0, …, n_x \}$，其中 $n_x$ 为时间步数量；</li>
<li>不同时间步环境中的 3D 点特征 $\pmb{l} = \{ l_0^1, …, l_{n_x}^{n_l} \}, l_k^i \in \mathbb{R}^3$，其中$i \in \{ 1, …, n_l \}$ 为 landmark 的唯一索引，$n_l$ 为检测到的 landmark 的总数。$\pmb{l} = \pmb{l_s} \cup \pmb{l_d}$ 包含静态 landmark 集合 $\pmb{l_s}$ 和动态 landmark 集合 $\pmb{l_d}$。</li>
<li>一个运动目标在不同时间的同一个点用不同的变量表示，即 $l_{k-1}^i$ 和 $l_k^i$，即不同时间步的第 $i$ 个特征点。</li>
</ul>
<h2 id="2-2-Motion-Model-of-a-Point-on-a-Rigid-Body"><a href="#2-2-Motion-Model-of-a-Point-on-a-Rigid-Body" class="headerlink" title="2.2  Motion Model of a Point on a Rigid Body"></a>2.2  Motion Model of a Point on a Rigid Body</h2><h3 id="2-2-1-符号设定"><a href="#2-2-1-符号设定" class="headerlink" title="2.2.1  符号设定"></a>2.2.1  符号设定</h3><p>符号：</p>
<ul>
<li><p>设 $\{ 0 \}$ 为参考坐标系（世界坐标系），$\{ L \}$ 为运动刚体的坐标系。</p>
</li>
<li><p>${}^0 L_k \in SE(3)$ 为刚体相对坐标系 $\{ 0 \}$ 的位姿。</p>
</li>
<li><p>对于物体上的特征点，${}^L l^i \in \mathbb{R}^3$ 表示点在该物体坐标系的坐标。记 ${}^0 l_k^i$ 为同一个特征点在时间步 $k$ 相对于坐标系 $\{0\}$ 的坐标。</p>
</li>
<li><p>注意，对运动中对刚体来说，${}^L l^i$ 是常量，${}^0 L_k, {}^0 l_k^i$ 则是随时间变化的。存在关系：</p>
<script type="math/tex; mode=display">
  {}^L \bar{l}^i = {}^0 L_k^{-1} {}^0\bar{l}_k^i
  \tag{1}</script><p>  其中 $\bar{l}$ 为齐次坐标。</p>
</li>
</ul>
<h3 id="2-2-2-坐标系变化之间的关系"><a href="#2-2-2-坐标系变化之间的关系" class="headerlink" title="2.2.2  坐标系变化之间的关系"></a>2.2.2  坐标系变化之间的关系</h3><p>物体 $L$ 从 $k-1$ 到 $k$ 到相对变换表示为 ${}^{L_{k-1}}_{k-1} H_k \in SE(3)$，成为 body-fixed 坐标位姿系变换，且存在关系：</p>
<script type="math/tex; mode=display">
{}_{k-1}^{L_{k-1}} H_k = {}^0 L_{k-1}^{-1} {}^0 L_k
\tag{2}</script><p>Fig.2a 表示了三个连续位姿的上述关系：</p>
<center>
    <img src="/papernote/Dynamic%20SLAM-The%20Need%20For%20Speed/Figure-2.png">
</center>

<p>由 (2) 可以得到心得刚体坐标可以写为增量形式：</p>
<script type="math/tex; mode=display">
{}^0 L_k = {}^0 L_{k-1} {}_{k-1}^{L_{k-1}} H_k
\tag{3}</script><p>考虑物体坐标系 $\{ L \}$ 中的点 ${}^L l^i$，联合 (1) (3) 可以得到：</p>
<script type="math/tex; mode=display">
{}^0 \bar{l}_k^i = {}^0 L_{k-1} {}_{k-1}^{L_{k-1}} H_k {}^0 L_{k-1}^{-1} {}^0\bar{l}_{k-1}^i
\tag{4}</script><p>由 (4) 可以得到 ${}^0_{k-1}H_k = {}^0 L_{k-1} {}_{k-1}^{L_{k-1}} H_k {}^0 L_{k-1}^{-1} \in SE(3)$，表示了位姿变换的坐标系变化，并说明了 (2) 中 body-fixed 坐标系的坐标系变化与参考坐标系的坐标系变化之间的关系。从而有：</p>
<script type="math/tex; mode=display">
{}^0 \bar{l}_k^i = {}^0_{k-1}H_k {}^0\bar{l}_{k-1}^i
\tag{5}</script><p>(5) 是本文方法的关键点，其限制了对物体位姿 ${}^0 L_k$ 估计的必要性，允许直接使用参考坐标系中的点 $\bar{l}_k^i$。</p>
<h3 id="2-2-3-线速度提取"><a href="#2-2-3-线速度提取" class="headerlink" title="2.2.3  线速度提取"></a>2.2.3  线速度提取</h3><p>给定刚体在惯性坐标系下的位姿变换 ${}_{k-1}^0 H_k$，刚体的线速度向量为：</p>
<script type="math/tex; mode=display">
v = {}_{k-1}^0 t_k - (I_3 - {}_{k-1}^0 R_k) c_{k-1}
\tag{6}</script><p>其中 ${}_{k-1}^0 R_k \in SO(3)$ 和 ${}_{k-1}^0 t_k \in \mathbb{R}^3$ 表示刚体位姿变换 ${}_{k-1}^0 H_k$ 的旋转和平移，$I_3$ 为单位矩阵，$c_{k-1}$ 为刚体在 $k-1$ 时的质心坐标。</p>
<p>由于算法设计，并没有直接获取物体的质心，而是近似为物体上检测到的特征点的 3D 质心。(6) 的详细推导参考 [1]。</p>
<h2 id="2-3-Motion-Factors-in-Dynamic-SLAM"><a href="#2-3-Motion-Factors-in-Dynamic-SLAM" class="headerlink" title="2.3  Motion Factors in Dynamic SLAM"></a>2.3  Motion Factors in Dynamic SLAM</h2><p>为了估计了相机位姿、静态和动态结构、动态结构的运动，运动因子和从机器人本体传感器获得的里程数，以及 landmark 观测被联合优化：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\boldsymbol{\theta}^*&=\underset{\boldsymbol{\theta}}{\operatorname{argmin}}\left\{\sum_{k=1}^{m_k} \rho_h\left(\left(h\left(x_k, l_k^i\right)-z_k^i\right)^{\top} \Sigma_{w_k}^{-1}\left(h\left(x_k, l_k^i\right)-z_k^i\right)\right)+\right. \\
&\sum_{i=1}^{m_i} \rho_h\left(\left(f\left(x_{k-1}, x_k\right)-o_k\right)^{\top} \Sigma_{v_k}^{-1}\left(f\left(x_{k-1}, x_k\right)-o_k\right)\right)+ \\
&\sum_{i, j}^{m_s} \rho_h\left(\left(g\left(l_{k-1}^i, l_k^i,{ }_{k-1}^0 H_k^j\right)\right)^{\top} \Sigma_q^{-1}\left(g\left(l_{k-1}^i, l_k^i,{ }_{k-1}^0 H_k^j\right)\right)\right\} \\
\pmb{\theta} &= \pmb{x} \cup \pmb{l} \cup \pmb{H}
\end{aligned}</script><p>$\pmb{H}$ 表示描述物体运动的变量集合。</p>
<p>其中（参考 Fig.2b 中红线）：</p>
<ul>
<li><p>$\rho_h$ 为 Huber 函数；</p>
</li>
<li><p>$h(x_k, l_k^i)$ 为 3D 点测量模型，$\Sigma_{w_k}$ 为点测量协方差，$\pmb{z} = \{ z_1, …, z_{m_k} \}, z_k \in \mathbb{R}^3$ 表示所有的时间步下的 $m_k$ 个 3D 点测量；</p>
</li>
<li><p>$f(x_{k-1}, x_k)$ 为惯导模型，$\Sigma_{v_k}$ 为惯导协方差，$\pmb{o} = \{ o_1, …, o_{m_i} \}$ 表示 $m_i$ 个惯导测量；</p>
</li>
<li><p>$g\left(l_{k-1}^i, l_k^i,{ }_{k-1}^0 H_k^j\right)$ 表示动态物体上的点点运动模型，$\Sigma_q$ 为运动协方差，$m_s$ 为所有的运动因子的数量。被检测到的刚体 $j$ 上的任意点的运动可以使用 (5) 表述为：</p>
<script type="math/tex; mode=display">
  g\left(l_{k-1}^i, l_k^i,{ }_{k-1}^0 H_k^j\right)={ }^0 l_{k-1}^i-{ }_{k-1}^0 R_k^{j 0} l_{k-1}^i-{ }_{k-1}^0 t_k^j+q_{s_j}
  \tag{8}</script><p>  其中 $q_s \sim \mathcal{N}(0, \Sigma_q)$ 为高斯噪声</p>
</li>
</ul>
<p>(8) 中的因子为三元因子（Fig.3 中橙色因子），成为刚体上点的运动模型。</p>
<center>
    <img src="/papernote/Dynamic%20SLAM-The%20Need%20For%20Speed/Figure-3.png">
</center>



<h2 id="2-4-The-Fractor-Graph"><a href="#2-4-The-Fractor-Graph" class="headerlink" title="2.4  The Fractor Graph"></a>2.4  The Fractor Graph</h2><p>本文评估了使用/没使用恒定运动（constant motion）模型的场景：</p>
<ul>
<li>城市场景：当物体的运动受到变化（加速、减速等）的影响时，运动建模是具有挑战性的。因此，允许在每个时间步上估计一个新的位姿变化。Fig.3a 为这种情况的因子图，途中使用两个运动顶点对两个不同的时间过渡估计同一物体的运动。一种约束是最小化这些运动估计之间的变化。</li>
<li>高速公路：每个物体（车辆）保持恒定运动，如 Fig.3b。</li>
</ul>
<p>进一步，如果 body-fixed 坐标系的位姿变化是恒定的，那么参考坐标系的位姿变化也是恒定的。对于任意 $k-1$ 和 $k’-1$，则：</p>
<script type="math/tex; mode=display">
{}_{k-1}^{L_{k-1}} H_k = C ={}_{k'-1}^{L_{k'-1}} H_{k'} \in SE(3) 
\tag{9}</script><p>用 (9) 替换 (3) 得到 ${}^0 L_k = {}^0 L_{k-1} C$，且有 ${}^0_{k-1}H_k = {}^0 L_{k-1} C {}^0 L_{k-1}^{-1}$，则：</p>
<script type="math/tex; mode=display">
{}^0_{k-1}H_k = {}^0 L_{k} C {}^0 L_{k}^{-1} = {}^0_k H_{k+1}
\tag{10}</script><p>这说明对于物体 $j$ 和任意时间 $k, k’$，有 ${}^0_{k-1}H_k^j = {}^0 H^j = {}^0_{k’} H^j_{k’+1} \in SE(3)$。</p>
<h1 id="3-System-Overview"><a href="#3-System-Overview" class="headerlink" title="3  System Overview"></a>3  System Overview</h1><center>
    <img src="/papernote/Dynamic%20SLAM-The%20Need%20For%20Speed/Figure-4.png">
</center>

<p>Fig.4 中设机器人携带 RGB-D 相机以及惯性传感器（里程计，IMU）。</p>
<p>为了保证运动物体上的特征被检测到，使用一个实例级的物体分割算法来产生物体掩码。前端利用物体掩码来检测潜在移动物体和静态背景上的特征。SLAM 前端通过目标分割和特征跟踪，识别和关联同一刚体对象上不同时间步的点。这些点共享一个基本的运动模型，利用这个模型来实现同步定位、映射和移动对象跟踪。</p>
<p>静态和动态三维测量以及本体感知传感器的测量被集成到后端，以同时估计摄像机运动、静态和动态结构以及场景中被检测对象的 SE(3) 位姿转换。</p>
<h1 id="4-Experiments-and-Results"><a href="#4-Experiments-and-Results" class="headerlink" title="4  Experiments and Results"></a>4  Experiments and Results</h1><ul>
<li>误差度量：<ul>
<li>相对平移误差 RTE；</li>
<li>相对旋转误差 RRE；</li>
<li>相对结构误差 RSE（所有静态和动态结构 landmark 在预测和 GT 中的位置误差）；</li>
<li>物体运动平移误差 OMTE，物体运动旋转误差 OMRE，物体运动速度误差 OMSE。</li>
</ul>
</li>
<li>数据集 KITTI 上的评估：只对分割掩码达到一定比例的目标进行估计，确保排除了远处和部分观察到的进入/离开相机视野的物体。对于 vKITTI，阈值设置为 6%，对于 KITTI  设置为2%。</li>
</ul>
<p>特征跟踪误差如 Fig.5，使用 PWC-Net 和特征描述子匹配，评价标准为 end-point error（EPE）：</p>
<center>
    <img src="/papernote/Dynamic%20SLAM-The%20Need%20For%20Speed/Figure-5.png">
</center>

<p>在观测的每个轴上分别添加 0 均值，标准差 $\sigma_1 = 0.02, \sigma_2 = 0.04, \sigma_3 = 0.06m$ 的高斯噪声，不同前端组件对相机位姿变换的预测准确率如 Fig.6：</p>
<center>
    <img src="/papernote/Dynamic%20SLAM-The%20Need%20For%20Speed/Figure-6.png">
</center>

<p>在 KITTI 上的实验结果如 Table II，Static Only 表示不考虑场景中的动态物体：</p>
<center>
    <img src="/papernote/Dynamic%20SLAM-The%20Need%20For%20Speed/Table-2.png">
</center>



<h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><p>为证明 (6) 等价于在 $\{ 0 \}$ 处观测到的从时间 $k-1$ 到 $k$ 物体位姿原点到平移，先进性如下的变换：</p>
<script type="math/tex; mode=display">
{}^0_{k-1}H_k = {}^0 L_{k-1} {}_{k-1}^{L_{k-1}} H_k {}^0 L_{k-1}^{-1} = {}^0 L_k {}^0 L_{k-1}^{-1}
\tag{11}</script><p>设 ${}^0 R_{L_{k-1}} \in SO(3)$ 和 ${}^0 t_{L_{k-1}} \in \mathbb{R}^3$ 为 ${}^0 L_{k-1}$ 中的旋转和平移，则 ${}^0_{k-1}H_k$ 的平移和旋转可以表示为 ${}^0 t_{L_{k}} - {}^0 R_{L_{k}}{}^0 R_{L_{k-1}}^\top {}^0 t_{L_{k-1}}$ 和 $R_{L_{k}}{}^0 R_{L_{k-1}}^\top$。将这两个式子带入 (6) 得到：</p>
<script type="math/tex; mode=display">
v = {}^0 t_{L_{k}} - {}^0 R_{L_{k}}{}^0 R_{L_{k-1}}^\top {}^0 t_{L_{k-1}} - (I_3 - {}^0 R_{L_{k}}{}^0 R_{L_{k-1}}^\top) {}^0 t_{L_{k-1}}
\tag{12}</script><p>最后化简得 $v = {}^0 t_{L_{k}} - {}^0 t_{L_{k-1}}$。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li>[1] G. S. Chirikjian, R. Mahony, S. Ruan, and J. Trumpf, “Pose changes from a different point of view,” in <em>Proceedings of the ASME International Design Engineering Technical Conferences (IDETC) 2017</em>. ASME, 2017.</li>
</ul>
]]></content>
      <categories>
        <category>Paper Note</category>
      </categories>
      <tags>
        <tag>Semantic SLAM</tag>
        <tag>Paper Note</tag>
        <tag>Dynamic Environmet</tag>
      </tags>
  </entry>
  <entry>
    <title>E-LOAM</title>
    <url>/papernote/E-LOAM/</url>
    <content><![CDATA[<p>关于论文 E-LOAM: LiDAR Odometry and Mapping with Expanded Local Structural Information (IEEE Transactions on Intelligent Vehicles, 2022) 的阅读总结。</p>
<span id="more"></span>
<p><img src="/papernote/E-LOAM/幻灯片1.png" alt><br><img src="/papernote/E-LOAM/幻灯片2.png" alt><br><img src="/papernote/E-LOAM/幻灯片3.png" alt><br><img src="/papernote/E-LOAM/幻灯片4.png" alt><br><img src="/papernote/E-LOAM/幻灯片5.png" alt><br><img src="/papernote/E-LOAM/幻灯片6.png" alt><br><img src="/papernote/E-LOAM/幻灯片7.png" alt><br><img src="/papernote/E-LOAM/幻灯片8.png" alt><br><img src="/papernote/E-LOAM/幻灯片9.png" alt><br><img src="/papernote/E-LOAM/幻灯片10.png" alt><br><img src="/papernote/E-LOAM/幻灯片11.png" alt><br><img src="/papernote/E-LOAM/幻灯片12.png" alt><br><img src="/papernote/E-LOAM/幻灯片13.png" alt><br><img src="/papernote/E-LOAM/幻灯片14.png" alt><br><img src="/papernote/E-LOAM/幻灯片15.png" alt><br><img src="/papernote/E-LOAM/幻灯片16.png" alt><br><img src="/papernote/E-LOAM/幻灯片17.png" alt><br><img src="/papernote/E-LOAM/幻灯片18.png" alt><br><img src="/papernote/E-LOAM/幻灯片19.png" alt><br><img src="/papernote/E-LOAM/幻灯片20.png" alt></p>
]]></content>
      <categories>
        <category>Paper Note</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Paper Note</tag>
        <tag>LiDAR SLAM</tag>
        <tag>LOAM</tag>
      </tags>
  </entry>
  <entry>
    <title>End2End Multi-View Feature Matching</title>
    <url>/papernote/End2End%20Multi-View%20Feature%20Matching/</url>
    <content><![CDATA[<p>关于论文 End2End Multi-View Feature Matching using Differentiable Pose Optimization (arXiv:2205.01694) 的阅读总结。</p>
<span id="more"></span>
<p><img src="/papernote/End2End%20Multi-View%20Feature%20Matching/幻灯片1.png" alt><br><img src="/papernote/End2End%20Multi-View%20Feature%20Matching/幻灯片2.png" alt><br><img src="/papernote/End2End%20Multi-View%20Feature%20Matching/幻灯片3.png" alt><br><img src="/papernote/End2End%20Multi-View%20Feature%20Matching/幻灯片4.png" alt><br><img src="/papernote/End2End%20Multi-View%20Feature%20Matching/幻灯片5.png" alt><br><img src="/papernote/End2End%20Multi-View%20Feature%20Matching/幻灯片6.png" alt><br><img src="/papernote/End2End%20Multi-View%20Feature%20Matching/幻灯片7.png" alt><br><img src="/papernote/End2End%20Multi-View%20Feature%20Matching/幻灯片8.png" alt><br><img src="/papernote/End2End%20Multi-View%20Feature%20Matching/幻灯片9.png" alt><br><img src="/papernote/End2End%20Multi-View%20Feature%20Matching/幻灯片10.png" alt><br><img src="/papernote/End2End%20Multi-View%20Feature%20Matching/幻灯片11.png" alt><br><img src="/papernote/End2End%20Multi-View%20Feature%20Matching/幻灯片12.png" alt><br><img src="/papernote/End2End%20Multi-View%20Feature%20Matching/幻灯片13.png" alt><br><img src="/papernote/End2End%20Multi-View%20Feature%20Matching/幻灯片14.png" alt><br><img src="/papernote/End2End%20Multi-View%20Feature%20Matching/幻灯片15.png" alt><br><img src="/papernote/End2End%20Multi-View%20Feature%20Matching/幻灯片16.png" alt><br><img src="/papernote/End2End%20Multi-View%20Feature%20Matching/幻灯片17.png" alt><br><img src="/papernote/End2End%20Multi-View%20Feature%20Matching/幻灯片18.png" alt><br><img src="/papernote/End2End%20Multi-View%20Feature%20Matching/幻灯片19.png" alt><br><img src="/papernote/End2End%20Multi-View%20Feature%20Matching/幻灯片20.png" alt><br><img src="/papernote/End2End%20Multi-View%20Feature%20Matching/幻灯片21.png" alt><br><img src="/papernote/End2End%20Multi-View%20Feature%20Matching/幻灯片22.png" alt><br><img src="/papernote/End2End%20Multi-View%20Feature%20Matching/幻灯片23.png" alt></p>
]]></content>
      <categories>
        <category>Paper Note</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>SLAM</tag>
        <tag>Paper Note</tag>
        <tag>Feature Matching</tag>
      </tags>
  </entry>
  <entry>
    <title>F-LOAM</title>
    <url>/papernote/F-LOAM/</url>
    <content><![CDATA[<p>关于论文 F-LOAM: Fast LiDAR Odometry And Mapping (IROS, 2021) 的阅读总结。</p>
<span id="more"></span>
<p><img src="/papernote/F-LOAM/幻灯片1.png" alt><br><img src="/papernote/F-LOAM/幻灯片2.png" alt><br><img src="/papernote/F-LOAM/幻灯片3.png" alt><br><img src="/papernote/F-LOAM/幻灯片4.png" alt><br><img src="/papernote/F-LOAM/幻灯片5.png" alt><br><img src="/papernote/F-LOAM/幻灯片6.png" alt><br><img src="/papernote/F-LOAM/幻灯片7.png" alt><br><img src="/papernote/F-LOAM/幻灯片8.png" alt><br><img src="/papernote/F-LOAM/幻灯片9.png" alt><br><img src="/papernote/F-LOAM/幻灯片10.png" alt><br><img src="/papernote/F-LOAM/幻灯片11.png" alt><br><img src="/papernote/F-LOAM/幻灯片12.png" alt><br><img src="/papernote/F-LOAM/幻灯片13.png" alt><br><img src="/papernote/F-LOAM/幻灯片14.png" alt><br><img src="/papernote/F-LOAM/幻灯片15.png" alt><br><img src="/papernote/F-LOAM/幻灯片16.png" alt></p>
]]></content>
      <categories>
        <category>Paper Note</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Paper Note</tag>
        <tag>LiDAR SLAM</tag>
        <tag>LOAM</tag>
      </tags>
  </entry>
  <entry>
    <title>Epipolar Geometry</title>
    <url>/summary/Epipolar%20Geometry/</url>
    <content><![CDATA[<p>对极几何（Epipolar Geometry）是 Structure from Motion 问题中，在两个相机位置产生的两幅图像之间存在的一种特殊几何关系，是 SfM 问题中 2D-2D 求解两帧间相机姿态的基本模型。</p>
<p>本文结合网络资源对对极几何进行整理。</p>
<span id="more"></span>
<center><b><font size="9">Epipolar Gemotry</font></b></center>

<h1 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="1  基本概念"></a>1  基本概念</h1><p>对极几何（Epipolar Geometry）是 Structure from Motion 问题中，在两个相机位置产生的两幅图像之间存在的一种特殊几何关系，是 SfM 问题中 2D-2D 求解两帧间相机姿态的基本模型。</p>
<h1 id="2-基本模型"><a href="#2-基本模型" class="headerlink" title="2  基本模型"></a>2  基本模型</h1><center>
    <img src="/summary/Epipolar%20Geometry/Figure-1.jpg">
</center>

<p>图中 $c_0, c_1$ 为两个相机中心，$\bold{p}$ 为空间中一点，$\bold{p}$ 在 $c_0, c_1$ 对应的像平面上的投影分别为 $\bold{x}_0, \bold{x}_1$。$c_0, c_1$ 连线与像平面的交点 $e_0, e_1$ 为极点（epipole），$\bold{l}_0, \bold{l}_1$ 为极线（epipolar line），$c_0, c_1, p$ 三点组成的平面称为极平面（epipolar plane）。</p>
<h1 id="3-对极约束"><a href="#3-对极约束" class="headerlink" title="3  对极约束"></a>3  对极约束</h1><p>根据针孔相机模型，相机成像平面一点的像素坐标 $\bold{p}$ 和该点在世界坐标系下的 3D 坐标 $\bold{P}$ 有 $\bold{p} = \bold{KP}$ 的关系，使用齐次坐标则有：</p>
<script type="math/tex; mode=display">
d \bold{p} = \bold{KP}
\tag{3.1}</script><p>其中 $d$ 为点深度，$\bold{K}$ 为内参矩阵，$\bold{p}$ 为齐次坐标。</p>
<p>以第一个相机的坐标系为参照，对于两个相机则有：</p>
<script type="math/tex; mode=display">
d_0 \bold{p}_0 = \bold{K}_0 \bold{P},\ d_1 \bold{p}_1 = \bold{K}_1 (\bold{RP} + \bold{t})
\tag{3.2}</script><p>令 $\bold{x} = \bold{K}^{-1} \bold{p}$，则上式可以归一化为：</p>
<script type="math/tex; mode=display">
d_0 \bold{x}_0 = \bold{P},\ d_1 \bold{x}_1 = \bold{RP} + \bold{t}
\tag{3.3}</script><p>由这两式得：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&d_1 \bold{x}_1 = \bold{R} (d_0 \bold{x}_0) + \bold{t} \\
\Rightarrow &\bold{t} \times d_1 \bold{x}_1 = \bold{t} \times \bold{R} d_0 \bold{x}_0 + \bold{t} \times \bold{t}\ (\text{叉乘}\ \bold{t}) \\
\Rightarrow &\bold{t} \times d_1 \bold{x}_1 = \bold{t} \times \bold{R} d_0 \bold{x}_0 \\
\Rightarrow &\bold{x}^T_1 (\bold{t} \times d_1 \bold{x}_1) = \bold{x}^T_1 (\bold{t} \times \bold{R} d_0 \bold{x}_0) \\
\Rightarrow &d_1 \bold{x}_1^T \bold{t}^{\wedge} \bold{x}_1 = d_0 \bold{x}_1^T \bold{t}^{\wedge} \bold{R} \bold{x}_0
\end{aligned}
\tag{3.4}</script><p>由于等号左边左乘 $\bold{x}_1^T$ 为乘了一个与自身（$\bold{t}^{\wedge} \bold{x}_1$）垂直的向量，因此等于 0，故而：</p>
<script type="math/tex; mode=display">
\bold{x}^T_1 \bold{t} \times \bold{R} \bold{x}_0 = \bold{x}^T_1 \bold{t}^{\wedge} \bold{R} \bold{x}_0 = 0
\tag{3.5}</script><p>上式即为<strong>对极约束</strong>。</p>
<p>对极约束的几何意义：</p>
<ul>
<li>$\bold{x}_1, \bold{t}, \bold{R} \bold{x}_0$ 三者的混合积为 0，即三个向量共勉，即上图中三角形的三边共面。</li>
</ul>
<h1 id="4-本质矩阵"><a href="#4-本质矩阵" class="headerlink" title="4  本质矩阵"></a>4  本质矩阵</h1><p>令 $\bold{E} = \bold{t} \times \bold{R} = \bold{t}^{\wedge} \bold{R}$，得到对极约束的新形式：</p>
<script type="math/tex; mode=display">
\bold{x}_1^T \bold{E} \bold{x}_0 = 0
\tag{4.1}</script><p>$\bold{E}$ 称为本质矩阵（Essential Matrix），由外参 $\bold{R}$ 和 $\bold{t}$ 决定。</p>
<p>本质矩阵的几何意义：</p>
<ul>
<li>$\bold{x}_1^T \bold{l}_1 = 0$，即 $\bold{x}_1$ 在直线 $\bold{l}_1 = \bold{E} \bold{x}_0$ 上，表示 $\bold{E}$ 将 $\bold{x}_0$ 投影到另一幅图像中的直线 $\bold{l}_1$ 上。</li>
</ul>
<h1 id="5-基本矩阵"><a href="#5-基本矩阵" class="headerlink" title="5  基本矩阵"></a>5  基本矩阵</h1><p>引入相机内参矩阵，将像点映射到像素平面：</p>
<script type="math/tex; mode=display">
\begin{cases}
\bold{p}_0 = \bold{K}_0 \bold{x}_0 \\
\bold{p}_1 = \bold{K}_1 \bold{x}_1
\end{cases}
\tag{5.1}</script><p>带入本质矩阵的  (4.1) 式可得：</p>
<script type="math/tex; mode=display">
\bold{p}_1^T \bold{K}_1^{-T} \bold{E} \bold{K}_0^{-1} \bold{p}_0 = 0
\tag{5.2}</script><p>令 $\bold{F} = \bold{K}_1^{-T} \bold{E} \bold{K}_0^{-1}$，上式变为：</p>
<script type="math/tex; mode=display">
\bold{p}_1^T \bold{F} \bold{p}_0 = 0
\tag{5.3}</script><p>$\bold{F}$ 称为基本矩阵（Fundamental Matrix），表示了同一 3D 点在两个相机像素平面上像素点之间的几何约束关系。</p>
<p><strong>本征矩阵与基本矩阵表征了两个透视模型对极几何的代数特征，以上 (4.1) 和 (5.2) 二式共同构成对极约束（Epipolar Constraint）。</strong></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.cnblogs.com/clarenceliang/p/6704970.html">https://www.cnblogs.com/clarenceliang/p/6704970.html</a></li>
<li><a href="https://blog.csdn.net/ssw_1990/article/details/53355572">https://blog.csdn.net/ssw_1990/article/details/53355572</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/399190891">https://zhuanlan.zhihu.com/p/399190891</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/79845576">https://zhuanlan.zhihu.com/p/79845576</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/33458436">https://zhuanlan.zhihu.com/p/33458436</a></li>
</ul>
]]></content>
      <categories>
        <category>Summary</category>
      </categories>
      <tags>
        <tag>Summary</tag>
        <tag>Knowledge</tag>
        <tag>SLAM</tag>
        <tag>Epipolar Geometry</tag>
        <tag>SfM</tag>
      </tags>
  </entry>
  <entry>
    <title>FPN</title>
    <url>/summary/FPN/</url>
    <content><![CDATA[<p>特征金字塔是多尺度目标检测中的一个基本组成部分，但是由于特征金字塔计算量大，所以多数方法为了检测速度而尽可能的避免使用特征金字塔，而是只使用高层的特征来进行预测。高层的特征虽然包含了丰富的语义信息，但是由于低分辨率，很难准确地保存物体的位置信息。与之相反，低层的特征虽然语义信息较少，但是由于分辨率高，就可以准确地包含物体位置信息。所以如果可以将低层的特征和高层的特征融合起来，就能得到一个识别和定位都准确的目标检测系统。</p>
<p>本文结合网络资源对 FPN 进行整理。</p>
<span id="more"></span>
<center><b><font size="9">特征金字塔网络</font></b></center>

<hr>
<h1 id="1-动机"><a href="#1-动机" class="headerlink" title="1  动机"></a>1  动机</h1><p>特征金字塔是多尺度目标检测中的一个基本组成部分，但是由于特征金字塔计算量大，所以多数方法为了检测速度而尽可能的避免使用特征金字塔，而是只使用高层的特征来进行预测。高层的特征虽然包含了丰富的语义信息，但是由于低分辨率，很难准确地保存物体的位置信息。与之相反，低层的特征虽然语义信息较少，但是由于分辨率高，就可以准确地包含物体位置信息。所以如果可以将低层的特征和高层的特征融合起来，就能得到一个识别和定位都准确的目标检测系统。</p>
<h1 id="2-结构"><a href="#2-结构" class="headerlink" title="2  结构"></a>2  结构</h1><center>
    <img src="/summary/FPN/1 常见结构.jpg">
    <div>
        图-1 常用的目标检测结构
    </div>
</center>


<ul>
<li>(a) Featurized image pyramid：先把图片拓展或压缩为不同尺寸，对每种尺寸的图片提取不同尺度的特征，再对每个尺度的特征都进行单独的预测。<ul>
<li>优点：不同尺度的特征都可以包含很丰富的语义信息。</li>
<li>缺点：时间成本太高。</li>
</ul>
</li>
<li>(b) Single feature map：这是在 SPPnet，Fast R-CNN，Faster R-CNN 中使用的，在网络的最后一层的特征图上进行预测。<ul>
<li>优点：计算速度比较快。</li>
<li>缺点：最后一层的特征图分辨率低，不能准确的包含物体的位置信息。</li>
</ul>
</li>
<li>(c) Pyramid feature hierarchy：这是 SSD 采用的多尺度融合的方法，从网络不同层抽取不同尺度的特征，然后在这不同尺度的特征上分别进行预测。<ul>
<li>优点：不需要额外的计算量。</li>
<li>缺点：有些尺度的特征语义信息不是很丰富，此外，SSD 没有用到足够低层的特征，而 FPN 的作者认为低层的特征对于小物体检测是非常有帮助的。</li>
</ul>
</li>
</ul>
<p>所以，为了使得不同尺度的特征都包含丰富的语义信息，同时又不使得计算成本过高，FPN 采用 Top-down 和 Lateral connection 的方式，让低层高分辨率低语义的特征和高层低分辨率高语义的特征融合在一起，使得最终得到的不同尺度的特征图都有丰富的语义信息，如图 (d)。</p>
<h1 id="3-FPN（Feature-Pyramid-Network）"><a href="#3-FPN（Feature-Pyramid-Network）" class="headerlink" title="3  FPN（Feature Pyramid Network）"></a>3  FPN（Feature Pyramid Network）</h1><p>特征金字塔网络的结构主要包括三部分，如图-2，上半部分自左至右为 <strong>Bottom-up</strong>、<strong>Top-down</strong>，下半部分为 <strong>Lateral connection</strong>。</p>
<center>
    <img src="/summary/FPN/2 FPN结构.jpg">
    <div>
        图-2 FPN的结构
    </div>
</center>

<h2 id="3-1-Bottom-up"><a href="#3-1-Bottom-up" class="headerlink" title="3.1  Bottom-up"></a>3.1  Bottom-up</h2><p>Bottom-up 是将图片输入到 backbone ConvNet 中提取特征的过程。backbone 输出的 feature map 的尺寸有的是不变的，有的是成 2 倍的减小。对于那些输出的尺寸不变的层，把它们归为一个 stage，每个 stage 的最后一层输出的特征则被抽取出来。以 ResNet 为例，将卷积块 conv2， conv3， conv4， conv5 的输出定义为 $\{C_2,C_3,C_4,C_5 \}$，这些都是每个 stage 中最后一个残差块的输出，这些输出分别是原图的 $\{1/4,1/8,1/16,1/32\}$ 倍。</p>
<h2 id="3-2-Top-down"><a href="#3-2-Top-down" class="headerlink" title="3.2 Top-down"></a>3.2 Top-down</h2><p>Top-down 是将高层得到的 feature map 进行上采样然后往下传递。因为高层的特征包含丰富的语义信息，经过 Top-down 的传播能使这些语义信息传播到低层特征上，使低层特征也包含丰富的语义信息。FPN 中，采样方法是最近邻上采样，使得特征图扩大 2 倍。上采样的目的就是放大图片，在原有图像像素的基础上在像素点之间采用合适的插值算法插入新的像素。</p>
<ul>
<li><p>最近邻上采样：最简单的一种插值方法，在待求像素的四个邻近像素中，将距离待求像素最近的邻近像素值赋给待求像素。设待求像素的坐标为 $(i+u,j+v)$，$i,j$ 为正整数，$u,v$ 为大于 0 小于 1 的小数，则待求像素灰度的值 $f(i+u,j+v)$ 的计算方式如下图所示：</p>
<center>
    <img src="/summary/FPN/3 最近邻插值.jpg">
    <div>
        图-3 最近邻插值
    </div>
</center>

<p>如果 $(i+u,j+v)$ 落在 A 区域内，即 $u&lt;0.5,v&lt;0.5$，则将 a 点的像素值赋给待求像素，其他三种情况同理。最邻近法计算量较小，但可能会造成插值生成的图像灰度上的不连续，在灰度变化的地方可能出现明显的锯齿状。</p>
</li>
</ul>
<h2 id="3-3-Lateral-Connection"><a href="#3-3-Lateral-Connection" class="headerlink" title="3.3  Lateral Connection"></a>3.3  Lateral Connection</h2><p>Lateral connection 主要包括三个步骤：</p>
<center>
    <img src="/summary/FPN/4 Lateral Connection.jpg">
    <div>
        图-4 Lateral Connection
    </div>
</center>

<ul>
<li><p>（1）对于每个 stage 输出的 feature map $C_n$，都先进行 $1\times1$ 的卷积降低维度。</p>
</li>
<li><p>（2）将得到的特征和上一层采样得到特征图 $P_{n+1}$ 进行融合，即直接相加，element-wise addition。因为每个 stage 输出的特征图之间是 2 倍的关系，所以上一层上采样得到的特征图的大小和本层的大小一样。</p>
</li>
<li><p>（3）相加完之后进行 $3\times3$ 的卷积得到本层的特征输出 $P_n$。使用 $3\times3$ 卷积的目的是消除上采样产生的混叠效应，就是指前面提到的<strong>插值生成的图像灰度不连续，在灰度变化的地方可能出现明显的锯齿状</strong>。</p>
</li>
</ul>
<center>
    <img src="/summary/FPN/5 FPN的具体结构.jpg">
    <div>
        图-5 FPN的具体结构
    </div>
</center>

<p>具体来说，$C_5$ 层先经过 $1\times1$ 卷积，得到 $M_5$ 特征。$M_5$ 通过上采样，再融合 $C_4$ 经过 $1\times1$ 卷积后的特征图，得到 $M_4$。这个过程再做两次，分别得到 $M_3$ 和 $M_2$。$M$ 层特征图再经过 $3\times3$ 卷积，得到最终的 $\{ P_2,P_3,P_4,P_5 \}$ 层特征，然后 $P$ 层特征图上分别做分类回归。</p>
<h1 id="4-应用"><a href="#4-应用" class="headerlink" title="4  应用"></a>4  应用</h1><h2 id="4-1-Feature-Pyramid-Network-for-RPN"><a href="#4-1-Feature-Pyramid-Network-for-RPN" class="headerlink" title="4.1  Feature Pyramid Network for RPN"></a>4.1  Feature Pyramid Network for RPN</h2><p>图-6 为 Faster R-CNN 中 RPN 的网络结构，接收单尺度的特征输入，然后经过 $3\times3$ 的卷积，并在 feature map 上的每个点处生成 9 个 anchor（3 个尺寸，每种尺寸对应 3 个宽高比），之后再在两个分支并行的进行 $1\times1$ 卷积，分别用于对 anchor 进行分类和回归。</p>
<center>
    <img src="/summary/FPN/6 RPN网络结构.jpg">
    <div>
        图-6 RPN网络结构
    </div>
</center>

<p>将 FPN 和 RPN 结合起来，RPN 的输入变成多尺度的 feature map，就需要在金字塔的每一层后边都接一个 RPN head（一个 $3\times3$ 卷积，两个 $1\times1$ 卷积），如图-7 所示，其中 $P_6$ 是通过 $P_5$ 下采样得到。</p>
<center>
    <img src="/summary/FPN/7 加入FPN的RPN.jpg">
    <div>
        图-7 在RPN中加入FPN
    </div>
</center>

<p>在生成 anchor 时，因为输入多尺度特征，因此不需要再对每层都使用 3 种不同尺度的 anchor，只为每层设定一种尺寸的 anchor，图中绿色数字就代表每层 anchor 的尺寸，但是每种尺寸仍然对应 3 种宽高比。因此，共有 15 种 anchor。anchor 的 ground truth label 和 Faster R-CNN 中的定义相同，即如果某个 anchor 和 ground-truth box 有最大的 IoU，或者 IoU 大于 0.7，那这个 anchor就是正样本，如果 IoU 小于0.3，那就是负样本。每层 RPN head 的参数是共享的。</p>
<h2 id="4-2-Feature-Pyramid-Network-for-Fast-R-CNN"><a href="#4-2-Feature-Pyramid-Network-for-Fast-R-CNN" class="headerlink" title="4.2  Feature Pyramid Network for Fast R-CNN"></a>4.2  Feature Pyramid Network for Fast R-CNN</h2><p>在 Fast R-CNN 中有一个 RoI Pooling 层，其使用 region proposal 的结果和特征图作为输入，得到每个 proposal 对应的特征后进行池化，之后再分别用于分类结果和边框回归。</p>
<p>之前 Fast R-CNN 使用的是单尺度的特征图，但是现在使用不同尺度的特征图，那么 RoI 需要在哪一个尺度的特征图上提取对应的特征呢？ FPN 的作者认为，不同尺度的 RoI 应该使用不同特征层作为 RoI pooling 层的输入，大尺度 RoI 就用后面一些的金字塔层，比如 $P_5$；小尺度 RoI 就用前面一点的特征层，比如 $P_4$。对于如何判断 RoI 该用哪个层的输出，这里定义了一个计算公式：</p>
<script type="math/tex; mode=display">
k=\lfloor k_0 + \log_2 \frac{\sqrt{wh}}{224} \rfloor</script><p>其中，输出的 $k$ 代表特征图的层数编号。$k_0(=5)$ 是基准值，代表第 5 层特征图 $P_5$。$w$ 和 $h$ 为 RoI 的宽和高，224 是 ImageNet 的标准输入。假设 RoI 的 $w$ 和 $h$ 为 $112\times122$，那么 $k_0 = k - 1$，即为 4，选择第 4 层特征图。</p>
<h2 id="4-3-Faster-R-CNN-with-FPN"><a href="#4-3-Faster-R-CNN-with-FPN" class="headerlink" title="4.3  Faster R-CNN with FPN"></a>4.3  Faster R-CNN with FPN</h2><center>
    <img src="/summary/FPN/8 加入FPN的Faster R-CNN.jpg">
    <div>
        图-8 在Faster R-CNN中加入FPN
    </div>
</center>

<p>和普通的 Faster R-CNN 相同，为每个 RoI 提取特征之后，经过 RoI pooling 层将 RoI 的特征 resize 为相同大小，此处 resize 为 $7\times7$ 的，之后连接两个 1024-d 的 FC layer，然后并行的输入到两个 FC layer 中分别进行分类和回归，得到最终结果。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://zhuanlan.zhihu.com/p/62604038">https://zhuanlan.zhihu.com/p/62604038</a></li>
<li><a href="https://blog.csdn.net/u014380165/article/details/72890275">https://blog.csdn.net/u014380165/article/details/72890275</a></li>
<li><a href="https://www.cnblogs.com/boligongzhu/p/15084655.html">https://www.cnblogs.com/boligongzhu/p/15084655.html</a></li>
<li><a href="https://juejin.cn/post/7043054519881580551">https://juejin.cn/post/7043054519881580551</a></li>
<li><a href="https://blog.csdn.net/qq_36825778/article/details/104328143">https://blog.csdn.net/qq_36825778/article/details/104328143</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/127022724">https://zhuanlan.zhihu.com/p/127022724</a></li>
</ul>
<h1 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h1><ul>
<li><a href="https://github.com/unsky/FPN">https://github.com/unsky/FPN</a></li>
<li><a href="https://github.com/jwyang/fpn.pytorch">https://github.com/jwyang/fpn.pytorch</a></li>
</ul>
]]></content>
      <categories>
        <category>Summary</category>
      </categories>
      <tags>
        <tag>Summary</tag>
        <tag>Deep Learning</tag>
        <tag>CNN</tag>
        <tag>FPN</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/HelloWorld/hello-world/</url>
    <content><![CDATA[<p>This is the first file generated by Hexo blog model.</p>
<span id="more"></span>
<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>Filter Algorithms</title>
    <url>/summary/Filter%20Algorithms/</url>
    <content><![CDATA[<p>本文结合网络资源对常见的几种滤波算法进行了总结整理。</p>
<span id="more"></span>
<center><font size="9"><b>滤波算法</b></font></center>

<hr>
<h1 id="1-限幅滤波法（程序判断滤波法）"><a href="#1-限幅滤波法（程序判断滤波法）" class="headerlink" title="1  限幅滤波法（程序判断滤波法）"></a>1  限幅滤波法（程序判断滤波法）</h1><h2 id="1-1-方法"><a href="#1-1-方法" class="headerlink" title="1.1  方法"></a>1.1  方法</h2><ol>
<li>根据经验判断，确定两次采样允许的最大偏差值（设为 $A$）</li>
<li>每次检测到新值时判断：<br> 如果 $本次值与上次值之差\le A$，则本次值有效；<br> 如果 $本次值与上次值之差&gt;A$，则本次值无效，放弃本次值，用上次值代替本次值。<h2 id="1-2-优点"><a href="#1-2-优点" class="headerlink" title="1.2  优点"></a>1.2  优点</h2></li>
</ol>
<ul>
<li><p>能有效克服因偶然因素引起的脉冲干扰</p>
<h2 id="1-3-缺点"><a href="#1-3-缺点" class="headerlink" title="1.3  缺点"></a>1.3  缺点</h2></li>
<li><p>无法抑制那种周期性的干扰</p>
</li>
<li>平滑度差</li>
</ul>
<h2 id="1-4-代码"><a href="#1-4-代码" class="headerlink" title="1.4  代码"></a>1.4  代码</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">A 值可根据实际情况调整</span></span><br><span class="line"><span class="comment">value 为有效值，new_value 为当前采样值</span></span><br><span class="line"><span class="comment">滤波程序返回有效的实际值</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> A 10</span></span><br><span class="line"><span class="function"><span class="type">char</span> value</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="type">char</span> <span class="title">filter</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">    <span class="type">char</span> new_value;</span><br><span class="line">    new_value = <span class="built_in">get_ad</span>();</span><br><span class="line">    <span class="keyword">if</span>((new_value - value &gt; A) || (value - new_value &gt; A)) &#123;</span><br><span class="line">        <span class="keyword">return</span> value;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> new_value;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="2-中位值滤波法"><a href="#2-中位值滤波法" class="headerlink" title="2  中位值滤波法"></a>2  中位值滤波法</h1><h2 id="2-1-方法"><a href="#2-1-方法" class="headerlink" title="2.1  方法"></a>2.1  方法</h2><ol>
<li>连续采样 $N$ 次（$N$ 取奇数）</li>
<li>把 $N$ 次采样值按大小排列</li>
<li>取中间值为本次有效值<h2 id="2-2-优点"><a href="#2-2-优点" class="headerlink" title="2.2  优点"></a>2.2  优点</h2></li>
</ol>
<ul>
<li>能有效克服因偶然因素引起的波动干扰</li>
<li><p>对温度、液位的变化缓慢的被测参数有良好的滤波效果</p>
<h2 id="2-3-缺点"><a href="#2-3-缺点" class="headerlink" title="2.3  缺点"></a>2.3  缺点</h2></li>
<li><p>对流量、速度等快速变化的参数不宜</p>
<h2 id="2-4-代码"><a href="#2-4-代码" class="headerlink" title="2.4  代码"></a>2.4  代码</h2></li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">N 值可根据实际情况调整</span></span><br><span class="line"><span class="comment">排序采用冒泡法</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> N 11</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">char</span> <span class="title">filter</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">    <span class="type">char</span> value_buf[N];</span><br><span class="line">    <span class="type">char</span> i, j, temp;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; N; i++) &#123;</span><br><span class="line">        value_buf[i] = <span class="built_in">get_ad</span>();</span><br><span class="line">        <span class="built_in">delay</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(j = <span class="number">0</span>; j &lt; N - <span class="number">1</span>; j++) &#123;</span><br><span class="line">        <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; N - j; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span>(value_buf[i] &gt; value_buf[i + <span class="number">1</span>]) &#123;</span><br><span class="line">                temp = value_buf[i];</span><br><span class="line">                value_buf[i] = value_buf[i + <span class="number">1</span>];</span><br><span class="line">                value_buf[i + <span class="number">1</span>] = temp;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> value_buf[(N - <span class="number">1</span>) / <span class="number">2</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="3-算术平均滤波法"><a href="#3-算术平均滤波法" class="headerlink" title="3  算术平均滤波法"></a>3  算术平均滤波法</h1><h2 id="3-1-方法"><a href="#3-1-方法" class="headerlink" title="3.1  方法"></a>3.1  方法</h2><ol>
<li>连续取 $N$ 个采样值进行算术平均运算</li>
<li>$N$ 值较大时：信号平滑度较高，但灵敏度较低</li>
<li>$N$ 值较小时：信号平滑度较低，但灵敏度较高</li>
<li>$N$ 值的选取：一般流量，$N=12$；压力：$N=4$<h2 id="3-2-优点"><a href="#3-2-优点" class="headerlink" title="3.2  优点"></a>3.2  优点</h2></li>
</ol>
<ul>
<li>适用于对一般具有随机干扰的信号进行滤波</li>
<li><p>这样信号的特点是有一个平均值，信号在某一数值范围附近上下波动</p>
<h2 id="3-3-缺点"><a href="#3-3-缺点" class="headerlink" title="3.3  缺点"></a>3.3  缺点</h2></li>
<li><p>对于测量速度较慢或要求数据计算速度较快的实时控制不适用</p>
</li>
<li>比较浪费 RAM<h2 id="3-4-代码"><a href="#3-4-代码" class="headerlink" title="3.4  代码"></a>3.4  代码</h2></li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> N 11</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">char</span> <span class="title">filter</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> sum = <span class="number">0</span>, i = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; N; i++) &#123;</span><br><span class="line">        sum += <span class="built_in">get_ad</span>();</span><br><span class="line">        <span class="built_in">delay</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> (<span class="type">char</span>)(sum / N);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="4-递推平均滤波法"><a href="#4-递推平均滤波法" class="headerlink" title="4  递推平均滤波法"></a>4  递推平均滤波法</h1><h2 id="4-1-方法"><a href="#4-1-方法" class="headerlink" title="4.1  方法"></a>4.1  方法</h2><ol>
<li>把连续取 $N$ 个采样值看成一个队列</li>
<li>队列的长度固定为 $N$</li>
<li>每次采样到一个新数据放入队尾，并扔掉原来队首的一次数据（先进先出原则）</li>
<li>把队列中的 $N$ 个数据进行算术平均运算，就可获得新的滤波结果</li>
<li>$N$ 值的选取：流量，$N=12$；压力：$N=4$；液面，$N=4\sim12$；温度，$N=1\sim4$<h2 id="4-2-优点"><a href="#4-2-优点" class="headerlink" title="4.2  优点"></a>4.2  优点</h2></li>
</ol>
<ul>
<li>对周期性干扰有良好的抑制作用，平滑度高</li>
<li><p>适用于高频振荡的系统    </p>
<h2 id="4-3-缺点"><a href="#4-3-缺点" class="headerlink" title="4.3  缺点"></a>4.3  缺点</h2></li>
<li><p>灵敏度低</p>
</li>
<li>对偶然出现的脉冲性干扰的抑制作用较差</li>
<li>不易消除由于脉冲干扰所引起的采样值偏差</li>
<li>不适用于脉冲干扰比较严重的场合</li>
<li>比较浪费 RAM<h2 id="4-4-代码"><a href="#4-4-代码" class="headerlink" title="4.4  代码"></a>4.4  代码</h2></li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> N 12</span></span><br><span class="line"></span><br><span class="line"><span class="type">char</span> value_buf[N];</span><br><span class="line"><span class="type">char</span> i = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">char</span> <span class="title">filter</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">    <span class="type">char</span> count = <span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> sum = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    value_buf[i++] = <span class="built_in">get_ad</span>();</span><br><span class="line">    <span class="keyword">if</span>(i == N) &#123;</span><br><span class="line">        i = <span class="number">0</span>;    <span class="comment">//先进先出</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(count = <span class="number">0</span>; count &lt; N; count++) &#123;</span><br><span class="line">        sum += value_buf[count];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> (<span class="type">char</span>)(sum / N);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="5-中位值平均滤波法（防脉冲干扰平均滤波法）"><a href="#5-中位值平均滤波法（防脉冲干扰平均滤波法）" class="headerlink" title="5  中位值平均滤波法（防脉冲干扰平均滤波法）"></a>5  中位值平均滤波法（防脉冲干扰平均滤波法）</h1><h2 id="5-1-方法"><a href="#5-1-方法" class="headerlink" title="5.1  方法"></a>5.1  方法</h2><ol>
<li>相当于“中位值滤波法”+“算术平均滤波法”</li>
<li>连续采样 $N$ 个数据，去掉一个最大值和一个最小值</li>
<li>然后计算 $N-2$ 个数据的算术平均值</li>
<li>$N$ 值的选取：$3\sim14$<h2 id="5-2-优点"><a href="#5-2-优点" class="headerlink" title="5.2  优点"></a>5.2  优点</h2></li>
</ol>
<ul>
<li>融合了两种滤波法的优点</li>
<li><p>对于偶然出现的脉冲性干扰，可消除由于脉冲干扰所引起的采样值偏差</p>
<h2 id="5-3-缺点"><a href="#5-3-缺点" class="headerlink" title="5.3  缺点"></a>5.3  缺点</h2></li>
<li><p>测量速度较慢，和算术平均滤波法一样</p>
</li>
<li>比较浪费 RAM<h2 id="5-4-代码"><a href="#5-4-代码" class="headerlink" title="5.4  代码"></a>5.4  代码</h2></li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> N 12</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">char</span> <span class="title">filter</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">    <span class="type">char</span> i = <span class="number">0</span>, j = <span class="number">0</span>, temp = <span class="number">0</span>;</span><br><span class="line">    <span class="type">char</span> value_buf[N];</span><br><span class="line">    <span class="type">int</span> sum = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; N; i++)&#123;</span><br><span class="line">        value_buf[i] = <span class="built_in">get_ad</span>();</span><br><span class="line">        <span class="built_in">delay</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(j = <span class="number">0</span>; j &lt; N - <span class="number">1</span>; j++) &#123;</span><br><span class="line">        <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; N - j; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span>(value_buf[i] &gt; value_buf[i + <span class="number">1</span>]) &#123;</span><br><span class="line">                temp = value_buf[i];</span><br><span class="line">                value_buf[i] = value_buf[i + <span class="number">1</span>];</span><br><span class="line">                value_buf[i + <span class="number">1</span>] = temp;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">1</span>; i &lt; N - <span class="number">1</span>; i++) &#123;</span><br><span class="line">        sum += value_buf[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> (<span class="type">char</span>)(sum / (N - <span class="number">2</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="6-限幅平均滤波法"><a href="#6-限幅平均滤波法" class="headerlink" title="6  限幅平均滤波法"></a>6  限幅平均滤波法</h1><h2 id="6-1-方法"><a href="#6-1-方法" class="headerlink" title="6.1  方法"></a>6.1  方法</h2><ol>
<li>相当于“限幅滤波法”+“递推平均滤波法”</li>
<li>每次采样到的新数据先进行限幅处理</li>
<li>再送入队列进行递推平均滤波处理<h2 id="6-2-优点"><a href="#6-2-优点" class="headerlink" title="6.2  优点"></a>6.2  优点</h2></li>
</ol>
<ul>
<li>融合了两种滤波法的优点</li>
<li><p>对于偶然出现的脉冲性干扰，可消除由于脉冲干扰所引起的采样值偏差</p>
<h2 id="6-3-缺点"><a href="#6-3-缺点" class="headerlink" title="6.3  缺点"></a>6.3  缺点</h2></li>
<li><p>比较浪费 RAM</p>
<h2 id="6-4-代码"><a href="#6-4-代码" class="headerlink" title="6.4  代码"></a>6.4  代码</h2></li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">结合程序 1 和 4</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure>
<h1 id="7-一阶滞后滤波法"><a href="#7-一阶滞后滤波法" class="headerlink" title="7  一阶滞后滤波法"></a>7  一阶滞后滤波法</h1><h2 id="7-1-方法："><a href="#7-1-方法：" class="headerlink" title="7.1  方法："></a>7.1  方法：</h2><ol>
<li>取 $a=[0,1]$</li>
<li>$本次滤波结果=(1-a)<em>本次采样值+a</em>上次滤波结果$<h2 id="7-2-优点"><a href="#7-2-优点" class="headerlink" title="7.2  优点"></a>7.2  优点</h2></li>
</ol>
<ul>
<li>对周期性干扰具有良好的抑制作用</li>
<li><p>适用于波动频率较高的场合</p>
<h2 id="7-3-缺点"><a href="#7-3-缺点" class="headerlink" title="7.3  缺点"></a>7.3  缺点</h2></li>
<li><p>相位滞后，灵敏度低</p>
</li>
<li>滞后程度取决于 $a$ 值大小</li>
<li>不能消除滤波频率高于采样频率的 $1/2$ 的干扰信号<h2 id="7-4-代码"><a href="#7-4-代码" class="headerlink" title="7.4  代码"></a>7.4  代码</h2></li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">为加块程序处理速度，假定基数 a = 0 ~ 100</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="type">char</span> value;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">char</span> <span class="title">filter</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">    <span class="type">char</span> new_value = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    new_value = <span class="built_in">get_ad</span>();</span><br><span class="line">    <span class="keyword">return</span> (a * value + (<span class="number">100</span> - a) * new_value) / <span class="number">100</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="8-加权递推平均滤波法"><a href="#8-加权递推平均滤波法" class="headerlink" title="8  加权递推平均滤波法"></a>8  加权递推平均滤波法</h1><h2 id="8-1-方法"><a href="#8-1-方法" class="headerlink" title="8.1  方法"></a>8.1  方法</h2><ul>
<li>是对递推平均滤波法的改进，即不同时刻的数据加以不同的权</li>
<li>通常是，越接近现时刻的数据，权取得越大</li>
<li><p>给予新采样值的权系数越大，则灵敏度越高，但信号平滑度越低</p>
<h2 id="8-2-优点"><a href="#8-2-优点" class="headerlink" title="8.2  优点"></a>8.2  优点</h2></li>
<li><p>适用于有较大纯滞后时间常数的对象</p>
</li>
<li><p>和采样周期较短的系统</p>
<h2 id="8-3-缺点"><a href="#8-3-缺点" class="headerlink" title="8.3  缺点"></a>8.3  缺点</h2></li>
<li><p>对于纯滞后时间常数较小，采样周期较长，变化缓慢的信号</p>
</li>
<li>不能迅速反应系统当前所受干扰的严重程度，滤波效果差<h2 id="8-4-代码"><a href="#8-4-代码" class="headerlink" title="8.4  代码"></a>8.4  代码</h2></li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">coe数组为加权系数表</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> N 12</span></span><br><span class="line"></span><br><span class="line"><span class="type">char</span> coe[N] = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>&#125;;</span><br><span class="line"><span class="type">char</span> sum_coe = <span class="number">1</span> + <span class="number">2</span> + <span class="number">3</span> + <span class="number">4</span> + <span class="number">5</span> + <span class="number">6</span> + <span class="number">7</span> + <span class="number">8</span> + <span class="number">9</span> + <span class="number">10</span> + <span class="number">11</span> + <span class="number">12</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">char</span> <span class="title">filter</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">    <span class="type">char</span> i = <span class="number">0</span>;</span><br><span class="line">    <span class="type">char</span> value_buf[N];</span><br><span class="line">    <span class="type">int</span> sum = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; N; i++) &#123;</span><br><span class="line">        value_buf[i] = <span class="built_in">get_ad</span>();</span><br><span class="line">        <span class="built_in">delay</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; N; i++) &#123;</span><br><span class="line">        sum += value_buf[i] * coe[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> (<span class="type">char</span>)(sum / sum_coe);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="9-消抖滤波法"><a href="#9-消抖滤波法" class="headerlink" title="9  消抖滤波法"></a>9  消抖滤波法</h1><h2 id="9-1-方法"><a href="#9-1-方法" class="headerlink" title="9.1  方法"></a>9.1  方法</h2><ol>
<li>设置一个滤波计数器</li>
<li>将每次采样值与当前有效值比较：<br>如果 $采样值＝当前有效值$，则计数器清零<br>如果 $采样值\ne当前有效值$，则 $计数器+1$，并判断计数器是否 $\ge$ 上限 $N$（溢出）</li>
<li>如果计数器溢出，则将本次值替换当前有效值，并清计数器<h2 id="9-2-优点"><a href="#9-2-优点" class="headerlink" title="9.2  优点"></a>9.2  优点</h2></li>
</ol>
<ul>
<li>对于变化缓慢的被测参数有较好的滤波效果</li>
<li><p>可避免在临界值附近控制器的反复开/关跳动或显示器上数值抖动</p>
<h2 id="9-3-缺点"><a href="#9-3-缺点" class="headerlink" title="9.3  缺点"></a>9.3  缺点</h2></li>
<li><p>对于快速变化的参数不宜</p>
</li>
<li>如果在计数器溢出的那一次采样到的值恰好是干扰值,则会将干扰值当作有效值导入系统<h2 id="9-4-代码"><a href="#9-4-代码" class="headerlink" title="9.4  代码"></a>9.4  代码</h2></li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> N 12</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">char</span> <span class="title">filter</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">    <span class="type">char</span> i = <span class="number">0</span>;</span><br><span class="line">    <span class="type">char</span> new_value = <span class="number">0</span>, value = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    new_value = <span class="built_in">get_ad</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span>(value != new_value) &#123;</span><br><span class="line">        i++;</span><br><span class="line">        <span class="keyword">if</span>(i &gt; N) &#123;</span><br><span class="line">            <span class="keyword">return</span> new_value;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">delay</span>();</span><br><span class="line">        new_value = <span class="built_in">get_ad</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> value;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="10-限幅消抖滤波法"><a href="#10-限幅消抖滤波法" class="headerlink" title="10  限幅消抖滤波法"></a>10  限幅消抖滤波法</h1><h2 id="10-1-方法"><a href="#10-1-方法" class="headerlink" title="10.1  方法"></a>10.1  方法</h2><ul>
<li>相当于“限幅滤波法”+“消抖滤波法”</li>
<li><p>先限幅，后消抖</p>
<h2 id="10-2-优点"><a href="#10-2-优点" class="headerlink" title="10.2  优点"></a>10.2  优点</h2></li>
<li><p>继承了“限幅”和“消抖”的优点</p>
</li>
<li><p>改进了“消抖滤波法”中的某些缺陷，避免将干扰值导入系统</p>
<h2 id="10-3-缺点"><a href="#10-3-缺点" class="headerlink" title="10.3  缺点"></a>10.3  缺点</h2></li>
<li><p>对于快速变化的参数不宜</p>
<h2 id="10-4-代码"><a href="#10-4-代码" class="headerlink" title="10.4  代码"></a>10.4  代码</h2></li>
</ul>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">参考程序 1 和 9</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure>
<h1 id="11-滑动窗口滤波法"><a href="#11-滑动窗口滤波法" class="headerlink" title="11  滑动窗口滤波法"></a>11  滑动窗口滤波法</h1><h2 id="11-1-方法一"><a href="#11-1-方法一" class="headerlink" title="11.1  方法一"></a>11.1  方法一</h2><ul>
<li>前提先要获得一组数据，大小排序去除明显无效的数据，然后指定一个宽度为4的滑块从做向右滑动，计算滑块最右端和最左端的差值，该差值应小于预设阈值（否则丢弃），找出差值最小的4个数据然后求平均输出。缺点：如果源数据由于（距离）某种原因，数据产生的速度变化较大。很长时间才来一个数据就不太合适</li>
<li>主控制器读取数据，并将数据进行从小到大排序，并删除溢出值，设置宽度为 ４ 的滑动窗口，从排序好的数据最前端，自左向右进行滑动，并记录窗口尾部与头部的差值，保留差值最小的数据簇。此时，该组数据即为这组测量过程中，最集中的一组数据。将该差值与预先设定好的阈值进行比较，当满足阈值规定，则认为该组数据有效，若不满足，即表明该组数据离散程度太高，无法保证数据的有效性。设定滑动窗口大小为 ４，如果窗口大小过小，则无法保证数据是否处于有效值范围内，窗口大小过大，增大了对原始数据的要求，特别是当测量环境不佳的情况下，有效值本身的数量较少，过大的窗口限制了测量的适用性。阈值设定的作用是防止数据过于离散，甚至可能出现所有数据都非有效值的情况，阈值设定过大，则容易导致误差较大，阈值设定过小，限制了测量的量程。窗口大小以及阈值大小的设定，通过实验经验设置，取得了较好的测量效果。该算法在常见的平均取值法上，增加了滑动窗口的概念，以及阈值比较的想法，与平均取值法相比，有了更坚实的理论基础。</li>
</ul>
<h2 id="11-2-方法二"><a href="#11-2-方法二" class="headerlink" title="11.2  方法二"></a>11.2  方法二</h2><ul>
<li>维护一个奇数长度（5）的窗口，根据预测和众数（数据重复的次数或者误差在一定范围内出现的次数）的方法来决定输出那个数据，次数小于一半的不输出，即 5 个数据中最少有 2 个相同的才输出该值（或这三个值的平均值），不同的值一定要留下，5 个元素中离新值差异最大的值丢弃。</li>
</ul>
<h1 id="12-卡尔曼滤波法"><a href="#12-卡尔曼滤波法" class="headerlink" title="12  卡尔曼滤波法"></a>12  卡尔曼滤波法</h1><h2 id="12-1-状态估计"><a href="#12-1-状态估计" class="headerlink" title="12.1  状态估计"></a>12.1  状态估计</h2><p>首先，对于一个我们关心的物理量，我们假设它符合下面的规律</p>
<script type="math/tex; mode=display">
x_k=ax_{k-1}</script><p>其中，$x_k$ 为该物理量本周期的实际值，$x_{k-1}$ 为该物理量上一个周期的实际值，当然这个物理量可能不符合这个规律，这里只是做了一个假设。下面我们再来看一下这个物理量的测量公式：</p>
<script type="math/tex; mode=display">
z_k=v_k+x_k</script><p>其中，$z_k$ 是这个物理量的测量值，$v_k$ 是测量噪声。这个公式体现的是实际值与测量值的关系。实际中，物理量一般不会像我们上面的公式那样简单，一般我们用下面的公式来表示：</p>
<script type="math/tex; mode=display">
x_k=ax_{k-1}+bu_{k}</script><p>其中，$bu_k$ 代表了处理噪声，这个噪声是处理模型与实际情况的差异，比如车速，他会受到人为加速、减速、路面不平等外界因素的影响。</p>
<p>卡尔曼滤波的基本思想是综合利用上一次的状态和测量值来对物理量的状态进行预测估计。我们用 $\hat{x}_k$ 来表示 $x_k$ 的估计值，则有下面的公式：</p>
<script type="math/tex; mode=display">
\hat{x}_k=\hat{x}_{k-1}+g_k(z_k-\hat{x}_{k-1})</script><p>在这个公式中，综合利用了上一个周期的估计值和这个周期的测量值来对进行估计。其中，$g_k$ 叫做卡尔曼增益，这个公式与一阶滞后滤波很相似，只不过卡尔曼增益是会变的，每个周期都会更新，一阶滤波的系数则是固定值。考虑极端的情况来分析增益的作用，当  $g_k=0$ 时，增益为 $0$，这时，这表示我们这个周期的估计值与上个周期是相同的，不信任当前的测量值；当 $g_k=1$ 时，增益为 $1$，这 $z_k=\hat{x}_{k-1}$ 时，这表示我们这个周期的估计值与测量值是相同的，不信任上个周期的估计值。在实际应用时，$g_k$ 介于 $0\sim1$ 之间，它代表了对测量值的信任程度。</p>
<h2 id="12-2-卡尔曼增益"><a href="#12-2-卡尔曼增益" class="headerlink" title="12.2  卡尔曼增益"></a>12.2  卡尔曼增益</h2><p>我们通过下面两个公式来计算并在每个周期进行卡尔曼增益的迭代更新：</p>
<script type="math/tex; mode=display">
g_k=p_{k-1}/(p_{k-1}+r)\\
p_k=(1-g_k)p_{k-1}</script><p>在上述公式中，$r$ 是测量噪声 $v_k$ 的平均值，测量噪声是符合高斯分布的，一般可以从传感器厂商那里获得测量噪声的均值，如果无法获得可以根据采集到的数据给出一个经验值。$r$ 的大小对最终滤波效果的影响是比较大的。$p_k$ 为本周期的预测误差。我们采用分析卡尔曼增益的方法来分析预测误差的作用，即采用假设极端情况的方法。假设前一次的预测误差 $p_{k-1}=0$，根据第一个公式则 $g_k=0$，根据上面的分析，这种情况估计值为上个周期的估计值；如果前一次的预测误差 $p_{k-1}=1$，则增益变为 $g_k=1/(1+r)$，$r$ 一般取值很小，所以 $g_k\approx1$，这种情况以新测量的值作为估计值。</p>
<p>对于第二个公式，当卡尔曼增益为 $0$ 时，$p_k=p_{k-1}$，即采用上一个周期的预测误差；当增益为1时，$p_k=0$。</p>
<h2 id="12-3-卡尔曼滤波算法"><a href="#12-3-卡尔曼滤波算法" class="headerlink" title="12.3  卡尔曼滤波算法"></a>12.3  卡尔曼滤波算法</h2><p>有了上面的推导，我们在下面列出来完成卡尔曼滤波的公式，卡尔曼滤波分为预测过程和更新过程两个过程，在公式中，我们又引入了缩放系数 $h$，和协方差 $q$。</p>
<ul>
<li>预测过程</li>
</ul>
<script type="math/tex; mode=display">
\hat{x}_k=a\hat{x}_{k-1}+bu_k\\
p_k=ap_{k-1}a+q</script><ul>
<li>更新过程</li>
</ul>
<script type="math/tex; mode=display">
g_k=p_kh/(hp_kh+r)\\
\hat{x}_k=\hat{x}_{k-1}+g_k(z_k-h\hat{x}_{k-1})\\
p_k=(1-g_kh)p_k</script><p>上面的公式适合一维变量的卡尔曼滤波，将变量扩展到多维，用向量和矩阵替换上面的变量，就可以实现多维变量的卡尔曼滤波，下面的公式适用于多维变量：</p>
<ul>
<li>预测过程</li>
</ul>
<script type="math/tex; mode=display">
\hat{x}_k=A\hat{x}_{k-1}+Bu_k\\
P_k=AP_{k-1}A^T+Q</script><ul>
<li>更新过程</li>
</ul>
<script type="math/tex; mode=display">
G_k=P_kH(HP_kH+R)^{-1}\\
\hat{x}_k=\hat{x}_{k-1}+G_k(z_k-H\hat{x}_{k-1})\\
P_k=(1-G_kH)P_k</script><h2 id="12-4-代码"><a href="#12-4-代码" class="headerlink" title="12.4  代码"></a>12.4  代码</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span>&#123;</span><br><span class="line">    <span class="type">float</span> filterValue; <span class="comment">//滤波后的值</span></span><br><span class="line">    <span class="type">float</span> kalmanGain; <span class="comment">//Kalamn增益</span></span><br><span class="line">    <span class="type">float</span> A; <span class="comment">//状态矩阵</span></span><br><span class="line">    <span class="type">float</span> H; <span class="comment">//观测矩阵</span></span><br><span class="line">    <span class="type">float</span> Q; <span class="comment">//状态矩阵的方差</span></span><br><span class="line">    <span class="type">float</span> R; <span class="comment">//观测矩阵的方差</span></span><br><span class="line">    <span class="type">float</span> P; <span class="comment">//预测误差</span></span><br><span class="line">    <span class="type">float</span> B;</span><br><span class="line">    <span class="type">float</span> u;</span><br><span class="line">&#125;KalmanInfo;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Kalm::initKalmanFilter</span><span class="params">(KalmanInfo *info)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    info-&gt;A = <span class="number">1</span>;</span><br><span class="line">    info-&gt;H = <span class="number">1</span>;</span><br><span class="line">    info-&gt;P = <span class="number">0.1</span>;</span><br><span class="line">    info-&gt;Q = <span class="number">0.05</span>;</span><br><span class="line">    info-&gt;R = <span class="number">0.1</span>;</span><br><span class="line">    info-&gt;B = <span class="number">0.1</span>;</span><br><span class="line">    info-&gt;u = <span class="number">0</span>;</span><br><span class="line">    info-&gt;filterValue = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">float</span> <span class="title">Kalm::kalmanFilterFun</span><span class="params">(KalmanInfo *info, <span class="type">float</span> new_value)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">float</span> predictValue = info-&gt;A*info-&gt;filterValue+info-&gt;B*info-&gt;u; <span class="comment">//计算预测值</span></span><br><span class="line">    info-&gt;P = info-&gt;A*info-&gt;A*info-&gt;P + info-&gt;Q; <span class="comment">//求协方差</span></span><br><span class="line">    info-&gt;kalmanGain = info-&gt;P * info-&gt;H /(info-&gt;P * info-&gt;H * info-&gt;H + info-&gt;R); <span class="comment">//计算卡尔曼增益</span></span><br><span class="line">    info-&gt;filterValue = predictValue + (new_value - predictValue)*info-&gt;kalmanGain; <span class="comment">//计算输出的值</span></span><br><span class="line">    info-&gt;P = (<span class="number">1</span> - info-&gt;kalmanGain* info-&gt;H)*info-&gt;P; <span class="comment">//更新协方差</span></span><br><span class="line">    <span class="keyword">return</span> info-&gt;filterValue;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Summary</category>
      </categories>
      <tags>
        <tag>Summary</tag>
        <tag>Knowledge</tag>
        <tag>Filter Algorithms</tag>
        <tag>Algorithms</tag>
      </tags>
  </entry>
  <entry>
    <title>Generalized-ICP (GICP)</title>
    <url>/papernote/Generalized-ICP%20(GICP)/</url>
    <content><![CDATA[<p>关于论文 Generalized-ICP (Robotics: Science and Systems, 2009) 的阅读总结。</p>
<span id="more"></span>
<p><img src="/papernote/Generalized-ICP%20(GICP)/Generalized-ICP (GICP" alt>/幻灯片1.png)<br><img src="/papernote/Generalized-ICP%20(GICP)/Generalized-ICP (GICP" alt>/幻灯片2.png)<br><img src="/papernote/Generalized-ICP%20(GICP)/Generalized-ICP (GICP" alt>/幻灯片3.png)<br><img src="/papernote/Generalized-ICP%20(GICP)/Generalized-ICP (GICP" alt>/幻灯片4.png)<br><img src="/papernote/Generalized-ICP%20(GICP)/Generalized-ICP (GICP" alt>/幻灯片5.png)<br><img src="/papernote/Generalized-ICP%20(GICP)/Generalized-ICP (GICP" alt>/幻灯片6.png)<br><img src="/papernote/Generalized-ICP%20(GICP)/Generalized-ICP (GICP" alt>/幻灯片7.png)<br><img src="/papernote/Generalized-ICP%20(GICP)/Generalized-ICP (GICP" alt>/幻灯片8.png)<br><img src="/papernote/Generalized-ICP%20(GICP)/Generalized-ICP (GICP" alt>/幻灯片9.png)<br><img src="/papernote/Generalized-ICP%20(GICP)/Generalized-ICP (GICP" alt>/幻灯片10.png)<br><img src="/papernote/Generalized-ICP%20(GICP)/Generalized-ICP (GICP" alt>/幻灯片11.png)<br><img src="/papernote/Generalized-ICP%20(GICP)/Generalized-ICP (GICP" alt>/幻灯片12.png)<br><img src="/papernote/Generalized-ICP%20(GICP)/Generalized-ICP (GICP" alt>/幻灯片13.png)<br><img src="/papernote/Generalized-ICP%20(GICP)/Generalized-ICP (GICP" alt>/幻灯片14.png)<br><img src="/papernote/Generalized-ICP%20(GICP)/Generalized-ICP (GICP" alt>/幻灯片15.png)<br><img src="/papernote/Generalized-ICP%20(GICP)/Generalized-ICP (GICP" alt>/幻灯片16.png)<br><img src="/papernote/Generalized-ICP%20(GICP)/Generalized-ICP (GICP" alt>/幻灯片17.png)</p>
]]></content>
      <categories>
        <category>Paper Note</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Paper Note</tag>
        <tag>ICP</tag>
      </tags>
  </entry>
  <entry>
    <title>Attention Mechanism</title>
    <url>/summary/Attention%20Mechanism/</url>
    <content><![CDATA[<p>注意力机制在 Transformer 中很常见，目前也经常被用于计算机视觉当中。本文整理总结了网络上关于注意力机制的介绍。</p>
<span id="more"></span>
<center><font size="9"><b>注意力机制</b></font></center>

<hr>
<h1 id="1-Encoder-Decoder-框架"><a href="#1-Encoder-Decoder-框架" class="headerlink" title="1  Encoder-Decoder 框架"></a>1  Encoder-Decoder 框架</h1><p>目前大多数注意力模型附着在 Encoder-Decoder 框架下。下图是 NLP 处理中常用的 Encoder-Decoder 框架：</p>
<center>
    <img src="/summary/Attention%20Mechanism/1 Encoder-Decoder框架.png" weight="600" height="300">
    <div>
        图-1 Encoder-Decoder 框架
    </div>
</center>

<p>可以简单理解为处理一个语句（篇章）生成另一个语句（篇章）的模型：对于句子 $<Source, target>$，目标是输入句子 $Source$，期待通过 Encoder-Decoder 框架生成目标句子 $Target$，$Source$ 和 $Target$ 的单词序列为：</Source,></p>
<script type="math/tex; mode=display">
Source=<x_1,x_2,...,x_m>\\
Target=<y_1,y_2,...,y_n></script><p>编码器 Encoder 通过对句子 $Source$ 进行编码，将输入的句子通过非线性变换转化为中间语义编码</p>
<script type="math/tex; mode=display">
C=\mathcal{F}(x_1,x_2,...,x_m)</script><p>解码器 Decoder 将中间语义编码 $C$ 和之前已经产生的历史输出单词序列 $y_1,y_2,…,y_{i-1}$ 来生成第 $i$ 步的单词</p>
<script type="math/tex; mode=display">
y_i=\mathcal{G}(C,y_1,y_2,...,y_{i-1})</script><p>若 $Source$ 和 $Target$ 是不同语言的句子，那么是机器翻译 Encoder-Decoder 框架；若 $Source$ 是一篇文章，而 $Target$ 是文章的概括性描述，那么是文本摘要 Encoder-Decoder 框架；若 $Source$ 是一句问句，而 $Target$ 是问句的回答，那么是问答系统或对话机器人 Encoder-Decoder 框架。</p>
<p>除 NLP 领域外，Encoder-Decoder 框架在其他领域也被广泛应用，如语音识别领域，Encoder 的输入是语音流，而 Decoder 的输出为语音所对应的文本；在 CV 领域，Encoder 的输入是一帧图片，Decoder 的输出是描述图片语义特征的描述语。通常 NLP 和语音识别时 Encoder 采用 RNN，而 CV 则采用 CNN。</p>
<h1 id="2-经典-Attention-模型（Soft-Attention）"><a href="#2-经典-Attention-模型（Soft-Attention）" class="headerlink" title="2  经典 Attention 模型（Soft-Attention）"></a>2  经典 Attention 模型（Soft-Attention）</h1><h2 id="2-1-Attention-的引入"><a href="#2-1-Attention-的引入" class="headerlink" title="2.1  Attention 的引入"></a>2.1  Attention 的引入</h2><p>图-1 中的 Encoder-Decoder 框架没有体现出 “注意力机制”，因此可以将其看作注意力不集中的分心模型。为什么称其注意力不集中？先观察下面 $Target$ 中每个单词的生成过程：</p>
<script type="math/tex; mode=display">
\begin{aligned}
y_1 &= \mathcal{G}(C) \\
y_2 &= \mathcal{G}(C, y_1) \\
y_3 &= \mathcal{G}(C, y_1, y_2) \\
...
\end{aligned}</script><p>式中 $\mathcal{G}$ 为 Decoder 的非线性变化函数。从上面的生成过程可以看出，不论生成哪一个单词，所使用的输入句子 $Source$ 的语义编码 $C$ 是相同的。需要注意的是 $C$ 由 $Sourc$ 的每个单词经 Encoder 编码生成，这也意味着，对于每一个 $Target$ 中的单词 $y_i$ 来说，输入句子 $Source$ 中的每一个单词对其生成都有着相同的影响力，因此称这个模型注意力不集中。</p>
<p><b><font face="楷体" color="#800000" size="4"><br>例：对于机器翻译，输入句子 “Tom chase Jerry”，Encoder-Decoder 逐步生成中文单词：“汤姆”、“追逐”、“杰瑞”。在翻译 “杰瑞” 时，分心模型里所有输入英文单词对目标单词 “杰瑞” 的影响是相同的，这显然不合理，因为 “Jerry” 翻译为 “杰瑞” 更加合理，而分心模型无法体现这一点。
</font></b></p>
<p>不引入注意力机制，在输入短句子时可能没有明显的问题，但是对于长难句来说，输入句子的语义信息仅通过一个中间语义编码向量来表示，单词本身的信息大量丢失，极大影响输出结果的正确性，因此，这就是引入注意力机制的重要原因。</p>
<p>再用上面的翻译例子，引入 Attention 后，在翻译 “杰瑞” 时，将体现出不同英文单词对于翻译当前中文单词不同的影响程度，例如如下的概率分布：</p>
<script type="math/tex; mode=display">
(\mathrm{Tom}, 0.3),\ (\mathrm{Chase}, 0.2),\ (\mathrm{Jerry}, 0.5)</script><p>每个英文单词的概率表示翻译 “杰瑞” 时，注意力分配给不同英文单词的注意力大小，这引入了新的信息，将有助于正确翻译目标单词。</p>
<p>同理，$Target$ 中的每个单词都应学到其对应的 $Source$ 中单词的注意力概率分布。这也就意味着，在生成单词 $y_i$ 时，前面所使用的相同的中间语义编码 $C$ 被替换为根据当前生成单词而不断变化的 $C_i$ ，如下图所示：</p>
<center>
    <img src="/summary/Attention%20Mechanism/2 注意力Encoder-Decoder框架.png">
    <br>
    <div>
        图-2 引入注意力机制的 Encoder-Decoder 框架
    </div>
</center>

<p>即生成 $Target$ 的过程变为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
y_1 &= \mathcal{G}_1 (C_1) \\
y_2 &= \mathcal{G}_2 (C_2, y_1) \\
y_3 &= \mathcal{G}_3 (C_3, y_1, y_2)
\end{aligned}</script><p>每个 $C_i$ 可能对应不同的 $Source$ 中单词的注意力概率分布，以前面的翻译例子，其对应的信息可能如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
C_{\mathrm{汤姆}} &= \mathcal{g}(0.6*\mathcal{f}(\mathrm{Tom}),\ 0.2*\mathcal{f}(\mathrm{Chase}),\ 0.2*\mathcal{f}(\mathrm{Jerry})) \\
C_{\mathrm{追逐}} &= \mathcal{g}(0.2*\mathcal{f}(\mathrm{Tom}),\ 0.7*\mathcal{f}(\mathrm{Chase}),\ 0.1*\mathcal{f}(\mathrm{Jerry})) \\
C_{\mathrm{杰瑞}} &= \mathcal{g}(0.3*\mathcal{f}(\mathrm{Tom}),\ 0.2*\mathcal{f}(\mathrm{Chase}),\ 0.5*\mathcal{f}(\mathrm{Jerry}))
\end{aligned}</script><p>其中，$\mathcal{f}$ 表示 Encoder 对输入英文单词的某种变换函数，例如：如果 Encoder 使用 RNN 模型，那么 $\mathcal{f}$ 的函数结果往往是某时刻输入 $x_i$ 后隐藏节点的状态值；$\mathcal{g}$ 表示 Encoder 根据单词的中间编码合成整个句子中间语义编码的变换函数，一般，$\mathcal{g}$ 函数是对构成元素的加权求和，即下式：</p>
<script type="math/tex; mode=display">
C_i = \sum_{j=1}^{L_x} a_{ij}h_j</script><p>其中：$L_x$ 表示句子 $Source$ 的长度；$a_{ij}$ 表示在 $Target$ 输出第 $i$ 个单词时 $Source$ 句子中第 $j$ 个单词的注意力系数分配；$h_j$ 表示 $Source$ 句子中第 $j$ 个单词的语义编码。以前面的例子为例，则有 $L_x = 3$，$h_1 = \mathcal{f}(\mathrm{Tom})$，$h_2 = \mathcal{f}(\mathrm{Chase})$，$h_3 = \mathcal{f}(\mathrm{Jerry})$ 分别是 $Source$ 中每个单词的语义编码，对于编码 $C_1$ 而言，权重为 $a_{11} = 0.6$，$a_{12}=0.2$，$a_{13}=0.2$，其形成过程如下图：</p>
<center>
    <img src="/summary/Attention%20Mechanism/3 Attention在翻译时的形成过程示例.png" weight="400" height="400">
    <br>
    <div>
        图-3 Attention 在翻译时的形成过程示意图
    </div>
</center>

<p>这个过程还存在一个问题，即在形成 Attention 时，如何得到 $Source$ 单词的注意力概率分布，例如翻译 “杰瑞” 时如何得到概率分布 $(\mathrm{Tom}, 0.3)$，$(\mathrm{Chase}, 0.2)$，$(\mathrm{Jerry}, 0.5)$？</p>
<h2 id="2-2-Attention-中注意力分布的生成"><a href="#2-2-Attention-中注意力分布的生成" class="headerlink" title="2.2  Attention 中注意力分布的生成"></a>2.2  Attention 中注意力分布的生成</h2><p>先对图-1 中非 Attention 的 Encoder-Decoder 框架细化，Encoder 和 Decoder 采用 RNN 模型，得到下面的框架图：</p>
<center>
    <img src="/summary/Attention%20Mechanism/4 使用RNN细化后的Encoder-Decoder框架.png">
    <br>
    <div>
        图-4 使用 RNN 细化后的 Encoder-Decoder 框架
    </div>
</center>

<p>用下图说明注意力概率分布的通用计算过程：</p>
<center>
    <img src="/summary/Attention%20Mechanism/5 注意力概率分布的计算过程.png" height="400" width="600">
    <br>
    <div>
        图-5 注意力概率分布的计算过程
    </div>
</center>

<p>对于使用 RNN 的 Decoder 而言，在时刻 $i$ 若要生成单词 $y_i$，由于可以得知时刻 $i-1$ 的隐藏层节点 $H_{i-1}$ 的值，那么可以把 $H_{i-1}$ 和 $Source$ 中每个单词对应的隐藏层节点 $h_j$ 分别计算，即通过函数 $F(h_j,H_{i-1})$ 获得目标单词 $y_i$ 和每个输入单词的对齐可能性（$F$ 在不同模型中使用的方法不同），然后函数 $F$ 的输出经过 Softmax 进行归一化后，即得到符合概率分布的注意力概率分布。图-6 中可视化的是英语-德语翻译系统加入 Attention 机制后，$Source$ 和 $Target$ 中单词对应的概率分布：</p>
<center>
    <img src="/summary/Attention%20Mechanism/6 英语-德语翻译中Attention的概率分布.png" height="400" width="400">
    <br>
    <div>
        图-6 英语-德语翻译中 Attention 的概率分布
    </div>
</center>



<h2 id="2-3-Attention-机制的本质"><a href="#2-3-Attention-机制的本质" class="headerlink" title="2.3  Attention 机制的本质"></a>2.3  Attention 机制的本质</h2><p>将 Attention 机制从 Encoder-Decoder 框架中剥离，并进一步抽象，可以更简单地看出 Attention 机制的本质。</p>
<center>
    <img src="/summary/Attention%20Mechanism/7 Attention机制的本质思想.png">
    <br>
    <div>
        图-7 Attention 机制的本质思想
    </div>
</center>

<p>以图-7 为例：将 $Source$ 中的构成元素抽象为一些列的键值对数据 $<key, value>$，此时，对于给定 $Target$ 中的某个元素 $query$，通过计算 $query$ 和各个 $key$ 的相似性或相关性，得到每个 $key$ 对应 $value$ 的权重系数，然后对 $value$ 进行加权求和，即得到最终的 Attention 数值。因此本质上，Attention 机制是对 $Source$ 中元素的 $value$ 值进行加权求和，而 $query$ 和 $key$ 用来计算对应 $value$ 的权重系数。即可以将其本质思想改写为如下公式：</key,></p>
<script type="math/tex; mode=display">
\mathrm{Attention}(query, Source) = \sum_{i=1}^{L_x} \mathrm{Similarity}(query, key_i) * value_i</script><p>其中，$L_x=||Source||$。在前面机器翻译的例子里计算 Attention 的过程中，$Source$ 中的 $key$ 和 $value$ 指向的则是同一个数据，即输入句子中单词的语义编码。</p>
<p>从图-7 可以引出另外一种理解，将 Attention 机制看作一种软寻址（Soft Addressing）：$Source$ 可以看作存储器内存储的内容，元素由地址 $key$ 和值 $value$ 组成，当前有个 $key=query$ 的查询，目的是取出存储器中对应的 $value$ 值，即 Attention 数值。通过 $query$ 和存储器内元素的地址 $key$ 进行相似性比较寻址。之所以成为<strong>软寻址</strong>，指的不同一般寻址从存储中找出取出一条数据，而是可能从每个 $key$ 地址都会取出内容，取出内容的重要性根据 $query$ 和 $key$ 的相似性决定，之后对 $value$ 进行加权求和，从而得到最终的 $value$ 值，即 Attention 值。</p>
<p>对于 Attention 机制的具体计算过程，对目前大多数方法进行抽象，可以归纳为两个过程三个阶段：</p>
<ul>
<li><p>过程 1：根据 $query$ 和 $key$ 计算权重系数</p>
<ul>
<li>阶段①：根据 $query$ 和 $key$ 计算两者的相似性或相关性</li>
<li>阶段②：对阶段①的原始分值进行归一化处理</li>
</ul>
</li>
<li><p>过程 2（阶段③）：根据权重系数对 $value$ 进行加权求和</p>
</li>
</ul>
<p>上面的过程可以得到图-8 中的结果：</p>
<center>
    <img src="/summary/Attention%20Mechanism/8 计算Attention的三个阶段.png">
    <br>
    <div>
        图-8 计算 Attention 的三个阶段
    </div>
</center>

<p>在阶段①，可以使用不同的函数和计算方法，根据 $query(=q)$ 和 $key_i(=k_i)$，计算两者的相似性或相关性，常见的方法如下（式中 $W$、$U$ 和 $v$ 为可学习的参数矩阵或向量，$D$ 为输入向量的维度）：</p>
<ul>
<li><p>点积</p>
<script type="math/tex; mode=display">
s_i = \mathrm{Similarity}(query, key_i) = query \cdot key_i = q^T k_i</script></li>
<li><p>缩放点积</p>
<script type="math/tex; mode=display">
s_i = \mathrm{Similarity}(query, key_i) = \frac{q^T k_i}{\sqrt{D}}</script></li>
<li><p>余弦（cosine）相似性</p>
<script type="math/tex; mode=display">
s_i = \mathrm{Similarity}(query, key_i) = \frac{query \cdot key_i}{||query|| \cdot ||key_i||}</script></li>
<li><p>MLP 网络：$s_i = \mathrm{Similarity}(query, key_i) = \mathrm{MLP}(query, key_i)$</p>
<ul>
<li>加性模型：$s_i = \mathrm{Similarity}(query, key_i) = v^T \tanh (Wk_i+Uq)$</li>
<li>双线性模型：$s_i = \mathrm{Similarity}(query, key_i) = k_i^T W q$，可以重塑为 $s_i = \mathrm{Similarity}(query, key_i) = k_i^T (U^T V) q = (U k_i)^T (Vq)$，即对 $query$ 和 $key$ 进行线性变换后，再计算点积。</li>
</ul>
</li>
</ul>
<p>阶段②引入类似 Softmax 的计算，对阶段①的结果一方面进行归一化，另一方面也突出重要元素的权重：</p>
<script type="math/tex; mode=display">
a_i = \mathrm{Softmax}(s_i) = \frac{\exp(s_i)}{\sum_{j=1}^{L_x}\exp(s_j)}</script><p>然后加权得到 Attention 值：</p>
<script type="math/tex; mode=display">
\mathrm{Attention}(query, Source) = \sum_{i=1}^{L_x} a_i \cdot value_i</script><p>目前大多数的注意力机制都符合上述的三阶段的计算过程。</p>
<h1 id="3-Attention-注意力机制的变体"><a href="#3-Attention-注意力机制的变体" class="headerlink" title="3  Attention 注意力机制的变体"></a>3  Attention 注意力机制的变体</h1><h2 id="3-1-硬性注意力（Hard-Attention）"><a href="#3-1-硬性注意力（Hard-Attention）" class="headerlink" title="3.1  硬性注意力（Hard Attention）"></a>3.1  硬性注意力（Hard Attention）</h2><p>软性注意力通过注意力分布加权求和来融合输入向量。而<strong>硬性注意力</strong>则不采用这种方式，它根据注意力分布选择输入向量中的一个作为输出：</p>
<script type="math/tex; mode=display">
\mathrm{Attention}(query, Source) = value_{i*} \\
\mathrm{where}\ i* = \mathrm{ArgCondition}(a_i\ \mathrm{or}\ s_i) \\
i*\ 即满足\ \mathrm{Condition}\ 条件的\ a_i\ 或\ s_i\ 的索引</script><p>这种选择有两种选择方式：</p>
<ul>
<li>选择注意力分布中，分数（概率）最大的一项所对应的输入向量作为 Attention 的输出。</li>
<li>根据注意力分布进行随机采样，采样结果作为 Attention 的输出。</li>
</ul>
<p>硬性注意力通过上面的方式选择 Attention 的输出，会使最终的损失函数与注意力分布之间的函数关系不可导，从而无法使用反向传播算法训练模型，硬性注意力通常需要使用强化学习来进行训练。一般深度学习算法会使用软性注意力的方式进行计算。</p>
<h2 id="3-2-多头注意力（Multi-Head-Attention）"><a href="#3-2-多头注意力（Multi-Head-Attention）" class="headerlink" title="3.2  多头注意力（Multi-Head Attention）"></a>3.2  多头注意力（Multi-Head Attention）</h2><p>多头注意力机制是利用多个查询向量 $Q = [q_1, q_2, … , q_m]$，并行地从输入信息 $<key, value>$ 或 $(K,V)=[(k_1,v_1),(k_2,v_2),…,(k_n,v_n)]$ 中选取多组信息。在查询过程中，每个查询向量 $q_i$ 将会关注输入信息的不同部分。</key,></p>
<p>假设 $a_{ij}$ 表示第 $i$ 个查询向量 $q_i$ 与第 $j$ 个输入信息 $k_j$ 的注意力权重，$s(\cdot)=\mathrm{Similarity}(\cdot)$，$context_i$ 表示由查询向量 $q_i$ 计算得出的 Attention 输出向量，其计算方式为：</p>
<script type="math/tex; mode=display">
a_{ij} = \mathrm{Softmax}(s(q_i,k_j)) = \frac{\exp(s(q_i,k_j))}{\sum_{t=1}^n \exp(s(q_i,k_t))} \\
context_i = \sum_{j=1}^n a_{ij} \cdot v_j</script><p>最终将所有查询向量的计算结果进行拼接得到最终结果（$\oplus$ 表示向量拼接操作）：</p>
<script type="math/tex; mode=display">
\mathrm{Attention} = context_1 \oplus context_2 \oplus \cdots \oplus context_m</script><h1 id="4-自注意力机制（Self-Attention）"><a href="#4-自注意力机制（Self-Attention）" class="headerlink" title="4  自注意力机制（Self-Attention）"></a>4  自注意力机制（Self-Attention）</h1><p>Self-Attention 也被称为 Intra-Attention（内部 Attention）。在一般 Encoder-Decoder 框架中，输入 $Source$ 和输出 $Target$ 内容是不相同的，Attention 机制发生在 $Target$ 的元素 $query$ 和 $Source$ 中的所有元素之间。Self-Attention 不是 $Target$ 和 $Source$ 之间的 Attention 机制，而是 $Source$ 内部元素之间或者 $Target$ 内部元素之间发生的 Attention 机制，也可以理解为 $Target=Source$ 这种特殊情况下的注意力计算机制。</p>
<p>在前面所介绍的 Attention 机制中，会使用一个查询向量 $q$ 和对应的输入 $H=[h_1,h_2,…,h_n]$ 进行计算，查询向量 $q$ 则和任务相关，例如 Encoder-Decoder 框架中，$q$ 可以是 Decoder 端前一时刻的<strong>输出状态向量</strong>。而<strong>在 Self-Attention 中，查询向量也可以使用输入信息生成</strong>，而非任务相关的向量。即模型读到输入信息后，根据输入信息本身决定当前的重要信息。</p>
<p>自注意力机制往往采用 <strong>Query-Key-Value</strong> 的模式，以 BERT 重点自注意力机制为例，展开下面的讨论，如图-9 所示：</p>
<center>
    <img src="/summary/Attention%20Mechanism/9 自注意力机制的计算过程.jpg" width="640" height="400">
    <br>
    <div>
        图-9 自注意力机制的计算过程
    </div>
</center>

<p>图-9 中，输入信息 $H=[h_1,h_2]$，蓝色矩阵中每行表示一个输入向量，三个矩阵 $W_q$，$W_k$，$W_v$ 将输入信息 $H$ 以此转换到对应的查询空间 $Q=[q_1,q_2]$，键空间 $K=[k_1,k_2]$ 和值空间 $V=[v_1,v_2]$：</p>
<script type="math/tex; mode=display">
\begin{aligned}
Q &= [q_1=h_1 W_q,\ q_2=h_2 W_q] &\Rightarrow Q = HW_q \\
K &= [k_1=h_1 W_k,\ k_2=h_2 W_k] &\Rightarrow K = HW_k \\
V &= [v_1=h_1 W_v,\ v_2=h_2 W_v] &\Rightarrow V = HW_v
\end{aligned}</script><p>不妨以 $h_1$ 为例计算这个位置的 Attention 输出向量 $context_1$，如图-10 所示，其中 $D_k$ 表示 $key$ 向量的维度（$query$ 向量、$key$ 向量和 $value$ 向量的维度相同）：</p>
<center>
    <img src="/summary/Attention%20Mechanism/10 自注意力机制的详细计算过程.jpg" width="600" height="600">
    <br>
    <div>
        图-10 自注意力机制的详细计算过程
    </div>
</center>

<p>在获得输入信息 $H$ 在不同空间的表达 $Q$、$K$ 和 $V$ 后，计算 $q_1$ 在 $h_1$ 和 $h_2$ 的分数（相似性或相关性） $s_{11}$ 和 $s_{12}$。然后用 Softmax 进行归一化，获得在 $h_1$ 这个位置的注意力分布 $a_{11}$ 和 $a_{12}$，代表了模型当前在 $h_1$ 这个位置需要对输入信息 $h_1$ 和 $h_2$ 的关注程度。最后，根据该位置的注意力分布对 $v_1$ 和 $v_2$ 进行加权平均获得 $h_1$ 位置的 Attention 向量 $context_1$。</p>
<p>同理，对于输入信息 $H=[h_1,h_2,…,h_n]$，可以得到每个位置的 Attention 向量  $\mathrm{Attention}=[context_1,context_2,…,context_n]$。</p>
<p>整个 Self-Attention 的计算过程的矩阵形式为：</p>
<script type="math/tex; mode=display">
\mathrm{Attention} = \mathrm{Softmax} (\frac{QK^T}{\sqrt{D_k}})V</script><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://www.cnblogs.com/jins-note/p/13056604.html">https://www.cnblogs.com/jins-note/p/13056604.html</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/393940472">https://zhuanlan.zhihu.com/p/393940472</a></li>
</ul>
]]></content>
      <categories>
        <category>Summary</category>
      </categories>
      <tags>
        <tag>Summary</tag>
        <tag>Deep Learning</tag>
        <tag>Knowledge</tag>
      </tags>
  </entry>
  <entry>
    <title>Bag-of-Words Model (BoW)</title>
    <url>/summary/Bag-of-Words%20Model%20(BoW)/</url>
    <content><![CDATA[<p>BoW 是 NLP 中的使用较多的模型，其用于检测单词相似度。在 CV 领域，BoW 用于 SLAM 的回环检测，通过提取图像特征作为图像单词，生成词汇树和词汇数据库，从而加速回环检测过程中对历史帧的提取匹配。</p>
<span id="more"></span>
<center><b><font size="9">BoW in CV</font></b></center>

<h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1  介绍"></a>1  介绍</h1><p>Bag-of-Words model (BoW model) 最早出现在自然语言处理和信息检索领域：</p>
<ul>
<li>该模型忽略掉文本的语法和语序等要素，将其仅仅看作是若干个词汇的集合，文档中每个单词的出现都是独立的。</li>
<li>BoW 使用一组无序的单词来表达一段文字或一个文档。</li>
<li>近年来，BoW 模型被广泛应用于计算机视觉中。</li>
</ul>
<h1 id="2-基于文本的-BoW-模型-——-举例"><a href="#2-基于文本的-BoW-模型-——-举例" class="headerlink" title="2  基于文本的 BoW 模型 —— 举例"></a>2  基于文本的 BoW 模型 —— 举例</h1><p>给定两个简单的文本：</p>
<ul>
<li>John likes to watch movies.  Mary likes too.</li>
<li>John also likes to watch football games.</li>
</ul>
<p>基于两个文本中的单词，可以构建如下的词典：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;John&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;likes&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;to&quot;</span><span class="punctuation">:</span> <span class="number">3</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;watch&quot;</span><span class="punctuation">:</span> <span class="number">4</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;movies&quot;</span><span class="punctuation">:</span> <span class="number">5</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;also&quot;</span><span class="punctuation">:</span> <span class="number">6</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;football&quot;</span><span class="punctuation">:</span> <span class="number">7</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;games&quot;</span><span class="punctuation">:</span> <span class="number">8</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;Mary&quot;</span><span class="punctuation">:</span> <span class="number">9</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;too&quot;</span><span class="punctuation">:</span> <span class="number">10</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>词典中包含 10 个单词，每个单词有唯一索引，那么每个文本可以使用一个 10 维向量来表示：</p>
<script type="math/tex; mode=display">
[1, 2, 1, 1, 1, 0, 0, 0, 1, 1] \\
[1, 1, 1, 1, 0, 1, 1, 1, 0, 0]</script><p>该向量与原来文本中单词出现的顺序没有关系，而是词典中每个单词在文本中出现的频率。上述向量也可以用频率分布直方图来表示。</p>
<h1 id="3-BoW-模型应用于图像检索"><a href="#3-BoW-模型应用于图像检索" class="headerlink" title="3  BoW 模型应用于图像检索"></a>3  BoW 模型应用于图像检索</h1><p>为了表示一副图像，可以将图像看作文档，即由若干个“视觉单词”构成的集合，同理，视觉单词之间没有顺序。</p>
<h2 id="3-1-特称提取"><a href="#3-1-特称提取" class="headerlink" title="3.1  特称提取"></a>3.1  特称提取</h2><p>由于视觉词汇不是文本中的单词，因此需要从图像中提取出相互独立的视觉单词，即提取出特征描述子：</p>
<ul>
<li>SIFT 是提取图像中局部不变特征应用较广的算法，因此采用 SIFT 进行特征提取。</li>
<li>将每幅图像提取出的描述子保存在一个文件中，构建视觉词典。</li>
</ul>
<h2 id="3-2-学习“视觉词典（Visual-Vocabulary）”"><a href="#3-2-学习“视觉词典（Visual-Vocabulary）”" class="headerlink" title="3.2  学习“视觉词典（Visual Vocabulary）”"></a>3.2  学习“视觉词典（Visual Vocabulary）”</h2><p>使用聚类算法实现视觉词典，常用的聚类算法有 K-means：</p>
<ol>
<li><p>随机初始化 $k$ 个聚类中心；</p>
</li>
<li><p>对于每个特征，根据与聚类中心的距离，赋值给某个聚类中心/类别，距离的计算可以采用欧式距离：</p>
<script type="math/tex; mode=display">
 D(X, M) = \sum_{\text{cluster } k} \sum_{\text{point } i \text{ in cluster } k} (x_i - m_k)^2</script></li>
<li><p>对于每个类别，根据得到的特征集重新计算聚类中心；</p>
</li>
<li><p>重复 2，3 步，直到收敛。</p>
</li>
</ol>
<h2 id="3-3-用视觉词典量化输入特征集"><a href="#3-3-用视觉词典量化输入特征集" class="headerlink" title="3.3  用视觉词典量化输入特征集"></a>3.3  用视觉词典量化输入特征集</h2><p>对于输入特征，量化的过程是将该特征映射到距离其最接近的视觉单词（前面的 $k$ 个聚类结果），并实现计数 。</p>
<p>选择合适的视觉词典规模是需要考虑的问题：</p>
<ul>
<li>若规模太少，会出现视觉单词无法覆盖所有可能出现的情况 。</li>
<li>若规模太多，又会计算量大，容易过拟合 。</li>
<li>只能通过不断的测试，才能找到最合适的词典规模。</li>
</ul>
<h2 id="3-4-输入图像转换为频率分布直方图"><a href="#3-4-输入图像转换为频率分布直方图" class="headerlink" title="3.4  输入图像转换为频率分布直方图"></a>3.4  输入图像转换为频率分布直方图</h2><p>利用 SIFT 算法，可以从每幅图像中提取多个特征点，通过统计每个视觉单词在词典中出现的次数，可以获得直方图。</p>
<h2 id="3-5-通过倒排表快速索引图像"><a href="#3-5-通过倒排表快速索引图像" class="headerlink" title="3.5  通过倒排表快速索引图像"></a>3.5  通过倒排表快速索引图像</h2><p>用 K 近邻算法进行图像检索。给定输入图像的 BoW 直方图，在数据库中查找 $k$ 个最近林的图像。对于图像分类问题，可以根据这 $k$ 个近邻图像的分类标签，获得分类结果。</p>
<h2 id="3-6-根据索引结果进行直方图匹配"><a href="#3-6-根据索引结果进行直方图匹配" class="headerlink" title="3.6  根据索引结果进行直方图匹配"></a>3.6  根据索引结果进行直方图匹配</h2><p>利用建立起来的索引找到包含特定单词的所有图像。为了获得包含多个单词的候选图像，有两种解决方法：</p>
<ol>
<li>可以在每个单词上进行遍历，得到包含该单词的所有图像列表，然后合并这些列表。在合并后的列表中，对每一个图像 ID 出现的次数进行跟踪排序，排在列表最前面的是最好的匹配图像。</li>
<li>如果不遍历所有的单词，可以根据其倒排序文档频率权重进行排序，并使用那些权重最高的单词，在这些单词上进行遍历，减少计算量。</li>
</ol>
<h1 id="4-图像检索-——-代码"><a href="#4-图像检索-——-代码" class="headerlink" title="4  图像检索 —— 代码"></a>4  图像检索 —— 代码</h1><h2 id="4-1-设置"><a href="#4-1-设置" class="headerlink" title="4.1  设置"></a>4.1  设置</h2><ul>
<li>每张图片生成相应的 .sift 文件、视觉单词，以建立 BoW 模型。</li>
<li>设着图像集大小为 100，词典包含 1000 个单词，K-means 类别数为 10。</li>
</ul>
<h2 id="4-2-建立词典"><a href="#4-2-建立词典" class="headerlink" title="4.2  建立词典"></a>4.2  建立词典</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">from</span> PCV.imagesearch <span class="keyword">import</span> vocabulary</span><br><span class="line"><span class="keyword">from</span> PCV.tools.imtools <span class="keyword">import</span> get_imlist</span><br><span class="line"><span class="keyword">from</span> PCV.localdescriptors <span class="keyword">import</span> sift</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取图像列表</span></span><br><span class="line">imlist = get_imlist(<span class="string">&#x27;./dataset/&#x27;</span>)</span><br><span class="line">nbr_images = <span class="built_in">len</span>(imlist)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取特征列表</span></span><br><span class="line">featlist = [imlist[i][:-<span class="number">3</span>] + <span class="string">&#x27;sift&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nbr_images)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取文件夹下图像的sift特征</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nbr_images):</span><br><span class="line">    sift.process_image(imlist[i], featlist[i])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成词汇</span></span><br><span class="line">voc = vocabulary.Vocabulary(<span class="string">&#x27;ukbenchtest&#x27;</span>)</span><br><span class="line">voc.train(featlist, <span class="number">100</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存词汇</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./dataset/vocabulary.pkl&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    pickle.dump(voc, f)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;vocabulary is:&#x27;</span>, voc.name, voc.nbr_words)</span><br></pre></td></tr></table></figure>
<h2 id="4-3-建立图像索引"><a href="#4-3-建立图像索引" class="headerlink" title="4.3  建立图像索引"></a>4.3  建立图像索引</h2><ul>
<li>创建表，索引和索引器 indexer 类，以便将图像数据写入数据库，将上面得到的数据模型存放数据库 testImaAdd.db 中。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">from</span> PCV.imagesearch <span class="keyword">import</span> imagesearch</span><br><span class="line"><span class="keyword">from</span> PCV.localdescriptors <span class="keyword">import</span> sift</span><br><span class="line"><span class="keyword">import</span> sqlite3</span><br><span class="line"><span class="keyword">from</span> PCV.tools.imtools <span class="keyword">import</span> get_imlist</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取图像列表</span></span><br><span class="line">imlist = get_imlist(<span class="string">&#x27;./dataset/&#x27;</span>)</span><br><span class="line">nbr_images = <span class="built_in">len</span>(imlist)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取特征列表</span></span><br><span class="line">featlist = [imlist[i][:-<span class="number">3</span>] + <span class="string">&#x27;sift&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nbr_images)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 载入词汇</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./dataset/vocabulary.pkl&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    voc = pickle.load(f)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建索引</span></span><br><span class="line">indx = imagesearch.Indexer(<span class="string">&#x27;testImaAdd.db&#x27;</span>, voc)</span><br><span class="line">indx.create_tables()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历所有的图像，并将它们的特征投影到词汇上</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nbr_images)[:<span class="number">100</span>]:</span><br><span class="line">    locs, descr = sift.read_features_from_file(featlist[i])</span><br><span class="line">    indx.add_to_index(imlist[i], descr)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提交到数据库</span></span><br><span class="line">indx.db_commit()</span><br><span class="line">con = sqlite3.connect(<span class="string">&#x27;testImaAdd.db&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(con.execute(<span class="string">&#x27;select count (filename) from imlist&#x27;</span>).fetchone())</span><br><span class="line"><span class="built_in">print</span>(con.execute(<span class="string">&#x27;select * from imlist&#x27;</span>).fetchone())</span><br></pre></td></tr></table></figure>
<h2 id="4-4-图像索引测试"><a href="#4-4-图像索引测试" class="headerlink" title="4.4  图像索引测试"></a>4.4  图像索引测试</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">from</span> PCV.localdescriptors <span class="keyword">import</span> sift</span><br><span class="line"><span class="keyword">from</span> PCV.imagesearch <span class="keyword">import</span> imagesearch</span><br><span class="line"><span class="keyword">from</span> PCV.geometry <span class="keyword">import</span> homography</span><br><span class="line"><span class="keyword">from</span> PCV.tools.imtools <span class="keyword">import</span> get_imlist</span><br><span class="line"></span><br><span class="line"><span class="comment"># 载入图像列表</span></span><br><span class="line">imlist = get_imlist(<span class="string">&#x27;./dataset/first1000/&#x27;</span>)</span><br><span class="line">nbr_images = <span class="built_in">len</span>(imlist)</span><br><span class="line"><span class="comment"># 载入特征列表</span></span><br><span class="line">featlist = [imlist[i][:-<span class="number">3</span> ] +<span class="string">&#x27;sift&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nbr_images)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 载入词汇</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./dataset/vocabulary.pkl&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    voc = pickle.load(f)</span><br><span class="line">src = imagesearch.Searcher(<span class="string">&#x27;testImaAdd.db&#x27;</span> ,voc)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询图像索引和查询返回的图像数</span></span><br><span class="line">q_ind = <span class="number">0</span></span><br><span class="line">nbr_results = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 常规查询(按欧式距离对结果排序)</span></span><br><span class="line">res_reg = [w[<span class="number">1</span>] <span class="keyword">for</span> w <span class="keyword">in</span> src.query(imlist[q_ind])[:nbr_results]]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;top matches (regular):&#x27;</span>, res_reg)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 载入查询图像特征</span></span><br><span class="line">q_locs ,q_descr = sift.read_features_from_file(featlist[q_ind])</span><br><span class="line">fp = homography.make_homog(q_locs[:, :<span class="number">2</span>].T)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用单应性进行拟合建立RANSAC模型</span></span><br><span class="line">model = homography.RansacModel()</span><br><span class="line"></span><br><span class="line">rank = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 载入候选图像的特征</span></span><br><span class="line"><span class="keyword">for</span> ndx <span class="keyword">in</span> res_reg[<span class="number">1</span>:]:</span><br><span class="line">    locs ,descr = sift.read_features_from_file(featlist[ndx])</span><br><span class="line">    matches = sift.<span class="keyword">match</span>(q_descr ,descr)</span><br><span class="line">    ind = matches.nonzero()[<span class="number">0</span>]</span><br><span class="line">    ind2 = matches[ind]</span><br><span class="line">    tp = homography.make_homog(locs[: ,:<span class="number">2</span>].T)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute homography, count inliers. if not enough matches return empty list</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        H ,inliers = homography.H_from_ransac(fp[: ,ind] ,tp[: ,ind2] ,model ,match_theshold=<span class="number">4</span>)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        inliers = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># store inlier count</span></span><br><span class="line">    rank[ndx] = <span class="built_in">len</span>(inliers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sort dictionary to get the most inliers first</span></span><br><span class="line">sorted_rank = <span class="built_in">sorted</span>(rank.items(), key=<span class="keyword">lambda</span> t: t[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">res_geom = [res_reg[<span class="number">0</span>] ] +[s[<span class="number">0</span>] <span class="keyword">for</span> s <span class="keyword">in</span> sorted_rank]</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;top matches (homography):&#x27;</span>, res_geom)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示查询结果</span></span><br><span class="line">imagesearch.plot_results(src ,res_reg[:<span class="number">11</span>])  <span class="comment"># 常规查询</span></span><br><span class="line">imagesearch.plot_results(src ,res_geom[:<span class="number">11</span>])  <span class="comment"># 重排后的结果</span></span><br></pre></td></tr></table></figure>
<h1 id="5-DBoW3"><a href="#5-DBoW3" class="headerlink" title="5  DBoW3"></a>5  DBoW3</h1><h2 id="5-1-简介"><a href="#5-1-简介" class="headerlink" title="5.1  简介"></a>5.1  简介</h2><ul>
<li><p>DBoW3 是一个开源的 C++ 词袋模型库，可以将图像转化成视觉词袋表示。它采用层级树状结构将相近的图像特征在物理存储上聚集在一起，创建一个视觉词典。</p>
</li>
<li><p>DBoW3 还生成一个图像数据库，带有顺序索引和逆序索引，可以使图像特征的检索和对比非常快。</p>
</li>
<li><p>DBoW3 是 DBoW2 的增强版，仅依赖 OpenCV，能够很方便的使用。</p>
</li>
<li><p>DBoW3 两个比较重要的类是 <code>Vocabulary</code> 和 <code>Database</code>：</p>
<ul>
<li><code>Vocabulary</code> 表示图像库的视觉词汇表，并可以将任一的图像转换为 BoW 表示；</li>
<li><code>Database</code>是一个图像数据库，能够方便的对图像进行检索。</li>
</ul>
</li>
</ul>
<h2 id="5-2-安装-DBoW3"><a href="#5-2-安装-DBoW3" class="headerlink" title="5.2  安装 DBoW3"></a>5.2  安装 DBoW3</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/rmsalinas/DBow3.git <span class="comment"># 下载源文件</span></span><br><span class="line"><span class="built_in">cd</span> DBoW3 					<span class="comment"># 进入源文件夹</span></span><br><span class="line"><span class="built_in">mkdir</span> build 				<span class="comment"># 新建一个编译目标文件夹</span></span><br><span class="line"><span class="built_in">cd</span> build 					<span class="comment"># 将build作为make工作路径</span></span><br><span class="line">cmake -DUSE_CONTRIB=ON .. 	<span class="comment"># 编译上一级目录，如果提前安装好了contrib_modules，则使用cmake选项-DUSE_CONTRIB=ON使能SURF，否则直接运行cmake ..</span></span><br><span class="line">make -j4 					<span class="comment"># 取决于您的电脑的线程数量</span></span><br><span class="line">sudo make install 			<span class="comment"># 安装DBoW3</span></span><br></pre></td></tr></table></figure>
<h2 id="5-3-Vocabulary-构建"><a href="#5-3-Vocabulary-构建" class="headerlink" title="5.3  Vocabulary 构建"></a>5.3  Vocabulary 构建</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">vocabulary</span><span class="params">(<span class="type">const</span> vector&lt;cv::Mat&gt; &amp;features, <span class="type">const</span> string &amp;file_path, <span class="type">int</span> k = <span class="number">9</span>, <span class="type">int</span> l = <span class="number">3</span>)</span></span>&#123;</span><br><span class="line">    <span class="comment">// Branching factor and depth levels</span></span><br><span class="line">    <span class="type">const</span> DBoW3::WeightingType weight = DBoW3::TF_IDF;</span><br><span class="line">    <span class="type">const</span> DBoW3::ScoringType score = DBoW3::L2_NORM;</span><br><span class="line"></span><br><span class="line">    <span class="function">DBoW3::Vocabulary <span class="title">voc</span><span class="params">(k, l, weight, score)</span></span>;</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;Creating a small &quot;</span> &lt;&lt; k &lt;&lt; <span class="string">&quot;^&quot;</span> &lt;&lt; l &lt;&lt; <span class="string">&quot; vocabulary...&quot;</span> &lt;&lt; endl;</span><br><span class="line">    voc.<span class="built_in">create</span>(features);</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;...done!&quot;</span> &lt;&lt; endl;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//cout &lt;&lt; &quot;Vocabulary infomation: &quot; &lt;&lt; endl &lt;&lt; voc &lt;&lt; endl &lt;&lt; endl;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// save the vocabulary to disk</span></span><br><span class="line">    cout &lt;&lt; endl &lt;&lt; <span class="string">&quot;Saving vocabulary...&quot;</span> &lt;&lt; endl;</span><br><span class="line">    stringstream ss;</span><br><span class="line">    ss &lt;&lt; file_path &lt;&lt; <span class="string">&quot;/small_voc.yml.gz&quot;</span>;</span><br><span class="line">    voc.<span class="built_in">save</span>(ss.<span class="built_in">str</span>());</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;Done&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="5-4-构建-Database"><a href="#5-4-构建-Database" class="headerlink" title="5.4  构建 Database"></a>5.4  构建 Database</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">database</span><span class="params">(<span class="type">const</span> vector&lt;cv::Mat&gt; &amp;features,<span class="type">const</span> string &amp;file_path)</span></span>&#123;</span><br><span class="line">    <span class="comment">// load the vocabulary from disk</span></span><br><span class="line">    stringstream ss ;</span><br><span class="line">    ss &lt;&lt; file_path &lt;&lt; <span class="string">&quot;/small_voc.yml.gz&quot;</span>;</span><br><span class="line">    <span class="function">DBoW3::Vocabulary <span class="title">voc</span><span class="params">(ss.str())</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">DBoW3::Database <span class="title">db</span><span class="params">(voc, <span class="literal">false</span>, <span class="number">0</span>)</span></span>; <span class="comment">// false = do not use direct index</span></span><br><span class="line">    <span class="comment">// add images to the database</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; features.<span class="built_in">size</span>(); i++)</span><br><span class="line">        db.<span class="built_in">add</span>(features[i]);</span><br><span class="line"></span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;... done!&quot;</span> &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;Database information: &quot;</span> &lt;&lt; endl &lt;&lt; db &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// we can save the database. The created file includes the vocabulary</span></span><br><span class="line">    <span class="comment">// and the entries added</span></span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;Saving database...&quot;</span> &lt;&lt; endl;</span><br><span class="line">    db.<span class="built_in">save</span>(<span class="string">&quot;small_db.yml.gz&quot;</span>);</span><br><span class="line">    cout &lt;&lt; <span class="string">&quot;... done!&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="5-5-调用-query-查找图片"><a href="#5-5-调用-query-查找图片" class="headerlink" title="5.5  调用 query 查找图片"></a>5.5  调用 query 查找图片</h2><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//auto fdetector=cv::xfeatures2d::SURF::create(400, 4, 2);</span></span><br><span class="line"><span class="keyword">auto</span> fdetector = xfeatures2d::SIFT::<span class="built_in">create</span>(<span class="number">0</span>, <span class="number">3</span>, <span class="number">0.2</span>, <span class="number">10</span>);</span><br><span class="line">vector&lt;cv::KeyPoint&gt; kpts;</span><br><span class="line">cv::Mat des;</span><br><span class="line">fdetector-&gt;<span class="built_in">detectAndCompute</span>(img, <span class="built_in">noArray</span>(), kpts, des);</span><br><span class="line"></span><br><span class="line">db.<span class="built_in">query</span>(des, ql, max_resuts);</span><br></pre></td></tr></table></figure>
<h1 id="6-回环检测中的-BoW"><a href="#6-回环检测中的-BoW" class="headerlink" title="6  回环检测中的 BoW"></a>6  回环检测中的 BoW</h1><h2 id="6-1-视觉词袋"><a href="#6-1-视觉词袋" class="headerlink" title="6.1  视觉词袋"></a>6.1  视觉词袋</h2><p>对描述子聚类后，可以用一个 $k$ 维向量表示该图。构建视觉词袋的步骤是：</p>
<ol>
<li>对图像集合检测特征，形成特征描述子；</li>
<li>对描述子进行聚类来形成词典，用层次 K-means 来形成词汇树（Vocabulary Tree），每个节点下的描述子相近，可以认为同一个节点下的这些描述子是一个视觉单词。如一个三层、每层 4 个聚类的词汇树，可以形成 4 维的词向量，也可以是 16 维。</li>
<li>对图像集中的每张图像，提取其对应特征描述子进行分类，然后得到分别属于 4 个类别的描述子个数，即得到一个 4 维向量，归一化该向量，即将该图像表示成一个 4 维向量。</li>
<li>将每张图像都表示为上述 4 维向量，根据向量相似度就可以知道两张图像之间的相似度。</li>
</ol>
<p>举例：</p>
<ul>
<li>看书的时候不认识那么多字，买一本词典，一遍翻词典，一遍看书；在后面的步骤中，需要产生一个数据库，用来记录查词典时所用到的单词，方便回看时能快速找到。</li>
</ul>
<p>在实际使用的过程中，第一步需要训练产生一本词典，如上例，单词就是一些特征聚类的结果。可能有几十万甚至几百万张训练的图片，需要依次提取特征，然后在特征空间中就会有多个特征信息，接下来要用这些特征聚类产生单词，聚类的数量取决于选择的树形结构。</p>
<h2 id="6-2-词典产生流程图"><a href="#6-2-词典产生流程图" class="headerlink" title="6.2  词典产生流程图"></a>6.2  词典产生流程图</h2><center>
    <img src="/summary/Bag-of-Words%20Model%20(BoW)/Fig-1.png">
</center>

<ol>
<li>所有特征描述子分布在特征空间；</li>
<li>根据设定好的树形结构参数，进行 Kmean++ 聚类，产生叶子结点；</li>
<li>对产生的节点依次进行 Kmean++ 聚类，直至产生根节点。</li>
</ol>
<h2 id="6-3-视觉词向量数据库"><a href="#6-3-视觉词向量数据库" class="headerlink" title="6.3  视觉词向量数据库"></a>6.3  视觉词向量数据库</h2><p>一般提前训练好词典，设计成单词树，每个树节点包含一个逆序索引信息包，每帧图像包含一个顺序索引信息包。</p>
<p>对新进来的图像帧，将其中每个特征点都从词汇树根节点往下遍历，取汉明距离最小的节点接着往下遍历直到叶节点。最终，计算各个叶节点上的数目并形成图像表达向量 $\bold{v}$，同时得到图像的顺序索引，以便用于之后的回环检测。</p>
<center>
    <img src="/summary/Bag-of-Words%20Model%20(BoW)/Fig-2.png">
</center>

<p><strong>逆序索引</strong>：</p>
<ul>
<li>逆序索引覆盖了所有出现的单词；</li>
<li>对每个单词节点，其存储了逆序索引。逆序索引存储的是图像的索引号，以及该单词在对应图像中的权重（即逆文本频率）。</li>
<li>每进来一张图像，数据库以及逆序索引就需要更新。</li>
<li>如上图，词汇树有 $L_W$ 层，0 层代表叶节点即单词 word 所在的节点，该节点包含多个特征点。节点 1 的逆序索引的表示，在图 68 的逆文本频率为 0.79，在图 82 为 0.73。</li>
<li>如下图为逆序索引加快搜索的示意。</li>
</ul>
<center>
    <img src="/summary/Bag-of-Words%20Model%20(BoW)/Fig-3.png">
</center>

<p><strong>顺序索引</strong>：</p>
<ul>
<li>在回环检测的最后阶段——几何结构验证阶段，可以加速匹配候选图像与当前图像之间的特征点对。</li>
<li>对于每张图像，顺序索引存储出现在图像中的每个特征（word）对应的父节点；找到父节点后再把父节点下边所有特征也加入顺序索引。在最后的几何验证阶段就可以迅速找到两帧之间的特征匹配点对。需要为每张图建立 $0 - L_W$ 所有层的顺序索引。</li>
<li>如上图，第 $l$ 层中，图 1 的顺序索引有节点 1，节点 1 下边的所有特征点在本图像中出现的只有本图像中的第 65 个特征点。</li>
</ul>
<h2 id="6-4-回环检测流程"><a href="#6-4-回环检测流程" class="headerlink" title="6.4  回环检测流程"></a>6.4  回环检测流程</h2><p>BoW 模型主要是利用训练词典产生高度差异型的模型，每张图片通过词典检索，都会得到一个独一无二的直方图向量，向量的维度就是词典个数（一般是 100 万）。在具体写程序时不会产生这么大维度的链表，可以有一个等效的模型。</p>
<p><strong>图像 $\bold{\rightarrow}$ 向量</strong>：</p>
<ol>
<li>输入图像 $I$，进行特征检测与特征描述，特征点数不超过阈值；</li>
<li>将每个特征通过树形结构字典，得到 BoW 向量 $\bold{v}$，向量的维度为特征数量或叶节点数量。</li>
</ol>
<p>向量 $\bold{v}$ 的数据单元结构为 map<index, value> 或 map<word index, weight>。</word></index,></p>
<p><strong>word index</strong>：</p>
<ul>
<li>将每个特征从根节点 $\rightarrow$ 叶节点，依次与当前子节点进行比较，选择汉明距离最小的节点作为中继节点，以此类推，直到叶节点。</li>
</ul>
<p><strong>weight</strong>：</p>
<script type="math/tex; mode=display">
w_t^i = \operatorname{tf}(i, I_t) \times \operatorname{idf}(i),\ w_t^i = \frac{w_t^i}{\sum w_t^i} \\
\operatorname{tf}(i, I_t) = \frac{n_{i I_t}}{n_{I_t}},\ \operatorname{idf}(i) = \log \frac{N}{n_i}</script><p>每个单词的权重有归一化过程，权重由 tf 和 idf 两部分构成：对于前者，分子为当前图片中出现单词 $i$ 的个数，分母为图像总共包含的单词个数，这是在线得到的；后者是词典的属性，已经离线生成，不会发生改变，分子为生成词典时所包含的训练图片数量，分母为出现单词 $i$ 的图片个数。</p>
<p>在得到了两张图片的 BoW 向量之后，需要比较两者的相似性：</p>
<script type="math/tex; mode=display">
s(\bold{v}_1, \bold{v}_2) = 1 - \frac{1}{2} \cdot \left| \frac{\bold{v_1}}{|\bold{v}_1|} - \frac{\bold{v_2}}{|\bold{v}_2|} \right|</script><h2 id="6-5-回环检测的数据库查询"><a href="#6-5-回环检测的数据库查询" class="headerlink" title="6.5  回环检测的数据库查询"></a>6.5  回环检测的数据库查询</h2><p>查询的数据库就是上述在线更新和维护的逆序索引。利用逆序索引数据库简化检索过程；不采用暴力匹配，只匹配包含相同单词的个别图像信息， 加快检索过程。</p>
<p>当新的一帧到来，先计算其表达向量 $\bold{v}_t$，随后凭借逆序索引得到一连串的相似图像,分别与每一张相似图像 $\bold{v}_{t_j}$ 的归一化相似度（$t - \Delta t$ 表示上一帧）：</p>
<script type="math/tex; mode=display">
\eta(\bold{v}_t, \bold{v}_{t_j}) = \frac{s(\bold{v}_t, \bold{v}_{t_j})}{s(\bold{v}_t, \bold{v}_{t - \Delta t})}</script><ol>
<li>防止因分母过小，而引起的得分过大，需要对分母增加条件（有特征点数要求或者得分要求）；</li>
<li>需要对最后得分设立阈值，过滤得分较少的候选图像，保留符合要求的候选图像进入组匹配进行校验</li>
</ol>
<h2 id="6-6-组匹配"><a href="#6-6-组匹配" class="headerlink" title="6.6  组匹配"></a>6.6  组匹配</h2><p>当图像 $I_t, I_{t’}$ 表示了一个真正的回环，则 $I_t$ 同样和 $I_{t \pm \Delta t}, I_{t \pm 2 \Delta t}$ 也有较高的相似性，定义相似得分和函数如下：</p>
<script type="math/tex; mode=display">
H (\bold{v}_t, \bold{V}_{T_i}) = \sum_{j = n_i}^{m_i} \eta (\bold{v}_t, \bold{v_{t_j}})</script><p>其中 $\bold{V}_{T_i}$ 表示候选图像所在集合，范围从 $\bold{v}_{t_{ni}}$ 到 $\bold{v}_{t_{mj}}$。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://blog.csdn.net/u010213393/article/details/40987945">https://blog.csdn.net/u010213393/article/details/40987945</a></li>
<li><a href="https://blog.csdn.net/lp201721124039/article/details/106276176">https://blog.csdn.net/lp201721124039/article/details/106276176</a></li>
<li><a href="https://blog.csdn.net/weixin_44462888/article/details/117607384">https://blog.csdn.net/weixin_44462888/article/details/117607384</a></li>
<li><a href="https://www.cnblogs.com/wangguchangqing/p/9435269.html">https://www.cnblogs.com/wangguchangqing/p/9435269.html</a></li>
<li><a href="https://blog.csdn.net/qq_24893115/article/details/52629248（BoW">https://blog.csdn.net/qq_24893115/article/details/52629248（BoW</a> in Loop Closure）</li>
<li><a href="https://blog.csdn.net/kevin_cc98/article/details/75212668（BoW">https://blog.csdn.net/kevin_cc98/article/details/75212668（BoW</a> in Loop Closure）</li>
</ul>
]]></content>
      <categories>
        <category>Summary</category>
      </categories>
      <tags>
        <tag>Summary</tag>
        <tag>Deep Learning</tag>
        <tag>Knowledge</tag>
        <tag>SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>Blitz-SLAM</title>
    <url>/papernote/Blitz-SLAM/</url>
    <content><![CDATA[<p>对论文 Blitz-SLAM: A Semantic SLAM in Dynamic Environments 的阅读总结。Blitz-SLAM 使用 BlitzNet 提取目标检测框和掩码，用于剔除动态物体，实现语义 SLAM。</p>
<span id="more"></span>
<center><b><font size="9">Blitz-SLAM</font></b></center>

<h1 id="论文情况"><a href="#论文情况" class="headerlink" title="论文情况"></a>论文情况</h1><ul>
<li>标题：Blitz-SLAM: A Semantic SLAM in Dynamic Environments</li>
<li>作者：Yingchun Fan, Qichi Zhang, Yuliang Tang, Shaofen Liu, Hong Han</li>
<li>期刊：Pattern Recognition 2022</li>
<li>源码：未开源</li>
</ul>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1  Introduction"></a>1  Introduction</h1><p>现存的多数 SLAM：</p>
<ul>
<li>假设：静态的环境。</li>
<li>问题：静态环境假设提取的特征点应出现在静态物体上，若场景中存在动态物体，则特征点可能会提取在这些物体上。</li>
<li>解决：使用 RANSAC 在静态或低动态环境中去除匹配失败的点。然而无法保证在高动态环境下也能去除匹配失败点。</li>
</ul>
<p>SLAM + Deep Learning：</p>
<ul>
<li>动机：目标检测和目标分割的成熟。</li>
<li>方法：将预先定义的运动物体（如人、汽车等）的语义信息与空间几何信息相结合，去除运动物体的负面影响。</li>
<li>问题：目前的语义分割方法所提供的物体原始掩码不能完全覆盖移动的物体。</li>
</ul>
<p>本文研究：</p>
<ul>
<li>成果：提出工作在室内动态环境下的语义 SLAM —— Blitz-SLAM。</li>
<li>特点：<ul>
<li>使用 ORB-SLAM2 的前端移除动态物体。</li>
<li>使用 BlitzNet[1] 获取物体的语义信息。BlitzNet 可以获取物体的 bounding box 和 mask。运动物体的边界框将图像分为动态区域和静态区域；使用深度图的几何信息矫正掩码，使其能更完整的覆盖动态物体，且能将局部点云的噪声块移除。</li>
</ul>
</li>
</ul>
<p>本文贡献：</p>
<ol>
<li>提出通过结合原始掩码和物体深度信息的掩码矫正方法；</li>
<li>提出在构建点云地图时，约束运动物体产生的噪声块的方法；</li>
<li>提出解决运动物体多次出现在全局点云地图中的方法。</li>
</ol>
<h1 id="2-System-Description"><a href="#2-System-Description" class="headerlink" title="2  System Description"></a>2  System Description</h1><h2 id="2-1-Overview-of-Blitz-SLAM"><a href="#2-1-Overview-of-Blitz-SLAM" class="headerlink" title="2.1  Overview of Blitz-SLAM"></a>2.1  Overview of Blitz-SLAM</h2><center>
    <img src="/papernote/Blitz-SLAM/Figure-1.png">
</center>

<p>RGB 图像先被输入 BlitzNet 中，同时获取到环境中物体的边界框和掩码。</p>
<ul>
<li>BlitzNet 使用 ResNet-50 作为 backbone，并在 PASCAL VOC 数据集上得到训练，能够分割和检测 20 个类别（如人、家具）的物体。</li>
</ul>
<p>根据环境中物体的状态，可以分为三类：</p>
<ul>
<li>运动中物体（moving objects）：如行走中的人。物体不仅直接影响相机位姿估计，还会影响建图。</li>
<li>静态物体（static objects）：如桌子、镜子等。主要以静态形式出现在环境中，并且不会被频繁移动。</li>
<li>可运动物体（movable objects）：如椅子、书等。可运动、也可静止的物体。</li>
</ul>
<p>由于 BlitzNet 获得的原始物体掩码不完整，因此物体所在区域的深度信息作用于原始掩码获取矫正掩码，使得能完全覆盖物体区域。如 Fig.1 中黄色箭头，通过移除动态物体产生的噪声块，可以得到局部点云。</p>
<p>在处理相机定位时，通过 moving objects 的边界框可以讲图像分为环境区域和潜在动态区域。利用环境区域的匹配点构造对极约束，分类潜在动态区域内的动态和静态匹配点。之后，静态匹配点用于相机定位，流程如 Fig.1 中蓝色箭头。</p>
<p>Fig.1 中青色箭头表示构建全局点云地图，本质是将每组关键帧对应的局部点云合并（非简单加和，只是一种抽象表达）：</p>
<script type="math/tex; mode=display">
P_E = \sum_{i=1}^n (R_i P_i + t_i)
\tag{1}</script><p>$P_i$ 表示局部点云，$P_E$ 表示全局点云，旋转 $R_i$ 和平移 $t_i$ 由相机在世界坐标系下的位姿决定。取第一组关键帧的相机坐标系为世界坐标系。</p>
<p>全局点云地图的准确性取决于：</p>
<ul>
<li>相机定位精度；</li>
<li>局部点云中噪声块（动态物体）的去除；</li>
<li>相机轨迹的完整性。</li>
</ul>
<h2 id="2-2-Depth-Region-Segmentation"><a href="#2-2-Depth-Region-Segmentation" class="headerlink" title="2.2  Depth Region Segmentation"></a>2.2  Depth Region Segmentation</h2><p>由于深度相机的缺陷，深度图像的一些区域会丢失深度。当物体表面非常光滑时，物体的深度信息也会严重丢失。</p>
<p>基于假设 [2]：场景中的物体（尤其人工物体）很大程度上是凸的（convex），因此在深度图的非连续区域放置边缘点。深度图分割方法则如下：</p>
<ul>
<li><p>使用 $2 \times 2$ 的窗口遍历深度图，用 (2) 式记录下此窗口的深度，其中 $(u,v)$ 表示像素坐标：</p>
<script type="math/tex; mode=display">
  D_{block} = d (u : u + 1, v : v + 1)
  \tag{2}</script></li>
<li><p>边缘点可以通过 (3) 式获取，其中 $\tau_1 (=500(0.1\mathrm{m}))$ 表示阈值，$Edge$ 表示边缘点集合：</p>
<script type="math/tex; mode=display">
  \begin{cases}
  \text{if}\ \max (D_{block}) - \min (D_{block}) > \tau_1, &(u,v) \in Edge \\
  \text{else}, &(u,v) \notin Edge
  \end{cases}
  \tag{3}</script></li>
</ul>
<h2 id="2-3-Removal-of-Depth-Unstable-Regions"><a href="#2-3-Removal-of-Depth-Unstable-Regions" class="headerlink" title="2.3  Removal of Depth Unstable Regions"></a>2.3  Removal of Depth Unstable Regions</h2><p>深度相机的精度随物体距相机距离的增大而减小。为观测点云上深度不稳定区域的影响，使用 TUM 数据集中的一些 RGB 和 深度图，生成的局部点云，如 Fig.2。</p>
<center>
    <img src="/papernote/Blitz-SLAM/Figure-2.png">
</center>

<ul>
<li>从正面看，远处木板的点云比桌面的点云占据更大的区域。</li>
<li>从上看，远处木板的点云具有严重的层次性，而桌面的点云则较为紧凑。</li>
<li>当多个局部点云合并时，远处平板的分层现象会更加严重，直接影响全局点云图的视觉效果。</li>
</ul>
<p>因此，为了点云地图的质量，需要去除深度不稳定区域：当深度图中点的深度值大于 $3000(0.6\mathrm{m})$ 时，设置点的深度值为 $0$。</p>
<h2 id="2-4-Depth-Mask"><a href="#2-4-Depth-Mask" class="headerlink" title="2.4  Depth Mask"></a>2.4  Depth Mask</h2><p>BlitzNet 获取的原始掩码存在两个问题：</p>
<ul>
<li>掩码无法完全覆盖物体；</li>
<li>被掩码覆盖的物体可能包含环境和其他物体的信息。</li>
</ul>
<p>因此直接使用原始掩码构建的语义点云地图存在两个问题：</p>
<ul>
<li>moving objects 的信息会渗入环境，产生噪声块；</li>
<li>如果掩码超出了 static objects 的区域，部分语义信息会被映射到其他环境点。</li>
</ul>
<center>
    <img src="/papernote/Blitz-SLAM/Figure-3.png">
</center>

<p>如 Fig.3，转椅的原始掩码（红色）覆盖了部分坐着的人，而右边（橙色）则没有完全覆盖椅子。将两把椅子原始掩码提供的语义信息映射到当前点云上，可以发现一些环境点也被染成了红色。</p>
<p>使用原始掩码 $M^o$ 覆盖区域的深度信息改进掩码，得到深度掩码 $M^d$：</p>
<ul>
<li><p>首先，物体的原始源码用于给图像中的物体打标签，将所有的物体的记为 $\{ Obj(1), \dots, Obj(k) \}$；</p>
</li>
<li><p>然后，计数原始掩码 $M^o_{Obj(i)}$ 区域内的深度信息，移除深度值为 $0$ 的点或 outliers，得到深度值集合 $d^o_{Obj(i)}$。用 (4) 式，找到深度图中和 $Obj(i)$ 有相同深度范围的像素：</p>
<script type="math/tex; mode=display">
  \begin{cases}
  \text{if}\ L_d - \tau_2 \le d(u,v) \le U_d + \tau_2, &(u,v) \in M^s_{Obj(i)} \\
  \text{else}, &(u,v) \notin M^s_{Obj(i)}
  \end{cases}
  \tag{4}</script><p>  $U_d$ 和 $L_d$ 为 $d^o_{Obj(i)}$ 的最大和最小值，$d(u,v)$ 表示像素 $(u.v)$ 的深度值，$M^s_{obj(i)}$ 表示和 $Obj(i)$ 有相同深度范围的区域。为提高系统鲁棒，扩展 $Obj(i)$ 区域的深度范围 $2 \times \tau_2$，其中 $\tau_2 = 500(0.1\mathrm{m})$。</p>
<p>  $M^s_{obj(i)}$ 可能包含多个有相同深度范围的图像块 $M_i$：</p>
<script type="math/tex; mode=display">
  M^s_{obj(i)} = \bigcup_{i=1}^m M_i
  \tag{5}</script></li>
<li><p>最后，构建语义约束来保存属于深度掩码 $M^d_{obj(i)}$ 的图像块：</p>
<script type="math/tex; mode=display">
  \begin{cases}
  \text{if}\ \frac{P_{M_i}^S}{P_{M_i}} \ge \tau_3, &M_i \in M^d_{obj(i)} \\
  \text{else}, &M_i \notin M^d_{obj(i)}
  \end{cases}
  \tag{6}</script><p>  $P_{M_i}^S$ 表示 $M_i$ 中有 $Obj(i)$ 语义信息的像素的数量，$P_{M_i}$ 为 $M_i$ 中总的像素数量，$\tau_3(=0.25)$ 为为阈值。</p>
</li>
</ul>
<p>深度掩码获取算法如下：</p>
<center>
    <img src="/papernote/Blitz-SLAM/Algorithm-1.png">
</center>

<p>矫正掩码 $M^r_{Obj(i)}$ 则为原始掩码和深度掩码的并集：</p>
<script type="math/tex; mode=display">
M^r_{Obj(i)} = M^o_{Obj(i)} \cup M^d_{Obj(i)}
\tag{7}</script><p>对 $Obj(i)$ 对处理：</p>
<ul>
<li>如果 $Obj(i)$ 为 moving object，则需要移除被 $M^r_{Obj(i)}$ 覆盖的区域；</li>
<li>如果 $Obj(i)$ 为 static object，当构建语义点云图时，被 $M^d_{Obj(i)}$ 覆盖区域的语义信息需要被映射分配。</li>
</ul>
<center>
    <img src="/papernote/Blitz-SLAM/Figure-4.png">
</center>

<p>Fig.4 表示了掩码的修正过程：</p>
<ul>
<li>输入（黄框）为两个人的原始掩码及对应的深度图，坐着的人标签为 $P(1)$，站着的人为 $P(2)$；</li>
<li>得到的同深度范围区域为 $M^s_{P(1)}$ 和 $M^s_{P(2)}$（蓝框），其中 $M^s_{P(1)}$ 包含多个图像块，通过 (6) 式去除多余图像块后，得到深度掩码 $M^d_{P(1)}$；</li>
<li>最后，深度掩码与原始掩码融合得到矫正掩码。</li>
</ul>
<h2 id="2-5-Judgement-of-Interaction-between-Moving-People-and-Movable-Object"><a href="#2-5-Judgement-of-Interaction-between-Moving-People-and-Movable-Object" class="headerlink" title="2.5  Judgement of Interaction between Moving People and Movable Object"></a>2.5  Judgement of Interaction between Moving People and Movable Object</h2><p>当人的矫正掩码和 movable object 的矫正掩码相交时，则：</p>
<script type="math/tex; mode=display">
\begin{cases}
\text{if}\ M^r_{P(i)} \cap M^r_{C_{Obj}(j)} \ne \empty, &C_{Obj}(j) \in D_{Obj} \\
\text{else}, &C_{Obj}(j) \in S_{Obj}
\end{cases}
\tag{8}</script><p>其中 $i = 1, …, n$ 表示图像中的人，$j=1,…,m$ 表示图像中的 movable objects。$D_{Obj}$ 表示 moving objects 集合，$S_{Obj}$ 为 static objects 集合。</p>
<p>人和 movable objects 的相交判断算法如下：</p>
<center>
    <img src="/papernote/Blitz-SLAM/Algorithm-2.png">
</center>

<p>Fig.5 表示了判断人和椅子是否相交：</p>
<center>
    <img src="/papernote/Blitz-SLAM/Figure-5.png">
</center>

<ul>
<li>输入两个人和两把椅子的矫正掩码（红框），从左到右，两把椅子的标签为 $C(1)$ 和 $C(2)$，人的标签为 $P(1)$ 和 $P(2)$。经过判断，$P(1)$ 和 $C(1)$ 相交，则 $C(1)$ 变为 moving objects。</li>
</ul>
<center>
    <img src="/papernote/Blitz-SLAM/Figure-6.png">
</center>

<ul>
<li>将被两个人和 $C(1)$ 的矫正掩码覆盖的区域的深度值设为 0，将 $C(2)$ 的语义信息映射到当前点云。如 Fig.6，moving objects 被有效去除，$C(2)$ 的语义信息也没有污染其他环境点。</li>
</ul>
<h2 id="2-6-Removal-of-the-Residual-Information-of-Moving-Objects"><a href="#2-6-Removal-of-the-Residual-Information-of-Moving-Objects" class="headerlink" title="2.6  Removal of the Residual Information of Moving Objects"></a>2.6  Removal of the Residual Information of Moving Objects</h2><p>移除矫正掩码覆盖的区域后，moving object 的部分边界信息可能会遗留在环境中。这些残留边界形成的噪声块长而窄，因此需要进行去除。如 Fig.7 为边界信息去除的流程，输入为 RGB 图像以及移除被矫正掩码覆盖区域后的深度图像，输出为无噪声块的深度图：</p>
<center>
    <img src="/papernote/Blitz-SLAM/Figure-7.png">
</center>

<p>首先，从前一帧 $I_P$ 和当前帧 $I_C$ 的环境区域提取特征点。$I_P$ 和 $I_C$ 之间的单应矩阵（homography matrix）$H$ 可以通过 RANSAC 和 LM 算法计算得到。</p>
<p>假设 $p_P = [u_P, v_P, 1]^T$ 为点在 $I_P$ 的坐标，$p_C = [u_C, v_C, 1]^T$ 为点在 $I_C$ 的坐标，这些点为特征匹配点，而且可以得到 (9) 式（得到 $p_P$ 在 $I_C$ 的投影点）：</p>
<script type="math/tex; mode=display">
p_C = H p_P
\tag{9}</script><p>相邻帧上残留边界位置的深度值通常相差较大，因此，噪声块可以通过以下 (10) 式的判断去除：</p>
<script type="math/tex; mode=display">
\begin{cases}
\mathrm{if}\ |d_C(u_C, v_C) - d_P(u_P, v_P)| > \tau_4, &d_C(u_C, v_C) = 0 \\
\mathrm{else}, &d_C(u_C, v_C) = d_C(u_C, v_C)
\end{cases}
\tag{10}</script><p>其中 $d_P$ 和 $d_C$ 表示 $I_P$ 和 $I_C$ 对应的深度图，$\tau_4 = 5000(0.1\text{m})$ 为阈值。</p>
<p>最后，通过形态学操作（morphological operations）移除深度图像上少于 1000 个像素的图像块，确保 moving objects 被完全移除。moving objects 的残留信息移除算法如 Algorithm 3:</p>
<center>
    <img src="/papernote/Blitz-SLAM/Algorithm-3.png">
</center>

<p>Fig.8 则为一个点云去除噪声块前后的对比，噪声块被红色框标出：</p>
<center>
    <img src="/papernote/Blitz-SLAM/Figure-8.png">
</center>

<p>每次有人移动 movable objects，这些物体的位置相较之前会发生更改，如果无法处理这种情况，这些物体产生的冗余信息就会体现在全局点云地图上。文中采取了只保留这些物体的最近信息。</p>
<center>
    <img src="/papernote/Blitz-SLAM/Figure-9.png">
</center>

<p>Fig.9 展示了两把椅子在 4 个阶段的状态变化，两把椅子需要被保留的信息为绿框中的内容：</p>
<ul>
<li>初始阶段，椅子是 static object，信息被保留在全局地图中。</li>
<li>第二阶段，两个人进入图像并与椅子相交，椅子成为动态物体且不参与建图。此时，椅子在地图中的信息为第一个红框中的信息。</li>
<li>第三阶段，两个人离开图像，椅子变为静止，由于椅子的位置发生改变，第一阶段保留的椅子信息被删除，保留当前的信息。</li>
<li>最后阶段，两个人再次与椅子相交，则如第二阶段，信息不被保留，且地图中当前为第三阶段保留的椅子信息。</li>
</ul>
<h2 id="2-7-Location-of-the-Static-Matching-Points"><a href="#2-7-Location-of-the-Static-Matching-Points" class="headerlink" title="2.7  Location of the Static Matching Points"></a>2.7  Location of the Static Matching Points</h2><p>在获得前一帧 $I_P$ 和当前帧 $I_C$ 的两个静态匹配点集 $P_P = \{ p_{P1}, p_{P2}, …, p_{Pn} \}$ 和 $P_C = \{ p_{C1}, p_{C2}, …, p_{Cn} \}$，从 $I_P$ 到 $I_C$ 的相机位姿可以通过求解一下最小二乘得到：</p>
<script type="math/tex; mode=display">
\min_{R,t} \sum_{i=1}^n \left\| p_{Pi} - (R p_{Ci} + t) \right\|^2
\tag{11}</script><p>2.6 部分仅提取了静态区域的特征点，然而提取方法中部分区域被视为潜在动态区域（$R(BBX_P)$ 和 $R(BBX_C)$），没有被提取到特征。因此，此部分则讨论如何提取潜在动态区域的特征点。</p>
<p>首先，从 $I_P$ 和 $I_C$ 的环境区域提取匹配特征点，通过 RANSAC 算法求得两帧图像之间的基础矩阵（fundamental matrix）$F$。</p>
<p>然后，在 $I_P$ 和 $I_C$ 的潜在动态区域提取匹配点集 $P_{DP} = \{ p_{DP} | p_{DP} = [u_{DP}, v_{DP}, 1] \}$ 和 $P_{DC} = \{ p_{DC} | p_{DC} = [u_{DC}, v_{DC}, 1] \}$。</p>
<p>之后，通过极线计算潜在动态匹配点之间的距离：</p>
<script type="math/tex; mode=display">
d = \frac{\left| p_{DC} F p_{DP}^T \right|}{\sqrt{l_x^2 + l_y^2}}
\tag{12}</script><p>其中，$l_x$ 和 $l_y$ 的值为：</p>
<script type="math/tex; mode=display">
[l_x, l_y, l_z]^T = F p_{DP}^T
\tag{13}</script><p>潜在动态区域的匹配点的第 $i$ 个特征点的状态，可以通过 (14) 式判断匹配点之间的距离获取：</p>
<script type="math/tex; mode=display">
\begin{cases}
\text{if}\ d_i > \tau_5, &i \in D \\
\text{else}, &i \in S
\end{cases}
\tag{14}</script><p>其中 $D$ 表示动态匹配点集，$S$ 表示静态匹配点集，$\tau_5 = 0.5$。静态匹配点的定位算法如下：</p>
<center>
    <img src="/papernote/Blitz-SLAM/Algorithm-4.png">
</center>



<h1 id="3-Experimental-Results"><a href="#3-Experimental-Results" class="headerlink" title="3  Experimental Results"></a>3  Experimental Results</h1><p>Baseline：ORB-SLAM2，Dyna-SLAM，DS-SLAM</p>
<p>Dataset：TUM RGB-D</p>
<p>相机的运动四种状态模式：</p>
<ul>
<li>halfsphere：相机沿着半球体的轨迹移动；</li>
<li>rpy：相机绕固定轴旋转；</li>
<li>static：相机固定在某个位置；</li>
<li>xyz：相机沿着 x，y，z 轴移动。</li>
</ul>
<p>八个图像数据序列：fr3/w/half，fr3/w/rpy，fr3/w/static，fr3/w/xyz，fr3/s/half，fr3/s/xyz，fr2/desk/p，fr2/xyz。其中 fr、s、w、p 表示 Freiburg（弗莱堡，德国地名）、sitting、walking、person，desk 序列为行走的人手持相机拍摄的数据。</p>
<h2 id="3-1-Evaluation-of-Camera-Localization"><a href="#3-1-Evaluation-of-Camera-Localization" class="headerlink" title="3.1  Evaluation of Camera Localization"></a>3.1  Evaluation of Camera Localization</h2><ul>
<li>定量测量：绝对轨迹误差（ATE）、相对位姿误差（RPE）。</li>
</ul>
<p>设 $P_1, …, P_n \in SE(3)$ 表示估计的相机位姿，$Q_1, …, Q_n \in SE(3)$ 表示 GT。全局轨迹连续性的 ATE，或时间步 $i$ 时的 ATE $F_i$ 可以通过 (15) 计算：</p>
<script type="math/tex; mode=display">
F_i = Q_i^{-1} S P_i
\tag{15}</script><p>其中 $S$ 表示将估计轨迹与实际轨迹对齐的刚体变换。</p>
<p>RPE 测量了固定时间 $\Delta$ 下的局部轨迹精度，时间步 $i$ 的 RPE $E_i$ 可以通过 (16) 式计算：</p>
<script type="math/tex; mode=display">
E_i = (Q_i^{-1} Q_{i+\Delta})^{-1} (P_i^{-1} P_{i+\Delta})
\tag{16}</script><ul>
<li>定量分析：RMSE（Root Mean Square Error）和 S.D.（Standard Deviation）。RMSE 测量观测值和真值的误差，S.D. 测量群体相对整体的偏差。</li>
</ul>
<p>如 Table-1 至 Table-3 为定量分析结果，表中 improvements 的计算为：</p>
<script type="math/tex; mode=display">
\kappa = (1 - \frac{\beta}{\alpha}) \times 100 \%
\tag{17}</script><p>其中 $\alpha$ 为 ORB-SLAM2 的测量结果，$\beta$ 为 Blitz-SLAM 的测量结果。</p>
<center>
    <img src="/papernote/Blitz-SLAM/Table-1.png">
</center>

<center>
    <img src="/papernote/Blitz-SLAM/Table-2.png">
</center>

<center>
    <img src="/papernote/Blitz-SLAM/Table-3.png">
</center>

<ul>
<li>在高动态环境下能实现明显优于 ORB-SLAM2 的结果；</li>
<li>在低动态环境下取得的效果相比 ORB-SLAM2 基本无明显下降。</li>
</ul>
<center>
    <img src="/papernote/Blitz-SLAM/Figure-10.png">
</center>

<center>
    <img src="/papernote/Blitz-SLAM/Table-4.png">
</center>



<h2 id="3-2-Evaluation-of-the-Point-Cloud-Map"><a href="#3-2-Evaluation-of-the-Point-Cloud-Map" class="headerlink" title="3.2  Evaluation of the Point Cloud Map"></a>3.2  Evaluation of the Point Cloud Map</h2><ul>
<li>Fig.11 为 moving objects 移除的效果（第二行和第三行为原始掩码和矫正掩码）：</li>
</ul>
<center>
    <img src="/papernote/Blitz-SLAM/Figure-11.png">
</center>

<ul>
<li>Fig.12、Fig.13 和 Fig.14 分别为高动态、低动态环境和静态环境下的建图结果：</li>
</ul>
<center>
    <img src="/papernote/Blitz-SLAM/Figure-12.png">
</center>

<center>
    <img src="/papernote/Blitz-SLAM/Figure-13.png">
</center>

<center>
    <img src="/papernote/Blitz-SLAM/Figure-14.png">
</center>



<h2 id="3-3-Evaluation-of-Processing-Time"><a href="#3-3-Evaluation-of-Processing-Time" class="headerlink" title="3.3  Evaluation of Processing Time"></a>3.3  Evaluation of Processing Time</h2><center>
    <img src="/papernote/Blitz-SLAM/Table-5.png">
</center>



<h2 id="3-4-Evaluation-in-the-Real-World-Environment"><a href="#3-4-Evaluation-in-the-Real-World-Environment" class="headerlink" title="3.4  Evaluation in the Real-World Environment"></a>3.4  Evaluation in the Real-World Environment</h2><center>
    <img src="/papernote/Blitz-SLAM/Figure-15.png">
</center>

<center>
    <img src="/papernote/Blitz-SLAM/Figure-16.png">
</center>



<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li>[1] N. Dvornik, K. Shmelkov, J. Mairal, C. Schmid, Blitznet: A real-time deep network for scene understanding, in: Proceedings of the IEEE international con- ference on computer vision, 2017, pp. 4154–4162.</li>
<li>[2] M. Runz, M. Buffier, L. Agapito, Maskfusion: Real-time recognition, tracking and reconstruction of multiple moving objects, in: 2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR), IEEE, 2018, pp. 10–20.</li>
</ul>
]]></content>
      <categories>
        <category>Paper Note</category>
      </categories>
      <tags>
        <tag>Semantic SLAM</tag>
        <tag>Paper Note</tag>
      </tags>
  </entry>
  <entry>
    <title>DCN and RepPoints</title>
    <url>/summary/DCN%20and%20RepPoints/</url>
    <content><![CDATA[<p>可变形卷积网络（DCN）和 RepPoints 的介绍，结合网络计算整理总结。</p>
<span id="more"></span>
<center><b><font size="9">DCN and RepPoints</font></b></center>

<h1 id="1-DCN-Deformable-Convolution-Network"><a href="#1-DCN-Deformable-Convolution-Network" class="headerlink" title="1  DCN (Deformable Convolution Network)"></a>1  DCN (Deformable Convolution Network)</h1><p>在计算机视觉领域，同一物体在不同场景、角度中未知的几何变换是检测/识别的一大挑战，通常来说有两种做法:</p>
<ol>
<li><p>通过充足的数据增强，扩充足够多的样本去增强模型适应尺度变换的能力。</p>
</li>
<li><p>设置一些针对几何变换不变的特征或者算法，如 SIFT 和 sliding windows。</p>
</li>
</ol>
<p>两种方法都有缺陷，第一种方法因为样本的局限性使得模型的泛化能力比较低；第二种方法则因为手工设计的不变特征和算法对于过于复杂的变换是很难的而无法设计。所以提出了Deformable Conv（可变形卷积）和 Deformable Pooling（可变形池化）来解决这个问题。</p>
<p>图-1 展示了卷积核大小为 $3\times3$ 的普通卷积和可变形卷积的采样方式。(a) 表示按照正常的卷积规律采样 9 个点（绿点），(b)(c)(d) 为可变形卷积，再正常的采样坐标上加了一个位移量（蓝色箭头），其中 (c)(d) 为 (b) 的特殊情况，展示了可变形卷积可以作为尺度变换、比例变换和旋转变换的特殊情况。</p>
<center>
    <img src="/summary/DCN%20and%20RepPoints/1 普通卷积和可变形卷积的采样方式.png">
    <div>
        图-1 普通卷积和可变形卷积的采样方式
    </div>
</center>
对于普通卷积，每个输出 $y(p_0)$ 都要从 $x$ 上采样 9 个点，这 9 个点以中心位置 $x(p_0)$ 向四周扩散，$(-1,-1)$ 代表 $x(p_0)$ 的左上角，$(1,1)$ 代表 $x(p_0)$ 的右下角：
$$
\mathcal{R}=\{(-1,-1),(-1,0),...,(0,1),(1,1)\}
$$
传统卷积的输出则可以表示为：
$$
y(p_0)=\sum_{p_n \in \mathcal{R}} w(p_n) \cdot x(p_0 + p_n)
$$
可变形卷积就是在传统的卷积操作上加入了一个偏移量，正是这个偏移量让卷积变形为不规则的卷积，这里要注意这个偏移量可以是小数，所以下面的式子的特征值需要通过**双线性插值**的方法来计算：
$$
y(p_0)=\sum_{p_n \in \mathcal{R}} w(p_n) \cdot x(p_0 + p_n + \Delta p_n)
$$
对于偏移量如何计算，将在下面的部分解释。




## 1.1  可变形卷积

<center>
    <img src="/summary/DCN%20and%20RepPoints/2 可变形卷积示意.jpg">
    <div>
        图-2 可变形卷积示意图
    </div>
</center>
假设输入 feature map 大小为 $W\times H$，将要进行的可变形卷积为 $kernel\_size=3\times3,\ stride=1,\ dialated=1$，那么首先用一个具有与当前可变性卷积层相同的空间分辨率和扩张率的卷积（$k=3\times3,\ s=1,\ dial=1$）进行学习 offset。此卷积会输出一个 $W\times H\times 2N$ 的 offset filed （$N$ 是可变性卷积的 $3\times3=9$ 个点，$2N$ 是每个点有 x 和 y 两个方向的偏移分量）。后面的可变形卷积可以看作先基于上面那部分生成的 offset 做了一个插值操作，然后再执行普通的卷积。



## 1.2  可变形卷积的可视化

<center>
    <img src="/summary/DCN%20and%20RepPoints/3 普通卷积与可变形卷积对比.jpg">
    <div>
        图-3 普通卷积与可变形卷积的对比
    </div>
</center>

<center>
    <img src="/summary/DCN%20and%20RepPoints/4 可变形卷积对图像的处理.jpg">
    <div>
        图-4 可变形卷积对图像的处理
    </div>
</center>

<p>可以从图-4 看到，当绿色点在目标上时，红色点所在区域也集中在目标位置，并且基本能够覆盖不同尺寸的目标，因此经过可变形卷积，可以更好地提取出感兴趣物体的完整特征。</p>
<h2 id="1-3-可变形池化"><a href="#1-3-可变形池化" class="headerlink" title="1.3  可变形池化"></a>1.3  可变形池化</h2><p>原始的 RoI Pooling 在操作过程中是将 RoI 划分为 $k\times k$ 个子区域（bin），最终得到 $k\times k$ 的 feature map，对于第 $(i,j)\ (0\le i,j &lt; k)$ 个 bin，有：</p>
<script type="math/tex; mode=display">
y(i,j)=\frac{1}{n_{ij}}\sum_{p\in bin(i,j)} x(p_0+p)</script><p>$n_{ij}$ 为 bin 中像素点的个数，对于第 $(i,j)$ 个 bin，有：</p>
<script type="math/tex; mode=display">
\lfloor i\frac{w}{k} \rfloor \le p_x \le \lceil (i+1)\frac{w}{k} \rceil \\
\lfloor j\frac{h}{k} \rfloor \le p_y \le \lceil (j+1)\frac{h}{k} \rceil</script><p>新的 RoI Pooling 可以计算规则为：</p>
<script type="math/tex; mode=display">
y(i,j)=\sum_{p\in bin(i,j)} x(p_0+p+\Delta p_{ij})/n_{ij}</script><p>可变形池化的偏移量其实就是子区域的偏移。每一个子区域都有一个偏移，偏移量对应子区域有 $k\times k$ 个。与可变形卷积不同的是，可变形池化的偏移量是通过全连接层得到的。对于可变形池化的偏移量，计算式子如下：</p>
<script type="math/tex; mode=display">
\Delta p_{ij}=\gamma\cdot\Delta\hat{p}_{ij}\circ(w,h)</script><p>首先 RoI Pooling 产生 pooled feature map（图-2 中绿色部分），然后用 FC（Fully Connected）去学习归一化后的偏移值（归一化的目的是使偏移量学习不受 RoI 大小的影响），这个偏移量会与 $(w,h)$ 做点乘，然后再乘以尺度因子 $\gamma$，其中 $w,\ h$ 是 RoI 的宽和高，$\gamma$ 为 0.1。用该位移作用在可变形 RoI Pooling 上（蓝色区域），以获得不局限于 RoI 固定网格的特征。</p>
<center>
    <img src="/summary/DCN%20and%20RepPoints/5 可变形RoI Pooling.jpg">
    <div>
        图-5 可变形 RoI Pooling
    </div>
</center>

<p>调整后的 RoI bin 如红色框所示，此时并不是规则的将黄色框 RoI 等分为 $3\times3$ 的 bins。</p>
<h2 id="1-4-Deformable-Network"><a href="#1-4-Deformable-Network" class="headerlink" title="1.4  Deformable Network"></a>1.4  Deformable Network</h2><p>网络设置方面：</p>
<ol>
<li>学习 offset 的参数采用的是0初始化，然后按照网络其它参数学习率的 $\beta$ 倍来学习，$\beta$ 默认是 1，但在某些情况下，比如 Faster R-CNN 中的 fc 是 0.01。</li>
<li>并不是所有的卷积都换成可变形卷积就是好的，在提取到一些语义特征后使用可变形卷积效果会更好，一般来说是网络靠后的几层。</li>
</ol>
<h1 id="2-DCN-v2"><a href="#2-DCN-v2" class="headerlink" title="2  DCN v2"></a>2  DCN v2</h1><p>图-6 为普通卷积核 DCN v1 再三张图片上的展示结果，每一列代表检测不同的物体，从左向右，物体面积增大；每一行代表不同的表现形式，第一行为物体周围的感受野分布，第二行为有效的采样位置，第三行为得到能相同的响应的最小区域。从第三行可以很明显的看出，DCN v1 会增加无关信息。为了提高网络适应几何变化的能力，减少无关因素的影响，DCN v2 提出了改进以提高其建模能力并帮助它利用这种增强的功能。</p>
<center>
    <img src="/summary/DCN%20and%20RepPoints/6 普通卷积和可变形卷积v1的特征提取.jpg">
    <div>
        图-6 普通卷积和 DCN v1 的特征提取
    </div>
</center>

<p>DCN v2 提出了三种解决方法：</p>
<ol>
<li>More Deformable Conv Layers（使用更多的可变形卷积）。</li>
<li>Modulated Deformable Modules（在 DCN v1 基础（添加 offset）上添加每个采样点的权重）</li>
<li>R-CNN Feature Mimicking（模拟 R-CNN 的 feature）。</li>
</ol>
<h2 id="2-1-More-Deformable-Conv-Layers"><a href="#2-1-More-Deformable-Conv-Layers" class="headerlink" title="2.1  More Deformable Conv Layers"></a>2.1  More Deformable Conv Layers</h2><p>DCN v1 只在 ResNet-50 的 conv5 中使用了三个可变形卷积，而 DCN v2 在 conv3、conv4 和 conv5 的 $3\times3$ 卷积层中都使用了可变形卷积，得到如图-7 中的结果。</p>
<center>
    <img src="/summary/DCN%20and%20RepPoints/7 增加可变形卷积的效果.jpg">
    <div>
        图-7 增加可变形卷积的效果
    </div>
</center>



<h2 id="2-2-Modulated-Deformable-Modules"><a href="#2-2-Modulated-Deformable-Modules" class="headerlink" title="2.2  Modulated Deformable Modules"></a>2.2  Modulated Deformable Modules</h2><p>在 DCN v1 中，卷积只是添加了 offset 参数 $\Delta p_{n}$：</p>
<script type="math/tex; mode=display">
y(p_0)=\sum_{p_n \in \mathcal{R}} w(p_n) \cdot x(p_0 + p_n + \Delta p_n)</script><p>或记 $K$ 为卷积核的采样点个数，$w_k,\ p_k$ 为第 $k$ 个采样点的卷积核权重和偏移，则上式可以写为：</p>
<script type="math/tex; mode=display">
y(p)=\sum_{k=1}^K w_k \cdot x(p + p_k + \Delta p_k)</script><p>为解决引入了一些无关区域的问题，DCN v2 中不只添加每一个采样点的偏移，还添加了一个权重系数 $\Delta m_k\ (\in [0,1])$，用于区分所引入的区域是否为感兴趣的区域。加入权重系数后，上式变为：</p>
<script type="math/tex; mode=display">
y(p)=\sum_{k=1}^K w_k \cdot x(p + p_k + \Delta p_k) \cdot \Delta m_k</script><p>$\Delta p_k$ 和 $\Delta m_k$ 是学习到的 offset 和权重，这样的好处是增加了更大的自由度，对于不需要的采样点则设置权重为 0 即可（权重学习为 0）。</p>
<p>$\Delta p_k$ 和 $\Delta m_k$ 都是通过在相同的输入 feature map $x$ 上应用单独的卷积层获取，该卷积层具有与当前卷积层相同的空间分辨率，输出为 $3K$ 通道，其中前 $2K$ 对应于学习的偏移，剩余 $K$ 进一步进入 Sigmoid 层用于获得权重，此卷积的卷积核初始为 0，$\Delta p_k$ 和 $\Delta m_k$ 的初始值为 0 和 0.5，学习率为其他现有卷积层的 0.1 倍。</p>
<p>Modulated Deformable RoI Pooling 的式子也因权重的加入相应变为：</p>
<script type="math/tex; mode=display">
y(k)=\frac{1}{n_k} \sum_{j=1}^{n_k} x(p_{kj} + \Delta p_k) \cdot \Delta m_k</script><p>$\Delta p_k$ 和 $\Delta m_k$ 由输入特征图上的分支产生。在这个分支中，RoI Pooling 在 RoI 上生成特征，然后输入两个 1024-D 的 FC 层。然后在这些后面有一个额外的 FC 层，产生 $3K$ 的通道输出，前 $2K$ 是可学习的归一化偏移 $\Delta p_k$，后 $K$ 是经 Sigmoid 标准化后产生的 $\Delta m_k$，额外的 FC 层的学习率和其他层相同。</p>
<h2 id="2-3-R-CNN-Feature-Mimicking"><a href="#2-3-R-CNN-Feature-Mimicking" class="headerlink" title="2.3  R-CNN Feature Mimicking"></a>2.3  R-CNN Feature Mimicking</h2><p>把 R-CNN 和 Faster R-CNN 的 classification score 结合起来可以提升 performance，说明 R-CNN 学到的 focus 在物体上的 feature 可以解决 redundant context 的问题。但是增加额外的 R-CNN 会使 inference 速度变慢很多。DCN v2 里的解决方法是把 R-CNN 当做 teacher network，让 DCN v2 的 RoI Pooling 之后的 feature 去模拟 R-CNN 的 feature。</p>
<center>
    <img src="/summary/DCN%20and%20RepPoints/8 模拟R-CNN的feature.jpg">
    <div>
        图-8 模拟 R-CNN 的 feature
    </div>
</center>

<p>左边的网络为主网络（Faster R-CNN），右边的网络为子网络（R-CNN）。实现上大致是用主网络训练过程中得到的 RoI 去裁剪原图，然后将裁剪到的图 resize 到 $224\times224$ 大小作为子网络的输入，将裁剪的 RoI 区域记为 $b$。在 R-CNN 分支中，骨干网络在调整大小的图像块上操作并产生 $14\times14$ 的特征图。可变形 RoI Pooling 层应用于特征图，其中输入 RoI 覆盖整个调整大小的图像块（左上角为 $(0,0)$，高度和宽度为 224 像素）。之后，应用 2 个 1024-D 层，产生输入图像的 R-CNN feature vector，表示为 $f_{RCNN}(b)$。用 Softmax 分类器进行分类，其中 $C$ 表示前景类别的数量，加上一个用于背景。在 R-CNN 特征表示 $f_{RCNN}(b)$ 和 Faster R-CNN 中的对应物 $f_{FRCNN}(b)$ 之间执行 feature mimic loss，其也是 $f_{RCNN}(b)$ 和 $f_{FRCNN}(b)$ 的余弦相似度，计算方法如下：</p>
<script type="math/tex; mode=display">
L_{mimic}=\sum_{b\in\Omega} [1-\cos(f_{RCNN}(b),f_{FRCNN}(b))]</script><p>因为 R-CNN 这个子网络的输入就是 RoI 在原输入图像上裁剪出来的图像，因此不存在 RoI 以外区域信息的干扰，这就使得 R-CNN 这个网络训练得到的分类结果更加可靠，以此通过一个损失函数监督主网络 Faster R-CNN 的分类支路训练就能够使网络提取到更多 RoI 内部特征，而不是自己引入的外部特征。</p>
<p>总的 loss 由三部分组成：mimic loss + R-CNN classification loss + Faster-RCNN loss</p>
<p>在训练时，主干网络、Deformable RoIPooling和 2 个 FC 共享参数。inference 阶段，只用 Faster R-CNN 来测试。</p>
<h1 id="3-RepPoints"><a href="#3-RepPoints" class="headerlink" title="3  RepPoints"></a>3  RepPoints</h1><p>在典型 anchor-based 的算法中，模型的效果往往受限于 anchor 的配置参数，如 anchor 大小、正负样本采样、anchor 的宽高比等。anchor-free 的算法无需配置 anchor 参数，即可训练得到一个好的检测模型，减少了训练前对数据复杂的分析过程。</p>
<p>anchor-free 算法又可分为基于 anchor-point 的算法和基于 key-point 的算法：</p>
<ul>
<li>anchor-point 算法本质上和 anchor-based 算法相似，通过预测目标中心点 $(x,y)$ 及边框距中心点的距离 $(w,h)$ 来检测目标，典型的此类算法有 FSAF、FCOS 等。</li>
<li>key-point 方法是通过检测目标的边界点（如：角点），再将边界点配对组合成目标的检测框，此类算法包括 CornerNet、RepPoints 等。</li>
</ul>
<p>经典的 bounding box 虽然有利于计算，但没有考虑目标的形状和姿态，而且从矩形区域得到的特征可能会受背景内容或其它的目标的严重影响，低质量的特征会进一步影响目标检测的性能。为了解决 bounding box 存在的问题，RepPoints 的提出能够进行更细粒度的定位能力以及更好的分类效果。</p>
<p>RepPoints 提出使用点集的方式来表示目标，该方法在不使用 anchor 的基础上取得了非常好的效果。如图-9 ，(a) 表示一般目标检测算法使用水平包围框来表示目标位置信息，(b) 则表示 RepPoints 使用点集来表示目标位置的方法。RepPoints 系列工作其实就是以点集表示为核心，从不同的角度去进一步提升该算法精度而做出改进：</p>
<ul>
<li>将验证（即分割）的过程融入 RepPoints，进一步提升结果，得到 RepPoints V2；</li>
<li>将点集的监督方式进行改进，并扩充点集的点数，实现了目标实力分割任务的统一范式，即 Dense RepPoints。</li>
</ul>
<center>
    <img src="/summary/DCN%20and%20RepPoints/9 目标检测的两种方法.jpg">
    <div>
        图-9 目标检测的两种方法
    </div>
</center>



<h2 id="3-1-RepPoints"><a href="#3-1-RepPoints" class="headerlink" title="3.1  RepPoints"></a>3.1  RepPoints</h2><p>bounding box 只是粗粒度的目标位置表示方法，只考虑了目标的矩形空间，没有考虑形状、姿态以及语义丰富的局部区域，而语义丰富的局部区域能够帮助网络更好的定位以及特征提取。为了解决上述的缺点，RepPoints 使用一组自适应的采样点表示目标：</p>
<script type="math/tex; mode=display">
\mathcal{R}=\{ (x_k,y_k) \}_{k=1}^n</script><p>$n$ 表示目标的采样点数，RepPoints 原始论文中设为 9。</p>
<h3 id="3-1-1-RepPoints-Refinement"><a href="#3-1-1-RepPoints-Refinement" class="headerlink" title="3.1.1  RepPoints Refinement"></a>3.1.1  RepPoints Refinement</h3><p>逐步调整 bounding box 定位和特征提取是 multi-stage 检测器成功的重要手段，对于 RepPoints，调整可简单地表示为：</p>
<script type="math/tex; mode=display">
\mathcal{R}_r = \{ (x_k + \Delta x_k, y_k + \Delta y_k) \}_{k=1}^n</script><p>$\{ (\Delta x_k,\Delta y_k) \}_{k=1}^n$ 为预测的新采样点相对与旧采样点的偏移值，采样点调整的尺寸都是一样的，不会像 bouning box 那样需要解决中心点坐标和边框长度的尺寸不一致问题。</p>
<h3 id="3-1-2-Converting-RepPoints-to-Bounding-Box"><a href="#3-1-2-Converting-RepPoints-to-Bounding-Box" class="headerlink" title="3.1.2  Converting RepPoints to Bounding Box"></a>3.1.2  Converting RepPoints to Bounding Box</h3><p>为了利用 bounding box 的标注信息进行训练以及验证 RepPoint-based 检测算法的性能，使用预设的转化方法 $\mathcal{T}:\mathcal{R}_P\to \mathcal{B}_P$ 将 RepPoints 转化成伪预测框（$\mathcal{R}_P$ 表示物体 $P$ 的 RepPoints，$\mathcal{B}_P = \mathcal{T}(\mathcal{R}_P)$ 表示相应的 pseudo box），共有三种转化方法：</p>
<ul>
<li>$\mathcal{T}=\mathcal{T}_1$： Min-max function，对所有的 RepPoints 进行 min-max 操作来获取预测框 $\mathcal{B}_p$</li>
<li>$\mathcal{T}=\mathcal{T}_2$：Partial min-max function，对部分的 RepPoints 进行 min-max 操作获取预测框 $\mathcal{B}_p$</li>
<li>$\mathcal{T}=\mathcal{T}_3$：Moment-based function，通过 RepPoints 的均值和标准差计算中心点位置以及预测框尺寸得到预测框 $\mathcal{B}_p$，尺寸通过全局共享的可学习参数 $\lambda_x$ 和 $\lambda_y$ 相乘得到</li>
</ul>
<p>这些函数都是可微的，可加入检测器中进行 end-to-end 的训练。通过实验验证，这 3 个转化方法效果都不错。</p>
<center>
    <img src="/summary/DCN%20and%20RepPoints/10 RepPoints和box.jpg">
    <div>
        图-10 RepPoints 和 bounding box 的关系
    </div>
</center>



<h2 id="3-2-RPDet-an-Anchor-Free-Detector"><a href="#3-2-RPDet-an-Anchor-Free-Detector" class="headerlink" title="3.2  RPDet: an Anchor Free Detector"></a>3.2  RPDet: an Anchor Free Detector</h2><p>anchor-free 的目标检测算法 RPDet 是基于 RepPoints 设计，包含两个识别阶段。因为可变形卷积可采样多个不规则分布的点进行卷积输出，所以可变形卷积十分适合 RepPoints 场景，能够根据识别结果的反馈进行采样点的引导。图-11 为 RPDet 的主要框架：</p>
<center>
    <img src="/summary/DCN%20and%20RepPoints/11 RPDet的主要框架.jpg">
    <div>
        图-11 RPDet的主要框架
    </div>
</center>



<h3 id="3-2-1-Center-Point-Based-Initial-Object-Representation"><a href="#3-2-1-Center-Point-Based-Initial-Object-Representation" class="headerlink" title="3.2.1  Center Point Based Initial Object Representation"></a>3.2.1  Center Point Based Initial Object Representation</h3><p>RPDet 将中心点作为初始的目标表示，然后逐步调整出最终的 RepPoints，中心点也可认为是特殊的 RepPoints。当两个目标存在于特征图同一位置时，这种基于中心点的方法通常会出现识别目标歧义的问题。此前的方法在同一位置设置多个预设的 anchor 来解决此问题，而 RPDet 则利用 FPN 来解决此问题：</p>
<ul>
<li>不同大小的目标由不同 level 的特征负责识别</li>
<li>小物体对应 level 的特征图一般较大，减少了同一物体存在同一位置的可能性</li>
</ul>
<h3 id="3-2-2-Utilization-of-RepPoints"><a href="#3-2-2-Utilization-of-RepPoints" class="headerlink" title="3.2.2  Utilization of RepPoints"></a>3.2.2  Utilization of RepPoints</h3><p>如图-11 所示，RepPoints 是 RPDet 的基础目标表示方法，从中心点开始，第一组 RepPoints 通过回归中心点的偏移值获得。第二组 RepPoints 代表最终的目标位置，由第一组 RepPoints 优化调整得到。RepPoints 的学习主要由两个目标驱动：</p>
<ul>
<li>伪预测框和 GT 框的左上角点和右上角点的距离损失</li>
<li>后续的目标分类损失</li>
</ul>
<p>第一组 RepPoints 由距离损失和分类损失引导，第二组 RepPoints 仅使用距离损失进行引导，主要为了学习到更精准的目标定位。</p>
<h3 id="3-2-3-Backbone-and-Head-Architectures"><a href="#3-2-3-Backbone-and-Head-Architectures" class="headerlink" title="3.2.3  Backbone and Head Architectures"></a>3.2.3  Backbone and Head Architectures</h3><center>
    <img src="/summary/DCN%20and%20RepPoints/12 RPDet的头的结构.png">
    <div>
        图-12 RPDet的头的结构
    </div>
</center>

<p>FPN 主干网络包含5层特征金字塔 level，从 stage3（下采样 8 倍）到 stage7（下采样 128 倍）。Head 的结构如图-12，Head 在不同的 level 中是共享的，包含两个独立的子网，分别负责定位（RepPoints 的生成）和分类：</p>
<ul>
<li>定位子网首先使用 3 个 256-d $3\times 3$ 卷积提取特征，每个卷积都接 group normalization 层，然后连续接两个小网络计算两组 RepPoints 的偏移值。</li>
<li>分类子网首先使用 3 个 256-d $3\times 3$ 卷积提取特征，每个卷积都接 group normalization 层，然后将定位子网输出的第一组 RepPoints 的偏移值输入到 256-d $3\times 3$ 可变形卷积中进一步提取特征，最后输出分类结果。</li>
</ul>
<p>尽管 RPDet 采用了两阶段定位，但其性能甚至比单阶段的 RetinaNet 要高，主要是 anchor-free 的设计使得分类层的计算减少了，覆盖了额外的定位阶段带来的少量消耗。</p>
<h3 id="3-2-4-Localization-Class-Target-Assignment"><a href="#3-2-4-Localization-Class-Target-Assignment" class="headerlink" title="3.2.4  Localization/Class Target Assignment"></a>3.2.4  Localization/Class Target Assignment</h3><p>如图-12，定位包含两个阶段，第一阶段从中心点得到第一组 RepPoints，第二阶段则从第一组 RepPoints 调整得到第二组 RepPoints，不同的阶段的正样本定义不同：</p>
<ul>
<li>对于第一阶段，特征点被认为是正样本需满足：1) 该特征点所在的特征金字塔 level 等于 $s(B)=\lfloor \log_2 (\sqrt{w_Bh_B}/4)\rfloor$。2) 目标的中心点在特征图上映射位置对应该特征点。</li>
<li>对于第二阶段，只有特征点对应的第一阶段产生的伪预测框与目标的 IoU 大于 0.5 才被认为是正样本。与当前的 anchor-based 方法有点类似，将第一阶段的输出当作 anchor。</li>
</ul>
<p>由于目标的分类只考虑第一组 RepPoints，所以，特征点对应的第一组 RepPoints 产生的伪预测框于目标的 IoU 大于 0.5 即认为是正样本，小于 0.4 则认为是背景类，其它则忽略。</p>
<h1 id="附-空洞卷积（Dilated-Convolution）"><a href="#附-空洞卷积（Dilated-Convolution）" class="headerlink" title="附  空洞卷积（Dilated Convolution）"></a>附  空洞卷积（Dilated Convolution）</h1><h2 id="Appendix-1-空洞卷积"><a href="#Appendix-1-空洞卷积" class="headerlink" title="Appendix.1  空洞卷积"></a>Appendix.1  空洞卷积</h2><p>如图.附-1 所示就是空洞卷积，这里 $stride=1,\ padding=0$。空洞卷积有两种理解：</p>
<ol>
<li>理解为将卷积核扩展，如图卷积核为 $3\times3$ 但是这里将卷积核变为 $5\times5$，即在卷积核每行每列中间加0 。</li>
<li>理解为在特征图上每隔 1 行或 1 列取数与 $3\times3$ 卷积核进行卷积。</li>
</ol>
<p>改变 stride 和 padding，空洞卷积就会和图.附-1 有区别。</p>
<center>
    <img src="/summary/DCN%20and%20RepPoints/附1 空洞卷积示意.gif">
    <div>
        图.附-1 空洞卷积示意图
    </div>
</center>



<h2 id="Appendix-2-空洞卷积的感受野"><a href="#Appendix-2-空洞卷积的感受野" class="headerlink" title="Appendix.2  空洞卷积的感受野"></a>Appendix.2  空洞卷积的感受野</h2><p>CNN 的感受野为：</p>
<script type="math/tex; mode=display">
r_n = r_{n-1} + (k-1) \times \prod_{i=1}^{n-1} s_i</script><p>$r_n$ 为本层感受野，$r_{n-1}$ 为上层感受野，$s_i$ 是第 $i$ 层卷积或池化的步长，$k$ 为卷积核的大小，例（初始感受野为 1）：</p>
<ul>
<li>第一层 $3\times3$ 卷积（$stride=1$）：$r=1+(3-1)=3$，感受野为 $3\times3$</li>
<li>第二层 $2\times2$ 池化（$stride=2$）：$r=3+(2-1)\times1=4$，感受野为 $4\times4$</li>
<li>第三层 $3\times3$ 卷积（$stride=3$）：$r=4+(3-1)\times2\times1=8$，感受野为 $8\times8$</li>
<li>第四层 $3\times3$ 卷积（$stride=2$）：$r=8+(3-1)\times3\times2\times1=20$，感受野为 $20\times20$</li>
</ul>
<p>空洞卷积的感受野计算方法和上面相同，空洞可以理解为扩大了卷积核的大小，空洞卷积的感受野变化（卷积核大小为 $3\times3$，$stride=1$)：</p>
<ul>
<li>第一层 1-dilated conv：$dilated=1$ 的空洞卷积为普通的 $3\times3$ 卷积，因此 $r=1+(3-1)=3\ (=2^{\log_2 1+2}-1)$，感受野为 $3\times3$</li>
<li>第二层 2-dilated conv：$dilated=2$ 的空洞卷积可以理解为卷积核变为了 $5\times5$，因此 $r=3+(5-1)\times1=7\ (=2^{\log_2 2+2}-1)$，感受野为 $7\times7$</li>
<li>第三层 4-dilated conv：$dilated=4$ 的空洞卷积可以理解为卷积核变味了 $9\times9$，因此 $r=7+(9-1)\times1\times1=15\ (=2^{\log_2 4+2}-1)$，感受野为 $15\times15$</li>
</ul>
<p>可以看到将卷积以上面的过程叠加，感受野变化会指数增长，感受野公式为 </p>
<script type="math/tex; mode=display">
r=2^{\log_2 dilated+2}-1</script><p>上文所述该计算公式是基于叠加的顺序，如果单用三个 $3\times3$ 的 2-dilated 卷积则感受野使用卷积感受野计算公式计算：</p>
<ul>
<li>第一层 $3\times3$ 的 2-dilated 卷积： $r=1+(5-1)=5$</li>
<li>第二层 $3\times3$ 的 2-dilated 卷积： $r=5+(5-1)\times1=9$</li>
<li>第三层 $3\times3$ 的 2-dilated 卷积： $r=9+(5-1)\times1\times1=13$</li>
</ul>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><h2 id="References-1-RepPoints"><a href="#References-1-RepPoints" class="headerlink" title="References.1  RepPoints"></a>References.1  RepPoints</h2><ul>
<li><a href="https://zhuanlan.zhihu.com/p/64522910">https://zhuanlan.zhihu.com/p/64522910</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/260656201">https://zhuanlan.zhihu.com/p/260656201</a></li>
<li><a href="https://segmentfault.com/a/1190000040265117">https://segmentfault.com/a/1190000040265117</a></li>
<li><a href="https://blog.csdn.net/weixin_43711554/article/details/104983391">https://blog.csdn.net/weixin_43711554/article/details/104983391</a></li>
</ul>
<h2 id="References-2-DCN"><a href="#References-2-DCN" class="headerlink" title="References.2  DCN"></a>References.2  DCN</h2><ul>
<li><a href="https://blog.csdn.net/qq_41917697/article/details/116193042">https://blog.csdn.net/qq_41917697/article/details/116193042</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/180075757">https://zhuanlan.zhihu.com/p/180075757</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/339660219">https://zhuanlan.zhihu.com/p/339660219</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/52578771">https://zhuanlan.zhihu.com/p/52578771</a></li>
</ul>
<h2 id="References-3-Dilated-Convolution"><a href="#References-3-Dilated-Convolution" class="headerlink" title="References.3  Dilated Convolution"></a>References.3  Dilated Convolution</h2><ul>
<li><a href="https://zhuanlan.zhihu.com/p/43784441">https://zhuanlan.zhihu.com/p/43784441</a></li>
</ul>
<h1 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h1><ul>
<li><p>DCN：<a href="https://link.zhihu.com/?target=https%3A//github.com/msracver/Deformable-ConvNets">https://link.zhihu.com/?target=https%3A//github.com/msracver/Deformable-ConvNets</a></p>
</li>
<li><p>RepPoints：<a href="https://github.com/microsoft/RepPoints">https://github.com/microsoft/RepPoints</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>Summary</category>
      </categories>
      <tags>
        <tag>Summary</tag>
        <tag>Deep Learning</tag>
        <tag>Object Detection</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
</search>
